{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Notes \u00b6 Notes about various topics, courses, books, etc. Contents \u00b6 OAuth 2.0 and OpenID Connect Building microservices Bitcoin and cryptocurrency technologies","title":"Home"},{"location":"#notes","text":"Notes about various topics, courses, books, etc.","title":"Notes"},{"location":"#contents","text":"OAuth 2.0 and OpenID Connect Building microservices Bitcoin and cryptocurrency technologies","title":"Contents"},{"location":"bitcoin-and-cryptocurrency-technologies/","text":"Bitcoin and cryptocurrency technologies \u00b6 Notes on the 2018 edition of the Coursera MOOC Bitcoin and Cryptocurrency Technologies by Princeton University. About this course: To really understand what is special about Bitcoin, we need to understand how it works at a technical level. We\u2019ll address the important questions about Bitcoin, such as: How does Bitcoin work? What makes Bitcoin different? How secure are your Bitcoins? How anonymous are Bitcoin users? What determines the price of Bitcoins? Can cryptocurrencies be regulated? What might the future hold? After this course, you\u2019ll know everything you need to be able to separate fact from fiction when reading claims about Bitcoin and other cryptocurrencies. You\u2019ll have the conceptual foundations you need to engineer secure software that interacts with the Bitcoin network. And you\u2019ll be able to integrate ideas from Bitcoin in your own projects. Course Lecturers: Arvind Narayanan, Princeton University Lessons: Cryptographic hash functions Digital signature Hash pointers Centralization and decentralization Distributed consensus","title":"Home"},{"location":"bitcoin-and-cryptocurrency-technologies/#bitcoin-and-cryptocurrency-technologies","text":"Notes on the 2018 edition of the Coursera MOOC Bitcoin and Cryptocurrency Technologies by Princeton University. About this course: To really understand what is special about Bitcoin, we need to understand how it works at a technical level. We\u2019ll address the important questions about Bitcoin, such as: How does Bitcoin work? What makes Bitcoin different? How secure are your Bitcoins? How anonymous are Bitcoin users? What determines the price of Bitcoins? Can cryptocurrencies be regulated? What might the future hold? After this course, you\u2019ll know everything you need to be able to separate fact from fiction when reading claims about Bitcoin and other cryptocurrencies. You\u2019ll have the conceptual foundations you need to engineer secure software that interacts with the Bitcoin network. And you\u2019ll be able to integrate ideas from Bitcoin in your own projects. Course Lecturers: Arvind Narayanan, Princeton University Lessons: Cryptographic hash functions Digital signature Hash pointers Centralization and decentralization Distributed consensus","title":"Bitcoin and cryptocurrency technologies"},{"location":"bitcoin-and-cryptocurrency-technologies/centralization-vs-decentralization/","text":"Centralization vs Decentralization \u00b6 Usually, in complex system, decentralization is not all-or-nothing (e.g. the email protocol is decentralized but there are private email providers). Aspects of decentralization in cryptocurrencies \u00b6 Who maintains the ledger? Who has authority over transactions validity? Who creates new coins ? Who determines how the rules of the system change? How does the currency acquire exchange value? Note that the cryptocurrencies protocols usually are decentralized but services built around them may be centralized (e.g. exchanges). Aspects of decentralization in Bitcoin \u00b6 Bitcoin is based on a p2p network: anybody is allowed to join the network. Also, the barrier to entry is really low. Mining: anyone is allowed to mine Bitcoins, but in this case the power concentrates in few entities in the network. Software updates: Bitcoin core developers are trusted by the whole community and thus they have a centralized power over the network.","title":"Centralization and decentralization"},{"location":"bitcoin-and-cryptocurrency-technologies/centralization-vs-decentralization/#centralization-vs-decentralization","text":"Usually, in complex system, decentralization is not all-or-nothing (e.g. the email protocol is decentralized but there are private email providers).","title":"Centralization vs Decentralization"},{"location":"bitcoin-and-cryptocurrency-technologies/centralization-vs-decentralization/#aspects-of-decentralization-in-cryptocurrencies","text":"Who maintains the ledger? Who has authority over transactions validity? Who creates new coins ? Who determines how the rules of the system change? How does the currency acquire exchange value? Note that the cryptocurrencies protocols usually are decentralized but services built around them may be centralized (e.g. exchanges).","title":"Aspects of decentralization in cryptocurrencies"},{"location":"bitcoin-and-cryptocurrency-technologies/centralization-vs-decentralization/#aspects-of-decentralization-in-bitcoin","text":"Bitcoin is based on a p2p network: anybody is allowed to join the network. Also, the barrier to entry is really low. Mining: anyone is allowed to mine Bitcoins, but in this case the power concentrates in few entities in the network. Software updates: Bitcoin core developers are trusted by the whole community and thus they have a centralized power over the network.","title":"Aspects of decentralization in Bitcoin"},{"location":"bitcoin-and-cryptocurrency-technologies/crypto-hash-functions/","text":"Hash function \u00b6 A function that maps a string to a fixed size output. H(x): x string -> fixed size output Properties \u00b6 Collision-free \u00b6 It's hard to find a collision. Note that no hash function has ever been formally proved to be collision-free. Application \u00b6 Message digest: if H(x) = H(y) then it's safe to assume that x = y . This means that hash functions can help verify the integrity of documents without scanning the whole document. Hiding \u00b6 Given H(x) , it's hard to find x . Application \u00b6 Commitment problem: we want to commit to value and reveal it later to an audience. If we hash that value, thanks to this property, we know that it will be hard for attackers to correctly guess it before we chose to disclose it. While our audience also knows that we wouldn't be able to change the committed value because hash functions are collision-free: it would be infeasible to generate a collision with the value we picked. Puzzle-friendly \u00b6 Given a puzzle id and a target set Y , try to find a solution x such that H(id | x) is in Y . Puzzle-friendly means that, for the stated problem, no solving strategy is much better that trying random values of x. Application \u00b6 Crypto puzzles: puzzles that can be used as proof of work. These puzzles are used in blockchain based coin technologies such as Bitcoin. Hash function examples \u00b6 SHA-256 \u00b6 A popular hash function. A high level description of SHA-256 is given by the following diagram:","title":"Cryptographic hash functions"},{"location":"bitcoin-and-cryptocurrency-technologies/crypto-hash-functions/#hash-function","text":"A function that maps a string to a fixed size output. H(x): x string -> fixed size output","title":"Hash function"},{"location":"bitcoin-and-cryptocurrency-technologies/crypto-hash-functions/#properties","text":"","title":"Properties"},{"location":"bitcoin-and-cryptocurrency-technologies/crypto-hash-functions/#collision-free","text":"It's hard to find a collision. Note that no hash function has ever been formally proved to be collision-free.","title":"Collision-free"},{"location":"bitcoin-and-cryptocurrency-technologies/crypto-hash-functions/#application","text":"Message digest: if H(x) = H(y) then it's safe to assume that x = y . This means that hash functions can help verify the integrity of documents without scanning the whole document.","title":"Application"},{"location":"bitcoin-and-cryptocurrency-technologies/crypto-hash-functions/#hiding","text":"Given H(x) , it's hard to find x .","title":"Hiding"},{"location":"bitcoin-and-cryptocurrency-technologies/crypto-hash-functions/#application_1","text":"Commitment problem: we want to commit to value and reveal it later to an audience. If we hash that value, thanks to this property, we know that it will be hard for attackers to correctly guess it before we chose to disclose it. While our audience also knows that we wouldn't be able to change the committed value because hash functions are collision-free: it would be infeasible to generate a collision with the value we picked.","title":"Application"},{"location":"bitcoin-and-cryptocurrency-technologies/crypto-hash-functions/#puzzle-friendly","text":"Given a puzzle id and a target set Y , try to find a solution x such that H(id | x) is in Y . Puzzle-friendly means that, for the stated problem, no solving strategy is much better that trying random values of x.","title":"Puzzle-friendly"},{"location":"bitcoin-and-cryptocurrency-technologies/crypto-hash-functions/#application_2","text":"Crypto puzzles: puzzles that can be used as proof of work. These puzzles are used in blockchain based coin technologies such as Bitcoin.","title":"Application"},{"location":"bitcoin-and-cryptocurrency-technologies/crypto-hash-functions/#hash-function-examples","text":"","title":"Hash function examples"},{"location":"bitcoin-and-cryptocurrency-technologies/crypto-hash-functions/#sha-256","text":"A popular hash function. A high level description of SHA-256 is given by the following diagram:","title":"SHA-256"},{"location":"bitcoin-and-cryptocurrency-technologies/digital-signature/","text":"Digital signature \u00b6 Digital signatures must verify 2 properties: Only you can sign some data, while anyone can verify the fact that you signed it. The signature must be specific to the data that it signs: if it isn't, anyone can just copy the signature you shared and apply it to different documents. Implementation \u00b6 Digital signature schemes use a public key and a private key : The private key is used to sign data The public key is used to verify signed data Digital signature schemes must guarantee that signed data is always correctly verified. Unforgeable signature schemes \u00b6 When is a signature scheme called unforgeable? Let's consider this game: There is an attacker who knows the public key and a challenger who knows the private key too. The attacker can pick a document and get the challenger to sign it. The challenger will sign that document and send the signed data to the attacker. The game can go on as for as much as the attacker wants (at least until a plausible amount of documents is signed) Then the attacker tries to sign a message that the challenger has not already signed: if the forged message verifies correctly then the attacker wins, else the challenger wins. So a signature scheme is unforgeable if, not matter what algorithm the attacker is using, he has only a slim chance to succeed. Use cases \u00b6 Public keys can be used as identities Signature schemes can be used to sign the last hash pointer in a blockchain, thus signing the whole blockchain. Signature scheme used in Bitcoin \u00b6 Bitcoin uses ECDSA. Note that a good randomness source is essential to avoid leaking your private key using your public key.","title":"Digital signature"},{"location":"bitcoin-and-cryptocurrency-technologies/digital-signature/#digital-signature","text":"Digital signatures must verify 2 properties: Only you can sign some data, while anyone can verify the fact that you signed it. The signature must be specific to the data that it signs: if it isn't, anyone can just copy the signature you shared and apply it to different documents.","title":"Digital signature"},{"location":"bitcoin-and-cryptocurrency-technologies/digital-signature/#implementation","text":"Digital signature schemes use a public key and a private key : The private key is used to sign data The public key is used to verify signed data Digital signature schemes must guarantee that signed data is always correctly verified.","title":"Implementation"},{"location":"bitcoin-and-cryptocurrency-technologies/digital-signature/#unforgeable-signature-schemes","text":"When is a signature scheme called unforgeable? Let's consider this game: There is an attacker who knows the public key and a challenger who knows the private key too. The attacker can pick a document and get the challenger to sign it. The challenger will sign that document and send the signed data to the attacker. The game can go on as for as much as the attacker wants (at least until a plausible amount of documents is signed) Then the attacker tries to sign a message that the challenger has not already signed: if the forged message verifies correctly then the attacker wins, else the challenger wins. So a signature scheme is unforgeable if, not matter what algorithm the attacker is using, he has only a slim chance to succeed.","title":"Unforgeable signature schemes"},{"location":"bitcoin-and-cryptocurrency-technologies/digital-signature/#use-cases","text":"Public keys can be used as identities Signature schemes can be used to sign the last hash pointer in a blockchain, thus signing the whole blockchain.","title":"Use cases"},{"location":"bitcoin-and-cryptocurrency-technologies/digital-signature/#signature-scheme-used-in-bitcoin","text":"Bitcoin uses ECDSA. Note that a good randomness source is essential to avoid leaking your private key using your public key.","title":"Signature scheme used in Bitcoin"},{"location":"bitcoin-and-cryptocurrency-technologies/distributed-consensus/","text":"Distributed consensus \u00b6 A key challenge of distributed systems is achieving distributed consensus , because it is required for reliability in the system. As example, consider a distributed database: if sometimes consensus is not achieved then some databases will not be consistent with the others. Definition \u00b6 Let's suppose there is a network with N nodes. Each node receives an input value. Consensus in the network happens if: The consensus protocol terminates. All N nodes decide on the same value. The decided value must be one of the input values. Consensus in Bitcoin \u00b6 Let's examine what happens when Alice wants to pay Bob some bitcoins: Alice signs the transaction referencing Bob's public key. The transactions contain the hash pointing to previously received coins by Alice. Alice broadcasts the transaction to the whole network. If Bob wants to be notified of the transaction, he might run a Bitcoin node. But his listening is not required for him to receive coins. The network will acknowledge (if valid) the transaction nonetheless. It is really important that the network reaches consensus on the validity and ordering of transactions if we want the whole system to work. But we cannot solve this problem with an algorithm that has the consensus properties described before, because: Nodes may crash Nodes may be malicious A p2p network is imperfect by nature (e.g. not all nodes are connected, there are faults, there is latency because the network has no notion of global time, etc.) Also, the literature on distributed consensus is pessimistic, presenting several impossibility results (e.g. Byzantine generals problem ) Still, there exist algorithms for achieving distributed consensus that trade off some properties with others (e.g. Paxos ). But note that the hypotheses under which impossible results were proved are not applicable to the Bitcoin network. In fact, distributed consensus works better in practice than in theory for Bitcoin, because: The idea of incentive is introduced Consensus happens over long periods of time (1h usually), not in fixed periods. As time goes on, the probability that an invalid transaction is considered valid decreases exponentially. So Bitcoins solves the distributed consensus problem with a probabilistic approach.","title":"Distributed consensus"},{"location":"bitcoin-and-cryptocurrency-technologies/distributed-consensus/#distributed-consensus","text":"A key challenge of distributed systems is achieving distributed consensus , because it is required for reliability in the system. As example, consider a distributed database: if sometimes consensus is not achieved then some databases will not be consistent with the others.","title":"Distributed consensus"},{"location":"bitcoin-and-cryptocurrency-technologies/distributed-consensus/#definition","text":"Let's suppose there is a network with N nodes. Each node receives an input value. Consensus in the network happens if: The consensus protocol terminates. All N nodes decide on the same value. The decided value must be one of the input values.","title":"Definition"},{"location":"bitcoin-and-cryptocurrency-technologies/distributed-consensus/#consensus-in-bitcoin","text":"Let's examine what happens when Alice wants to pay Bob some bitcoins: Alice signs the transaction referencing Bob's public key. The transactions contain the hash pointing to previously received coins by Alice. Alice broadcasts the transaction to the whole network. If Bob wants to be notified of the transaction, he might run a Bitcoin node. But his listening is not required for him to receive coins. The network will acknowledge (if valid) the transaction nonetheless. It is really important that the network reaches consensus on the validity and ordering of transactions if we want the whole system to work. But we cannot solve this problem with an algorithm that has the consensus properties described before, because: Nodes may crash Nodes may be malicious A p2p network is imperfect by nature (e.g. not all nodes are connected, there are faults, there is latency because the network has no notion of global time, etc.) Also, the literature on distributed consensus is pessimistic, presenting several impossibility results (e.g. Byzantine generals problem ) Still, there exist algorithms for achieving distributed consensus that trade off some properties with others (e.g. Paxos ). But note that the hypotheses under which impossible results were proved are not applicable to the Bitcoin network. In fact, distributed consensus works better in practice than in theory for Bitcoin, because: The idea of incentive is introduced Consensus happens over long periods of time (1h usually), not in fixed periods. As time goes on, the probability that an invalid transaction is considered valid decreases exponentially. So Bitcoins solves the distributed consensus problem with a probabilistic approach.","title":"Consensus in Bitcoin"},{"location":"bitcoin-and-cryptocurrency-technologies/hash-pointers-and-data-structures/","text":"Hash pointer \u00b6 A hash pointer consists of 2 informations: Address where some info is stored Hash of that info Hash pointers can be used in every non-cyclical data structures that uses pointers. Blockchain \u00b6 A blockchain is a list of linked records, called blocks. Each block contains a cryptographic hash of the previous block. Use cases \u00b6 A blockchain can be used as a tamper evident log. For example, in Bitcoin, a blockchain logs all the transactions (organized in blocks) approved by the network. Merkle tree \u00b6 A Merkle tree is a tamper evident binary tree structure. The following diagram explains how to build a Merkle tree starting from a known amount of data blocks: A Merkle tree needs to show log(N) items to provide proof of membership for a given data block. The time complexity of this operation is log(N) too. Use cases \u00b6 Merkle trees can be used to give informations about a sequence of transactions without needing the data of all the transactions in the sequence, while still preventing attackers to easily tamper that data.","title":"Hash pointers"},{"location":"bitcoin-and-cryptocurrency-technologies/hash-pointers-and-data-structures/#hash-pointer","text":"A hash pointer consists of 2 informations: Address where some info is stored Hash of that info Hash pointers can be used in every non-cyclical data structures that uses pointers.","title":"Hash pointer"},{"location":"bitcoin-and-cryptocurrency-technologies/hash-pointers-and-data-structures/#blockchain","text":"A blockchain is a list of linked records, called blocks. Each block contains a cryptographic hash of the previous block.","title":"Blockchain"},{"location":"bitcoin-and-cryptocurrency-technologies/hash-pointers-and-data-structures/#use-cases","text":"A blockchain can be used as a tamper evident log. For example, in Bitcoin, a blockchain logs all the transactions (organized in blocks) approved by the network.","title":"Use cases"},{"location":"bitcoin-and-cryptocurrency-technologies/hash-pointers-and-data-structures/#merkle-tree","text":"A Merkle tree is a tamper evident binary tree structure. The following diagram explains how to build a Merkle tree starting from a known amount of data blocks: A Merkle tree needs to show log(N) items to provide proof of membership for a given data block. The time complexity of this operation is log(N) too.","title":"Merkle tree"},{"location":"bitcoin-and-cryptocurrency-technologies/hash-pointers-and-data-structures/#use-cases_1","text":"Merkle trees can be used to give informations about a sequence of transactions without needing the data of all the transactions in the sequence, while still preventing attackers to easily tamper that data.","title":"Use cases"},{"location":"building-microservices/","text":"Building microservices \u00b6 Notes on the Building Microservices: Designing Fine-Grained Systems book by Sam Newman. About this book: Distributed systems have become more fine-grained in the past 10 years, shifting from code-heavy monolithic applications to smaller, self-contained microservices. But developing these systems brings its own set of headaches. With lots of examples and practical advice, this book takes a holistic view of the topics that system architects and administrators must consider when building, managing, and evolving microservice architectures. Microservice technologies are moving quickly. Author Sam Newman provides you with a firm grounding in the concepts while diving into current solutions for modeling, integrating, testing, deploying, and monitoring your own autonomous services. You\u2019ll follow a fictional company throughout the book to learn how building a microservice architecture affects a single domain. Chapters: Microservices Evolutionary architects How to model services Integration Splitting the monolith Deployment Testing Monitoring Security Conway\u2019s Law and System Design Microservices at Scale Bringing it all together","title":"Home"},{"location":"building-microservices/#building-microservices","text":"Notes on the Building Microservices: Designing Fine-Grained Systems book by Sam Newman. About this book: Distributed systems have become more fine-grained in the past 10 years, shifting from code-heavy monolithic applications to smaller, self-contained microservices. But developing these systems brings its own set of headaches. With lots of examples and practical advice, this book takes a holistic view of the topics that system architects and administrators must consider when building, managing, and evolving microservice architectures. Microservice technologies are moving quickly. Author Sam Newman provides you with a firm grounding in the concepts while diving into current solutions for modeling, integrating, testing, deploying, and monitoring your own autonomous services. You\u2019ll follow a fictional company throughout the book to learn how building a microservice architecture affects a single domain. Chapters: Microservices Evolutionary architects How to model services Integration Splitting the monolith Deployment Testing Monitoring Security Conway\u2019s Law and System Design Microservices at Scale Bringing it all together","title":"Building microservices"},{"location":"building-microservices/conclusion/","text":"Bringing it all together \u00b6 This diagram summarizes the principles of microservices:","title":"Bringing It All Together"},{"location":"building-microservices/conclusion/#bringing-it-all-together","text":"This diagram summarizes the principles of microservices:","title":"Bringing it all together"},{"location":"building-microservices/conway/","text":"Conway\u2019s Law and System Design \u00b6 Melvin Conway\u2019s paper How Do Committees Invent (April 1968) observed that: Any organization that designs a system (defined more broadly here than just information systems) will inevitably produce a design whose structure is a copy of the organization\u2019s communication structure. This idea can be summarized as \u201cIf you have four groups working on a compiler, you\u2019ll get a 4-pass compiler.\u201d Evidence \u00b6 Various studies have found supporting evidence for this claim. You can read more on Wikipedia . Some examples from the IT industry: Amazon conceived its two-pizza teams from this idea. This organizational structure mostly is what drove the creation of AWS. Netflix designed the organizational structure for the system architecture it wanted. Small teams allowed for independent services. Applications of Conway's Law \u00b6 Let's examine 3 cases: Single team owns a single service (i.e. multiple teams own different services). Here takes place fine-grained communication, which suits well the nature of software communication inside the service's boundaries. Team communication is fast-paced just like function calls. Following Conway's Law, the outcome will be an efficient system which is isolated from external services because communications is harder between different teams. Single team owns multiple services. Here takes place fine-grained communication, so the services might end up being coupled. Multiple teams own the same service. Here takes place coarse-grained communication, so the development process will be inefficient and the service's code unnecessarily abstract and/or complex. Service Ownership \u00b6 Having one team responsible for deploying and maintaining the application means it has an incentive to create services that are easy to deploy. There will be no one else to catch the code if the team wants to throw it over the wall . Some factors that drive away from the ideal service ownership model: High cost of splitting a service may make multiple teams work on the same service. Try to gradually split the service. Feature teams own the same service but work on separate feature. This approach bases the organization on the technical model (i.e. UI, database, etc.). It's an approach to avoid because microservices are by nature to be aligned with the domain model, not the technical one. Another reason to avoid it is that, in the end, no team will end up having clear ownership of anything; this gives space to a lot of blaming. Delivery bottlenecks may make multiple teams work on the same service. This can be solved by temporarily adding a new member to the overloaded team or by splitting the service if the feature load is really high and it's expected to be kept up or increased in the future. If, for some reason, it's unavoidable having shared services, we can adopt the internal open source model. In this model, a service is owned by a core team of trusted committers that review changes requested by untrusted committers. Summary \u00b6 Conway\u2019s law highlights the perils of trying to enforce a system design that doesn\u2019t match the organization.","title":"Conway's Law and System Desing"},{"location":"building-microservices/conway/#conways-law-and-system-design","text":"Melvin Conway\u2019s paper How Do Committees Invent (April 1968) observed that: Any organization that designs a system (defined more broadly here than just information systems) will inevitably produce a design whose structure is a copy of the organization\u2019s communication structure. This idea can be summarized as \u201cIf you have four groups working on a compiler, you\u2019ll get a 4-pass compiler.\u201d","title":"Conway\u2019s Law and System Design"},{"location":"building-microservices/conway/#evidence","text":"Various studies have found supporting evidence for this claim. You can read more on Wikipedia . Some examples from the IT industry: Amazon conceived its two-pizza teams from this idea. This organizational structure mostly is what drove the creation of AWS. Netflix designed the organizational structure for the system architecture it wanted. Small teams allowed for independent services.","title":"Evidence"},{"location":"building-microservices/conway/#applications-of-conways-law","text":"Let's examine 3 cases: Single team owns a single service (i.e. multiple teams own different services). Here takes place fine-grained communication, which suits well the nature of software communication inside the service's boundaries. Team communication is fast-paced just like function calls. Following Conway's Law, the outcome will be an efficient system which is isolated from external services because communications is harder between different teams. Single team owns multiple services. Here takes place fine-grained communication, so the services might end up being coupled. Multiple teams own the same service. Here takes place coarse-grained communication, so the development process will be inefficient and the service's code unnecessarily abstract and/or complex.","title":"Applications of Conway's Law"},{"location":"building-microservices/conway/#service-ownership","text":"Having one team responsible for deploying and maintaining the application means it has an incentive to create services that are easy to deploy. There will be no one else to catch the code if the team wants to throw it over the wall . Some factors that drive away from the ideal service ownership model: High cost of splitting a service may make multiple teams work on the same service. Try to gradually split the service. Feature teams own the same service but work on separate feature. This approach bases the organization on the technical model (i.e. UI, database, etc.). It's an approach to avoid because microservices are by nature to be aligned with the domain model, not the technical one. Another reason to avoid it is that, in the end, no team will end up having clear ownership of anything; this gives space to a lot of blaming. Delivery bottlenecks may make multiple teams work on the same service. This can be solved by temporarily adding a new member to the overloaded team or by splitting the service if the feature load is really high and it's expected to be kept up or increased in the future. If, for some reason, it's unavoidable having shared services, we can adopt the internal open source model. In this model, a service is owned by a core team of trusted committers that review changes requested by untrusted committers.","title":"Service Ownership"},{"location":"building-microservices/conway/#summary","text":"Conway\u2019s law highlights the perils of trying to enforce a system design that doesn\u2019t match the organization.","title":"Summary"},{"location":"building-microservices/deployment/","text":"Deployment \u00b6 Deployment in microservices differs from monolithic systems. It's important to have a working approach following the continuous integration and delivery practices. Mapping Continuous Integration to Microservices \u00b6 The goal is to be able to deploy microservices independently. So, how to map microservices to builds and code repositories? We have different options: Single repository and single build \u00b6 Use a single repository to store all our code, and have a single build, triggered on every code integration, that produces every build artifact we need. Benefits: Easy to implement. Easy to commit changes. Downsides: A small change to a single service will trigger builds we do not need. What services do we need to deploy? Hard to determine which services changed by only looking at the pushed commit. If a commit breaks the build, the build needs to be fixed before any other team can push code in the repository, locking those teams. Single repository and multiple builds \u00b6 A variation of the previous approach is to have a single repository but setup multiple CI builds mapping to parts of the source code. Benefits: Easy to commit changes. Downsides: Developers can get into the habit of making changes to different services in the same commit. Multiple repositories and multiple builds \u00b6 Each microservice has its own repository and CI build. Benefits: Only needed builds and tests are run when the build is triggered. A team can own the repository that it's working on. Downsides: Making changes across microservices is more difficult, but this is still preferable to the single repository approach. Build Pipelines and Continuous Delivery \u00b6 In build processes usually there are a lot of fast, small-scoped tests, and a few large-scoped, slow tests. We will not get fast feedback when our fast tests fail if we're waiting for the other tests to finish. Also, if the fast tests fail, there is no need to run other tests. A solution to this problem is to have different stages in our build, i.e. a build pipeline . Build pipelines allow to track the software as it goes through each build stage, giving a clear idea of its stability. In continuous delivery ( CD ) we get constant feedback on the production readiness of each and every check-in, and treat each and every check-in as a release candidate. So clearly CD benefits from build pipelines. In microservices with CI, we want one pipeline per service, in which a single artifact will move through our path to production. Exceptions to Continuous Delivery \u00b6 In the starting stage of a project, a single repository and single build approach may be more convenient since developers are not confident with the domain yet because the service boundaries are likely to change a lot. In this case, having a multi repository model will increase a lot the cost of these changes. Then, when the development team acquires experience in the domain, it can start moving out services in their own repositories and build pipelines. Platform-Specific Artifacts \u00b6 Some artifacts are platform-specific (e.g. JAR files). This means that they need a specific configuration and a specific platform to be run in an environment. Tools like Puppet and Chef can help to automate this process. Operating System Artifacts \u00b6 Another option for platform-specific artifacts is to use os-specific artifacts. This way, the OS can manage dependencies, installation and removal of your software. The downside is in actually creating these packages, because the difficulty depends on the target OS (e.g. teams using Windows, not known for package management capabilities, may be unhappy with this approach). Another downside is that if you need to deploy on different OS, there is an increase in complexity in your build and test process. Custom images \u00b6 The problem with tools like Puppet and Chef is that they take time to provision a machine . They need to install platforms (e.g. JVM) or perform expensive checks on the system to detect if a valid platform version is already installed. And if we're using an on-demand compute platform we might be constantly shutting down and spinning up new instances frequently, making the time cost of these tools really high. If you need to install the same tools multiple times per day (e.g. because of CI) this becomes a real problem in terms of providing fast feedback. It can also lead to increased downtime when deploying in production if your systems do not allow zero-downtime deployment ( blue/green deployment can help mitigate this issue). One approach to reducing the provisioning time is to create a virtual machine image that bakes in some common dependencies we use. When we want to deploy our software, we spin up an instance of this custom image, and all we have to do is install the latest version of our service. When you launch new copies of this image you don't need to spend time installing your dependencies, as they are already there. This can result in significant time savings. There are drawbacks too: Build times are increased. Resulting images can be very large, making it hard to move them across the network. The image build process differs from platform to platform (e.g. VMWare images, Vagrant images). Tools like Packer can help. As we'll see later, container technology mitigates these drawbacks. Images as Artifacts \u00b6 Why stop at including only dependencies in these images? We can also include our software in it. This will make our software platform agnostic and it is a good way to start implementing the immutable server deployment concept. Immutable Servers \u00b6 To keep our servers immutable we also must be sure that no one is able to access them after they've been deployed (e.g. by disabling SSH in the image artifact). Otherwise, the configuration could be edited, causing a configuration drift . If we want to have environments that are easy to reason about, every configuration change must pass through a build pipeline. Environments \u00b6 Our microservice artifact will move in different environments during the CD pipeline. Usually these are: Slow tests environment. UAT environment. Performance/load test environment. Production environment. As you go on in the pipeline, you want the environments to look more like the production environment, allowing us to catch production problems before they happen in production. But consider that production environments are more expensive and slower to set up. So you should balance the ability to find production-like bugs with the ability to get fast feedback from builds. Service configuration \u00b6 Our services need some configuration (e.g. db username and password). Ideally this should be a small amount of data. Also, it's best to minimize configuration that changes between environments, so that you minimize chances for environment-specific bugs. But how to handle this kind of configuration? Bundling the configuration in your build artifacts is to be avoided because it violates the principles of CD . In this case it would be hard to avoid having sensitive data (e.g. passwords) in your source code. Also, build times are increased since you now have more images. Then you have to know at build time which environments exist, coupling the build process with the delivery process. Create a single artifact and place configuration files in environments or use a dedicated system for providing configuration (a popular approach in microservices). Service-to-Host Mapping \u00b6 In this era of virtualization, the mapping between a single host running an operating system and the underlying physical infrastructure can vary a lot. Let's define host to be the generic unit of isolation, i.e. an operating system onto which you can install and run your services. So how many services per host should we have? There are different options. Multiple Services Per Host \u00b6 Having multiple instances of your service per host. Benefits: Simpler work for the team that manages the infrastructure. Using host virtualization can add overhead and thus increase costs. Easier for developers to deploy: a deploy with this setup works like a deploy to a dev machine. Downsides: Make monitoring more difficult (e.g. monitor the host CPU usage or each instance?). Causes side effects (e.g. when a service is under heavy load, it's likely some other service instances will slow down too). Need to ensure that a service deployment does not affect other services on the same host. Usually this is solved by deploying all service in one step, thus losing ability to deploy independently. Autonomy of teams is inhibited in case services of different teams are deployed to the same host. Cannot deploy images and immutable servers. It can be complicated to target scaling at a service in a host. If a service handles sensitive data or has different needs (e.g. another network segment), you cannot deploy it with the others. Application Containers \u00b6 Use an application container (e.g. IIS or Java servlet container) that provides utilities such as management, monitoring and scaling of services. Benefits: Has too for managing monitoring, scaling and other aspects. If all services require the same runtime, this approach reduces overhead (e.g. for n Java services only a single JVM instance is needed). Downsides: Technology choice and tools that automate services management are constrained. Losing automation here means having to do a lot of work in managing services. Usually slow spin-up times, slowing feedback for developers. Analyzing resources use is hard, as you have multiple applications sharing a single process. Application containers have their own resource consumption overhead. Single Service per Host \u00b6 A host contains only a single service. Benefits: Easier to monitor resources usage. Easier to avoid the side effects of having multiple services in a single host. Reduces complexity of your system. Downsides: More hosts mean more servers to manage and costs might increase. You can mitigate the complexity of managing more hosts by using a platform as a service (PaaS). This way, the host management problem is simplified, but you lose control over your hosts. Tip: some PaaS try to automate too much (e.g. automate scaling), making them less effective for your specific use case. Automation \u00b6 Automation is the solution to many of the problems we have raised so far. One of the pushbacks for switching to single service per host is the perception that the amount of overhead for management will increase. If you do everything manually, it surely will, but automation will prevent this issue. Automation also allow developers to be productive, especially if they have access to the same technologies used in production because it will help catch bugs early on. Embracing a culture of automation is key if you want to keep the complexities of microservice architectures in check. From Physical to Virtual \u00b6 One of the key tools available to us in managing many hosts is finding ways of chunking up existing physical machines into smaller parts. Traditional Virtualization \u00b6 Having lots of hosts can be really expensive if you need a physical server per host. By virtualizing you can split a physical machine in separate parts but of course this comes with an overhead. For example, in Type 2 virtualization, the hypervisor sets aside resources for each virtual machine it manages, but these resources could be used for something else instead of being idle and reserved. Vagrant \u00b6 A deployment platform usually employed for development and testing. It allows us to define instructions about how to setup and configure VMs. This makes it easier for you to create production-like environments on your local machine. One of the downsides is that if we have one service to one VM, you may not be able to bring up your entire system on your local machine. Linux containers \u00b6 Linux containers, instead of using an hypervisor, create a separate process space in which other processes live. Each container is effectively a subtree of the overall system process tree. These containers can have physical resources allocated to them, something the kernel handles for us. Benefits: No need for an hypervisor. Much faster to provision than traditional VMs. Finer-grained control over assignation of resources. Since they are lighter than VMs, we can have more containers running on the same host. Downsides: The host OS has to share the same kernel with the base OS. Not as isolated from other processes as VMs, not suitable for running code you don't trust. How to expose containers to the outer world? A specific network configuration is needed, something that is usually provided by hypervisors. Docker \u00b6 Docker is a platform built on top of lightweight containers. Docker manages the container provisioning, handles some networking problems and provides its own registry that allows you to store and version Docker applications. Docker can also alleviate some downsides of running lots of services locally for dev and test purposes, in a more efficient way than Vagrant. Several technologies are build around the Docker concepts, such as CoreOS , a stripped-down Linux OS that provides only the essential services to allow Docker to run. Docker itself doesn\u2019t solve all problems for us. Think of it as a simple PaaS that works on a single machine. If you want tools to help you manage services across multiple Docker instances across multiple machines, you\u2019ll need to look at software such as Kubernetes or CoreOS. A Deployment Interface \u00b6 Whatever underlying platform or artifacts you use, having a uniform interface to deploy a given service is vital to easily deploy microservices to development, test, production and other environments. A good way to trigger deployments is via CLI tools, because it can be triggered by other scripts, used in CI and called manually. We need some information for a deploy: What microservice we want to deploy. What version of said microservice we want to deploy. What environment we want our microservice deployed into. For this to work, we need to define in some way what our environments look like. YAML could be a good way of expressing our environments definitions. Summary \u00b6 Main points collected in this chapter: Maintain the ability to deploy microservices independently. Separate source code and CI builds for each microservices. Use a single-service per host/container model. Evaluate the tooling aiming for high levels of automation. Understand how deployment choices affects developers. Creating tools that make it easy to deploy to different environments helps a lot.","title":"Deployment"},{"location":"building-microservices/deployment/#deployment","text":"Deployment in microservices differs from monolithic systems. It's important to have a working approach following the continuous integration and delivery practices.","title":"Deployment"},{"location":"building-microservices/deployment/#mapping-continuous-integration-to-microservices","text":"The goal is to be able to deploy microservices independently. So, how to map microservices to builds and code repositories? We have different options:","title":"Mapping Continuous Integration to Microservices"},{"location":"building-microservices/deployment/#single-repository-and-single-build","text":"Use a single repository to store all our code, and have a single build, triggered on every code integration, that produces every build artifact we need. Benefits: Easy to implement. Easy to commit changes. Downsides: A small change to a single service will trigger builds we do not need. What services do we need to deploy? Hard to determine which services changed by only looking at the pushed commit. If a commit breaks the build, the build needs to be fixed before any other team can push code in the repository, locking those teams.","title":"Single repository and single build"},{"location":"building-microservices/deployment/#single-repository-and-multiple-builds","text":"A variation of the previous approach is to have a single repository but setup multiple CI builds mapping to parts of the source code. Benefits: Easy to commit changes. Downsides: Developers can get into the habit of making changes to different services in the same commit.","title":"Single repository and multiple builds"},{"location":"building-microservices/deployment/#multiple-repositories-and-multiple-builds","text":"Each microservice has its own repository and CI build. Benefits: Only needed builds and tests are run when the build is triggered. A team can own the repository that it's working on. Downsides: Making changes across microservices is more difficult, but this is still preferable to the single repository approach.","title":"Multiple repositories and multiple builds"},{"location":"building-microservices/deployment/#build-pipelines-and-continuous-delivery","text":"In build processes usually there are a lot of fast, small-scoped tests, and a few large-scoped, slow tests. We will not get fast feedback when our fast tests fail if we're waiting for the other tests to finish. Also, if the fast tests fail, there is no need to run other tests. A solution to this problem is to have different stages in our build, i.e. a build pipeline . Build pipelines allow to track the software as it goes through each build stage, giving a clear idea of its stability. In continuous delivery ( CD ) we get constant feedback on the production readiness of each and every check-in, and treat each and every check-in as a release candidate. So clearly CD benefits from build pipelines. In microservices with CI, we want one pipeline per service, in which a single artifact will move through our path to production.","title":"Build Pipelines and Continuous Delivery"},{"location":"building-microservices/deployment/#exceptions-to-continuous-delivery","text":"In the starting stage of a project, a single repository and single build approach may be more convenient since developers are not confident with the domain yet because the service boundaries are likely to change a lot. In this case, having a multi repository model will increase a lot the cost of these changes. Then, when the development team acquires experience in the domain, it can start moving out services in their own repositories and build pipelines.","title":"Exceptions to Continuous Delivery"},{"location":"building-microservices/deployment/#platform-specific-artifacts","text":"Some artifacts are platform-specific (e.g. JAR files). This means that they need a specific configuration and a specific platform to be run in an environment. Tools like Puppet and Chef can help to automate this process.","title":"Platform-Specific Artifacts"},{"location":"building-microservices/deployment/#operating-system-artifacts","text":"Another option for platform-specific artifacts is to use os-specific artifacts. This way, the OS can manage dependencies, installation and removal of your software. The downside is in actually creating these packages, because the difficulty depends on the target OS (e.g. teams using Windows, not known for package management capabilities, may be unhappy with this approach). Another downside is that if you need to deploy on different OS, there is an increase in complexity in your build and test process.","title":"Operating System Artifacts"},{"location":"building-microservices/deployment/#custom-images","text":"The problem with tools like Puppet and Chef is that they take time to provision a machine . They need to install platforms (e.g. JVM) or perform expensive checks on the system to detect if a valid platform version is already installed. And if we're using an on-demand compute platform we might be constantly shutting down and spinning up new instances frequently, making the time cost of these tools really high. If you need to install the same tools multiple times per day (e.g. because of CI) this becomes a real problem in terms of providing fast feedback. It can also lead to increased downtime when deploying in production if your systems do not allow zero-downtime deployment ( blue/green deployment can help mitigate this issue). One approach to reducing the provisioning time is to create a virtual machine image that bakes in some common dependencies we use. When we want to deploy our software, we spin up an instance of this custom image, and all we have to do is install the latest version of our service. When you launch new copies of this image you don't need to spend time installing your dependencies, as they are already there. This can result in significant time savings. There are drawbacks too: Build times are increased. Resulting images can be very large, making it hard to move them across the network. The image build process differs from platform to platform (e.g. VMWare images, Vagrant images). Tools like Packer can help. As we'll see later, container technology mitigates these drawbacks.","title":"Custom images"},{"location":"building-microservices/deployment/#images-as-artifacts","text":"Why stop at including only dependencies in these images? We can also include our software in it. This will make our software platform agnostic and it is a good way to start implementing the immutable server deployment concept.","title":"Images as Artifacts"},{"location":"building-microservices/deployment/#immutable-servers","text":"To keep our servers immutable we also must be sure that no one is able to access them after they've been deployed (e.g. by disabling SSH in the image artifact). Otherwise, the configuration could be edited, causing a configuration drift . If we want to have environments that are easy to reason about, every configuration change must pass through a build pipeline.","title":"Immutable Servers"},{"location":"building-microservices/deployment/#environments","text":"Our microservice artifact will move in different environments during the CD pipeline. Usually these are: Slow tests environment. UAT environment. Performance/load test environment. Production environment. As you go on in the pipeline, you want the environments to look more like the production environment, allowing us to catch production problems before they happen in production. But consider that production environments are more expensive and slower to set up. So you should balance the ability to find production-like bugs with the ability to get fast feedback from builds.","title":"Environments"},{"location":"building-microservices/deployment/#service-configuration","text":"Our services need some configuration (e.g. db username and password). Ideally this should be a small amount of data. Also, it's best to minimize configuration that changes between environments, so that you minimize chances for environment-specific bugs. But how to handle this kind of configuration? Bundling the configuration in your build artifacts is to be avoided because it violates the principles of CD . In this case it would be hard to avoid having sensitive data (e.g. passwords) in your source code. Also, build times are increased since you now have more images. Then you have to know at build time which environments exist, coupling the build process with the delivery process. Create a single artifact and place configuration files in environments or use a dedicated system for providing configuration (a popular approach in microservices).","title":"Service configuration"},{"location":"building-microservices/deployment/#service-to-host-mapping","text":"In this era of virtualization, the mapping between a single host running an operating system and the underlying physical infrastructure can vary a lot. Let's define host to be the generic unit of isolation, i.e. an operating system onto which you can install and run your services. So how many services per host should we have? There are different options.","title":"Service-to-Host Mapping"},{"location":"building-microservices/deployment/#multiple-services-per-host","text":"Having multiple instances of your service per host. Benefits: Simpler work for the team that manages the infrastructure. Using host virtualization can add overhead and thus increase costs. Easier for developers to deploy: a deploy with this setup works like a deploy to a dev machine. Downsides: Make monitoring more difficult (e.g. monitor the host CPU usage or each instance?). Causes side effects (e.g. when a service is under heavy load, it's likely some other service instances will slow down too). Need to ensure that a service deployment does not affect other services on the same host. Usually this is solved by deploying all service in one step, thus losing ability to deploy independently. Autonomy of teams is inhibited in case services of different teams are deployed to the same host. Cannot deploy images and immutable servers. It can be complicated to target scaling at a service in a host. If a service handles sensitive data or has different needs (e.g. another network segment), you cannot deploy it with the others.","title":"Multiple Services Per Host"},{"location":"building-microservices/deployment/#application-containers","text":"Use an application container (e.g. IIS or Java servlet container) that provides utilities such as management, monitoring and scaling of services. Benefits: Has too for managing monitoring, scaling and other aspects. If all services require the same runtime, this approach reduces overhead (e.g. for n Java services only a single JVM instance is needed). Downsides: Technology choice and tools that automate services management are constrained. Losing automation here means having to do a lot of work in managing services. Usually slow spin-up times, slowing feedback for developers. Analyzing resources use is hard, as you have multiple applications sharing a single process. Application containers have their own resource consumption overhead.","title":"Application Containers"},{"location":"building-microservices/deployment/#single-service-per-host","text":"A host contains only a single service. Benefits: Easier to monitor resources usage. Easier to avoid the side effects of having multiple services in a single host. Reduces complexity of your system. Downsides: More hosts mean more servers to manage and costs might increase. You can mitigate the complexity of managing more hosts by using a platform as a service (PaaS). This way, the host management problem is simplified, but you lose control over your hosts. Tip: some PaaS try to automate too much (e.g. automate scaling), making them less effective for your specific use case.","title":"Single Service per Host"},{"location":"building-microservices/deployment/#automation","text":"Automation is the solution to many of the problems we have raised so far. One of the pushbacks for switching to single service per host is the perception that the amount of overhead for management will increase. If you do everything manually, it surely will, but automation will prevent this issue. Automation also allow developers to be productive, especially if they have access to the same technologies used in production because it will help catch bugs early on. Embracing a culture of automation is key if you want to keep the complexities of microservice architectures in check.","title":"Automation"},{"location":"building-microservices/deployment/#from-physical-to-virtual","text":"One of the key tools available to us in managing many hosts is finding ways of chunking up existing physical machines into smaller parts.","title":"From Physical to Virtual"},{"location":"building-microservices/deployment/#traditional-virtualization","text":"Having lots of hosts can be really expensive if you need a physical server per host. By virtualizing you can split a physical machine in separate parts but of course this comes with an overhead. For example, in Type 2 virtualization, the hypervisor sets aside resources for each virtual machine it manages, but these resources could be used for something else instead of being idle and reserved.","title":"Traditional Virtualization"},{"location":"building-microservices/deployment/#vagrant","text":"A deployment platform usually employed for development and testing. It allows us to define instructions about how to setup and configure VMs. This makes it easier for you to create production-like environments on your local machine. One of the downsides is that if we have one service to one VM, you may not be able to bring up your entire system on your local machine.","title":"Vagrant"},{"location":"building-microservices/deployment/#linux-containers","text":"Linux containers, instead of using an hypervisor, create a separate process space in which other processes live. Each container is effectively a subtree of the overall system process tree. These containers can have physical resources allocated to them, something the kernel handles for us. Benefits: No need for an hypervisor. Much faster to provision than traditional VMs. Finer-grained control over assignation of resources. Since they are lighter than VMs, we can have more containers running on the same host. Downsides: The host OS has to share the same kernel with the base OS. Not as isolated from other processes as VMs, not suitable for running code you don't trust. How to expose containers to the outer world? A specific network configuration is needed, something that is usually provided by hypervisors.","title":"Linux containers"},{"location":"building-microservices/deployment/#docker","text":"Docker is a platform built on top of lightweight containers. Docker manages the container provisioning, handles some networking problems and provides its own registry that allows you to store and version Docker applications. Docker can also alleviate some downsides of running lots of services locally for dev and test purposes, in a more efficient way than Vagrant. Several technologies are build around the Docker concepts, such as CoreOS , a stripped-down Linux OS that provides only the essential services to allow Docker to run. Docker itself doesn\u2019t solve all problems for us. Think of it as a simple PaaS that works on a single machine. If you want tools to help you manage services across multiple Docker instances across multiple machines, you\u2019ll need to look at software such as Kubernetes or CoreOS.","title":"Docker"},{"location":"building-microservices/deployment/#a-deployment-interface","text":"Whatever underlying platform or artifacts you use, having a uniform interface to deploy a given service is vital to easily deploy microservices to development, test, production and other environments. A good way to trigger deployments is via CLI tools, because it can be triggered by other scripts, used in CI and called manually. We need some information for a deploy: What microservice we want to deploy. What version of said microservice we want to deploy. What environment we want our microservice deployed into. For this to work, we need to define in some way what our environments look like. YAML could be a good way of expressing our environments definitions.","title":"A Deployment Interface"},{"location":"building-microservices/deployment/#summary","text":"Main points collected in this chapter: Maintain the ability to deploy microservices independently. Separate source code and CI builds for each microservices. Use a single-service per host/container model. Evaluate the tooling aiming for high levels of automation. Understand how deployment choices affects developers. Creating tools that make it easy to deploy to different environments helps a lot.","title":"Summary"},{"location":"building-microservices/evolutionary-architects/","text":"Evolutionary architects \u00b6 Architects of microservices based systems need to face difficult choices: Degree of technology unification between microservices Team policies (e.g. allow different teams to use different patterns?) How to merge/split microservices? But to provide effective guidance we must first understand the role of software architects in IT. IT is a young industry that borrowed the architect term from actual architects and engineers but there is a substantial difference in these roles: software is not subject to physical constraints. Software is flexible and can be easily adapted and evolved to new requirements. Architects need to: Set direction in broad strokes (i.e. set software zones), they must be involved in specific implementation details in limited cases Ensure that the system is suitable for the current requirements Ensure that the system can accommodate future requirements Make the system work for both users and developers Understand implementation complexity Tip: an architect should spend some time working on user stories with developers to better understand the state/challenges of the system. A principled approach \u00b6 There are lots of tradeoffs in decisions about microservices based systems. Defining a set of principles and practices can guide us through these choices. Principles are rules made by an architect to align the development activity to larger system goals. An example is the 12 factor app , defined by Heroku to guide the development of scalable cloud SaaS applications. Practices are ways to make sure a principle is followed. Practices can differ when following same principles (e.g. different practices for .NET and Java systems following the same principles). Principles and practices adopted should depend on goals that we want to achieve, also taking into account strategic goals (i.e. the long term goals of your organization). This is what happens in the real world: The required standard \u00b6 One of the core balances to find is how much variability to allow in your system. Too much variability can cause issues such as onboarding problems and other expressed before. One way to identify a standard is to identify the attributes of an ideal microservice. Zoning \u00b6 Our zones are service boundaries or groups of services. As architects, more important to know how services communicate between each other than how a single isolated service works. Many organizations are using microservices to make teams more autonomous, architects then rely on those teams to make local optimal decisions. Still, care is needed for choosing the technologies of single services: sparse technology does not facilitate experience growth and makes it harder for developers to switch teams. Also, care of protocol for communication between microservices, because each ms will need to know how to operate with a certain protocol and this adds complexity. Monitoring \u00b6 It's important to monitor the health of the whole system and gather health and log data in a single place in order to analyze it. Remember to use an agnostic log/health/data reporting protocol/format so your monitoring system does not change as services change. Interfaces \u00b6 Keep interfaces of services as simple as possible, supporting the minimum standards required. This makes it easier to handle versioning and system complexity, because it will be easier to evolve the system. Architectural safety \u00b6 Services need to resist to partial failures in the system. A partial failure should not affect the system as a whole. Governance through code \u00b6 Making sure that developers are implementing the defined standards can be a burden. Exemplars and service templates help a lot with this problem. Exemplars should ideally be real-world services following your standards. Developers can safely look at exemplars to further develop the application. Service templates are a set of technologies or even frameworks to be used in your services. These can guide the developer teams and make their work easier. But be careful: frameworks should not be enforced by an external team and they should be user-friendly. Another danger is that service templates can cause coupling between services. Technical debt \u00b6 Often the technical vision cannot be fully followed through because of business requirements. This is a source of technical debt because a short-term benefit will be paid with a long-term cost. Teams can manage their technical debt or it can be managed by a centralized source. Handling exceptions to the rules \u00b6 Sometimes you will need to build a part of your system while not following some rules of the standard you defined. If you find this happening too often, it can make sense to change you rule set. Summary \u00b6 The following core responsibilities of architects emerged: Vision Empathy Collaboration Adaptability Team autonomy Governance Architects need to constantly balance aspects of their systems to successfully do their job.","title":"Evolutionary architects"},{"location":"building-microservices/evolutionary-architects/#evolutionary-architects","text":"Architects of microservices based systems need to face difficult choices: Degree of technology unification between microservices Team policies (e.g. allow different teams to use different patterns?) How to merge/split microservices? But to provide effective guidance we must first understand the role of software architects in IT. IT is a young industry that borrowed the architect term from actual architects and engineers but there is a substantial difference in these roles: software is not subject to physical constraints. Software is flexible and can be easily adapted and evolved to new requirements. Architects need to: Set direction in broad strokes (i.e. set software zones), they must be involved in specific implementation details in limited cases Ensure that the system is suitable for the current requirements Ensure that the system can accommodate future requirements Make the system work for both users and developers Understand implementation complexity Tip: an architect should spend some time working on user stories with developers to better understand the state/challenges of the system.","title":"Evolutionary architects"},{"location":"building-microservices/evolutionary-architects/#a-principled-approach","text":"There are lots of tradeoffs in decisions about microservices based systems. Defining a set of principles and practices can guide us through these choices. Principles are rules made by an architect to align the development activity to larger system goals. An example is the 12 factor app , defined by Heroku to guide the development of scalable cloud SaaS applications. Practices are ways to make sure a principle is followed. Practices can differ when following same principles (e.g. different practices for .NET and Java systems following the same principles). Principles and practices adopted should depend on goals that we want to achieve, also taking into account strategic goals (i.e. the long term goals of your organization). This is what happens in the real world:","title":"A principled approach"},{"location":"building-microservices/evolutionary-architects/#the-required-standard","text":"One of the core balances to find is how much variability to allow in your system. Too much variability can cause issues such as onboarding problems and other expressed before. One way to identify a standard is to identify the attributes of an ideal microservice.","title":"The required standard"},{"location":"building-microservices/evolutionary-architects/#zoning","text":"Our zones are service boundaries or groups of services. As architects, more important to know how services communicate between each other than how a single isolated service works. Many organizations are using microservices to make teams more autonomous, architects then rely on those teams to make local optimal decisions. Still, care is needed for choosing the technologies of single services: sparse technology does not facilitate experience growth and makes it harder for developers to switch teams. Also, care of protocol for communication between microservices, because each ms will need to know how to operate with a certain protocol and this adds complexity.","title":"Zoning"},{"location":"building-microservices/evolutionary-architects/#monitoring","text":"It's important to monitor the health of the whole system and gather health and log data in a single place in order to analyze it. Remember to use an agnostic log/health/data reporting protocol/format so your monitoring system does not change as services change.","title":"Monitoring"},{"location":"building-microservices/evolutionary-architects/#interfaces","text":"Keep interfaces of services as simple as possible, supporting the minimum standards required. This makes it easier to handle versioning and system complexity, because it will be easier to evolve the system.","title":"Interfaces"},{"location":"building-microservices/evolutionary-architects/#architectural-safety","text":"Services need to resist to partial failures in the system. A partial failure should not affect the system as a whole.","title":"Architectural safety"},{"location":"building-microservices/evolutionary-architects/#governance-through-code","text":"Making sure that developers are implementing the defined standards can be a burden. Exemplars and service templates help a lot with this problem. Exemplars should ideally be real-world services following your standards. Developers can safely look at exemplars to further develop the application. Service templates are a set of technologies or even frameworks to be used in your services. These can guide the developer teams and make their work easier. But be careful: frameworks should not be enforced by an external team and they should be user-friendly. Another danger is that service templates can cause coupling between services.","title":"Governance through code"},{"location":"building-microservices/evolutionary-architects/#technical-debt","text":"Often the technical vision cannot be fully followed through because of business requirements. This is a source of technical debt because a short-term benefit will be paid with a long-term cost. Teams can manage their technical debt or it can be managed by a centralized source.","title":"Technical debt"},{"location":"building-microservices/evolutionary-architects/#handling-exceptions-to-the-rules","text":"Sometimes you will need to build a part of your system while not following some rules of the standard you defined. If you find this happening too often, it can make sense to change you rule set.","title":"Handling exceptions to the rules"},{"location":"building-microservices/evolutionary-architects/#summary","text":"The following core responsibilities of architects emerged: Vision Empathy Collaboration Adaptability Team autonomy Governance Architects need to constantly balance aspects of their systems to successfully do their job.","title":"Summary"},{"location":"building-microservices/how-to-model-services/","text":"How to model services \u00b6 What makes a good service? The whole point of microservices is the ability to deploy them independently, so loose coupling and high cohesion (same kind of logic not distributed across different microservices) are needed. Let's introduce the fictional domain of MusicCorp, an old company who wants to sell music tapes online. The Bounded Context \u00b6 Bounded context is a concept introduced in DDD ( Domain Driven Design ). A bounded context has private and public models relative to a domain context. Only public models are exposed to other contexts. Also, public models can be a different representation of private models (they can be mapped models). This allows for both loose coupling, since there are no references to whole models, and high cohesion, since these contexts are modeled from actual domain contexts. For example, in the MusicCorp online business, Warehouse and Finance are different bounded contexts. Here Stock item is a public model shared by each context which has different representations in each context. The benefits provided by bounded contexts makes them really good candidates to be microservices. But be careful: restructuring bounded contexts has a high cost, so architects must not fall into the premature decomposition trap. If you have a solid understanding and vision of the whole system, your bounded contexts will be solid. Otherwise, you may need to reorganize them frequently. Nesting \u00b6 It's best to think first about coarse-grained bounded contexts. Then these usually can be further divided into subcontexts. Should you keep the nested contexts public or private to the parent context? Usually, if each subcontext has a respective team that handles that area in the organization, it's ideal to make each subcontext public (thus an effective context).","title":"How to model services"},{"location":"building-microservices/how-to-model-services/#how-to-model-services","text":"What makes a good service? The whole point of microservices is the ability to deploy them independently, so loose coupling and high cohesion (same kind of logic not distributed across different microservices) are needed. Let's introduce the fictional domain of MusicCorp, an old company who wants to sell music tapes online.","title":"How to model services"},{"location":"building-microservices/how-to-model-services/#the-bounded-context","text":"Bounded context is a concept introduced in DDD ( Domain Driven Design ). A bounded context has private and public models relative to a domain context. Only public models are exposed to other contexts. Also, public models can be a different representation of private models (they can be mapped models). This allows for both loose coupling, since there are no references to whole models, and high cohesion, since these contexts are modeled from actual domain contexts. For example, in the MusicCorp online business, Warehouse and Finance are different bounded contexts. Here Stock item is a public model shared by each context which has different representations in each context. The benefits provided by bounded contexts makes them really good candidates to be microservices. But be careful: restructuring bounded contexts has a high cost, so architects must not fall into the premature decomposition trap. If you have a solid understanding and vision of the whole system, your bounded contexts will be solid. Otherwise, you may need to reorganize them frequently.","title":"The Bounded Context"},{"location":"building-microservices/how-to-model-services/#nesting","text":"It's best to think first about coarse-grained bounded contexts. Then these usually can be further divided into subcontexts. Should you keep the nested contexts public or private to the parent context? Usually, if each subcontext has a respective team that handles that area in the organization, it's ideal to make each subcontext public (thus an effective context).","title":"Nesting"},{"location":"building-microservices/integration/","text":"Integration \u00b6 Desirable properties of communication between microservices: Avoid breaking changes as much as possible. Technology agnostic APIs. Make it easy to consume APIs. Hidden internal implementation details. Shared database \u00b6 The most common form of integration. Has the following issues: Internal representations are not private, causing high coupling. Logic to modify some kind of data is present in different services, causing loss of cohesion. Every kind of data must be stored using the same DBMS technology. These issues would eliminate the benefits of using microservices, so shared databases are to avoid. Synchronous vs Asynchronous \u00b6 Synchronous communication starts with a blocking call to the server that resolves once the operation completes. It's easy to debug but lacks capabilities to effectively handle long-running processes. Asynchronous communication does not wait for the server to respond. In theory, a client may even not need to know if the server completed the operation. It's not easy to debug but can effectively handle long-running processes. These two different modes of communication can enable two different styles of collaboration: Request/response: natural fit to synchronous communication, can handle asynchronous communication too using callbacks. Event-based: natural fit to asynchronous communication. It's more flexible since a client just issues an event, allowing for more services to listen on that event later on, without modifying the client's code. Orchestration vs Choreography \u00b6 Orchestration means having an orchestrator service that instructs other services on what to do and organizes the whole flow. This provides a clear view of the whole flow but can cause coupling if the orchestrator becomes a \u201cgod\u201d microservice. Choreography means that services can issue or listen to events. This approach keeps services decoupled but can make it hard to understand the whole flow. Remote procedure calls \u00b6 Remote procedure call refers to the technique of making a local call and having it execute on a remote service somewhere. RPC fit well with the request/response collaboration style. The selling point of RPC is ease of use: it's really practical to make a remote call look like a local call. However, RPC has issues too: Usually it causes technology coupling between client and server. Local calls must not be confused with remote calls, because of latency and unreliability. Brittleness, because server signatures and interfaces need to match exactly the ones in the client. Compared to database integration, RPC is certainly an improvement when we think about options for request/response collaboration. REST \u00b6 REpresentational State Transfer (REST) is an architectural style inspired by the Web. The most important concept is the one of resource, which can be requested in different representations. This favours decoupling between internal and external representations. There are many styles of REST, compared in the Richardson Maturity Model . Usually REST is implemented over HTTP because HTTP provides parts of the REST specification, such as verbs. Also, there are lots of tools supporting REST with HTTP. HATEOAS \u00b6 Another principle introduced in REST that can help us avoid the coupling between client and server is the concept of hypermedia as the engine of application state (often abbreviated as HATEOAS). One of the downsides is that the navigation of controls can be quite chatty, as the client needs to follow links to find the operation it wants to perform. Ultimately, this is a trade-off. Serialization format \u00b6 REST provides flexibility over the serialization format of the data. The most popular choices are JSON and XML. XML has built-in support for hypermedia while there are standards to provide hypermedia data with JSON. Downsides to REST Over HTTP \u00b6 Not easy to generate stubs for REST over HTTP services as it would be with RPC. Some web servers do not fully support all the HTTP verbs. Performance is penalized because of hypermedia data and HTTP overhead. HTTP is not suited for frequently exchanging small volumes of data, WebSockets or protocol buffers are more suitable for this kind of communication. Despite these disadvantages, REST over HTTP is a sensible default choice for service-to-service interactions. Implementing Asynchronous Event-Based Collaboration \u00b6 To implement asynchronous event-based collaboration we need to consider: A way for our microservices to emit events. A way for our consumers to find out those events have happened. Traditionally, message brokers like RabbitMQ can handle both problems, while also being able to scale and have resiliency. But note that this kind of collaboration comes with a system complexity increase (e.g. if you're not careful, you could have catastrophic failovers as intended by Martin Fowler). Reactive extensions can help you a lot when handling lots of calls to downstream services. They are a popular choice in distributed systems. DRY in Microservices \u00b6 Following the DRY principle can cause coupling between microservices. As a general rule, DRY is to be followed only inside service boundaries. Across different services, code duplication is a smaller problem than coupling. An exception to this rule can be model-agnostic code such as logging, which can be safely shared between microservices. Client libraries \u00b6 Client libraries can cause coupling between services and clients. To limit this danger, it's best if different developer teams develop the server API and the client library: this way there should be no logic leaks from the server into the client. It's also important to give clients control on when to upgrade their client libraries, to avoid coupling in deploys. Access by Reference \u00b6 Sometimes it may happen to pass around outdated information: we request a Customer and then we use that customer in another request, but in the meanwhile it has changed. In order to retrieve the current state, such requests must include an ID of the involved resources. But this approach has downsides too: It may cause the Customers service to be accessed too much. It causes overhead in requests. This is a tradeoff to consider. The point is: be aware of the freshness of data passed between microservices. Service versioning \u00b6 The following points can help you have a good service versioning in your system: Defer breaking changes as long as possible (e.g. by using the Tolerant reader pattern). Robustness principle: \u201c Be conservative in what you do, be liberal in what you accept from others \u201d. Catch breaking changes early, tests help a lot here. Use semantic versioning. Have coexisting service versions to gradually adopt the new version in the system. Another option is to concurrently deploy microservices of different versions, but suppose you need to fix a bug in the service, then you would need to deploy 2 different services. Still, this is a good approach if you are doing blue/green deploys. User interfaces \u00b6 Each type of user interface (e.g. browser, desktop, mobile) has its own constraints. So even though our core services are the same, we might need a way to adapt them for these constraints. Let\u2019s look at a few models of user interfaces to see how this might be achieved. API composition \u00b6 Each part of the UI communicates with a specific service via its API. Downsides: Little ability to tailor the responses for different sorts of devices. If another team is creating the UI, making even small changes requires change requests to multiple teams. This communication could also be fairly chatty. Opening lots of calls directly to services can be quite intensive for mobile devices. UI Fragment composition \u00b6 Rather than having our UI make API calls and map everything back to UI controls, we could have our services provide parts of the UI directly. The same team that makes changes to the services can also be in charge of making changes to those parts of the UI, allowing us to get changes out faster. Downsides: We need to ensure consistency of the user experience, CSS and HTML style guides can help. Not ideal for native interfaces, it would require falling back to the API composition model. The more cross-cutting a form of interaction is, the less likely this model will fit, falling back to the API composition model. Backends for Frontends \u00b6 A common solution to the problem of chatty interfaces with backend services, or the need to vary content for different types of devices, is to have a server-side aggregation endpoint, or API gateway . The problem that can occur is that normally we\u2019ll have one giant layer for all our services, losing ability to deploy clients independently. A model that solves this problem is Backends for frontends ( BFFs ), it restricts the use of backends for a specific client. The danger with this approach is the same as with any aggregating layer: it can take on logic it shouldn\u2019t. These BFFs should only contain behavior specific to delivering a particular user experience. A Hybrid Approach \u00b6 Some systems use different models together (e.g. BFFs for mobile and UI fragment composition for web). The tricky part still remains avoiding putting too much logic into any intermediate layer. This causes coupling and low cohesion. Integrating with Third-Party Software \u00b6 Challenges associated with integrating third-party software into your system: Lack of control: probably many of the technical decisions have been made for you to simplify product usage. The tool selection process should take into account ease of use of third-party software. Customization: many enterprise tools sell themselves on their ability to be heavily customized just for you. But the cost of customization can be more expensive than building something bespoke from scratch. Integration spaghetti: ideally you want to standardize on a few types of integration. If one product forces you tu use proprietary protocols, it could mean troubles. Best practices: Treat third-party software as a service and place all the customization code in services you control, if possible. When moving away from integrated COTS or legacy software, adopt the Strangler Application Pattern : intercept calls to such software and route them either to the legacy services or to your new services. This allows for a gradual switch. Summary \u00b6 To ensure our microservices remain as decoupled as possible from their other collaborators: Avoid database integration at all costs. Understand the trade-offs between REST and RPC, but strongly consider REST as a good starting point for request/response integration. Prefer choreography over orchestration. Avoid breaking changes and the need to version by understanding Postel\u2019s Law and using tolerant readers. Think of user interfaces as compositional layers.","title":"Integration"},{"location":"building-microservices/integration/#integration","text":"Desirable properties of communication between microservices: Avoid breaking changes as much as possible. Technology agnostic APIs. Make it easy to consume APIs. Hidden internal implementation details.","title":"Integration"},{"location":"building-microservices/integration/#shared-database","text":"The most common form of integration. Has the following issues: Internal representations are not private, causing high coupling. Logic to modify some kind of data is present in different services, causing loss of cohesion. Every kind of data must be stored using the same DBMS technology. These issues would eliminate the benefits of using microservices, so shared databases are to avoid.","title":"Shared database"},{"location":"building-microservices/integration/#synchronous-vs-asynchronous","text":"Synchronous communication starts with a blocking call to the server that resolves once the operation completes. It's easy to debug but lacks capabilities to effectively handle long-running processes. Asynchronous communication does not wait for the server to respond. In theory, a client may even not need to know if the server completed the operation. It's not easy to debug but can effectively handle long-running processes. These two different modes of communication can enable two different styles of collaboration: Request/response: natural fit to synchronous communication, can handle asynchronous communication too using callbacks. Event-based: natural fit to asynchronous communication. It's more flexible since a client just issues an event, allowing for more services to listen on that event later on, without modifying the client's code.","title":"Synchronous vs Asynchronous"},{"location":"building-microservices/integration/#orchestration-vs-choreography","text":"Orchestration means having an orchestrator service that instructs other services on what to do and organizes the whole flow. This provides a clear view of the whole flow but can cause coupling if the orchestrator becomes a \u201cgod\u201d microservice. Choreography means that services can issue or listen to events. This approach keeps services decoupled but can make it hard to understand the whole flow.","title":"Orchestration vs Choreography"},{"location":"building-microservices/integration/#remote-procedure-calls","text":"Remote procedure call refers to the technique of making a local call and having it execute on a remote service somewhere. RPC fit well with the request/response collaboration style. The selling point of RPC is ease of use: it's really practical to make a remote call look like a local call. However, RPC has issues too: Usually it causes technology coupling between client and server. Local calls must not be confused with remote calls, because of latency and unreliability. Brittleness, because server signatures and interfaces need to match exactly the ones in the client. Compared to database integration, RPC is certainly an improvement when we think about options for request/response collaboration.","title":"Remote procedure calls"},{"location":"building-microservices/integration/#rest","text":"REpresentational State Transfer (REST) is an architectural style inspired by the Web. The most important concept is the one of resource, which can be requested in different representations. This favours decoupling between internal and external representations. There are many styles of REST, compared in the Richardson Maturity Model . Usually REST is implemented over HTTP because HTTP provides parts of the REST specification, such as verbs. Also, there are lots of tools supporting REST with HTTP.","title":"REST"},{"location":"building-microservices/integration/#hateoas","text":"Another principle introduced in REST that can help us avoid the coupling between client and server is the concept of hypermedia as the engine of application state (often abbreviated as HATEOAS). One of the downsides is that the navigation of controls can be quite chatty, as the client needs to follow links to find the operation it wants to perform. Ultimately, this is a trade-off.","title":"HATEOAS"},{"location":"building-microservices/integration/#serialization-format","text":"REST provides flexibility over the serialization format of the data. The most popular choices are JSON and XML. XML has built-in support for hypermedia while there are standards to provide hypermedia data with JSON.","title":"Serialization format"},{"location":"building-microservices/integration/#downsides-to-rest-over-http","text":"Not easy to generate stubs for REST over HTTP services as it would be with RPC. Some web servers do not fully support all the HTTP verbs. Performance is penalized because of hypermedia data and HTTP overhead. HTTP is not suited for frequently exchanging small volumes of data, WebSockets or protocol buffers are more suitable for this kind of communication. Despite these disadvantages, REST over HTTP is a sensible default choice for service-to-service interactions.","title":"Downsides to REST Over HTTP"},{"location":"building-microservices/integration/#implementing-asynchronous-event-based-collaboration","text":"To implement asynchronous event-based collaboration we need to consider: A way for our microservices to emit events. A way for our consumers to find out those events have happened. Traditionally, message brokers like RabbitMQ can handle both problems, while also being able to scale and have resiliency. But note that this kind of collaboration comes with a system complexity increase (e.g. if you're not careful, you could have catastrophic failovers as intended by Martin Fowler). Reactive extensions can help you a lot when handling lots of calls to downstream services. They are a popular choice in distributed systems.","title":"Implementing Asynchronous Event-Based Collaboration"},{"location":"building-microservices/integration/#dry-in-microservices","text":"Following the DRY principle can cause coupling between microservices. As a general rule, DRY is to be followed only inside service boundaries. Across different services, code duplication is a smaller problem than coupling. An exception to this rule can be model-agnostic code such as logging, which can be safely shared between microservices.","title":"DRY in Microservices"},{"location":"building-microservices/integration/#client-libraries","text":"Client libraries can cause coupling between services and clients. To limit this danger, it's best if different developer teams develop the server API and the client library: this way there should be no logic leaks from the server into the client. It's also important to give clients control on when to upgrade their client libraries, to avoid coupling in deploys.","title":"Client libraries"},{"location":"building-microservices/integration/#access-by-reference","text":"Sometimes it may happen to pass around outdated information: we request a Customer and then we use that customer in another request, but in the meanwhile it has changed. In order to retrieve the current state, such requests must include an ID of the involved resources. But this approach has downsides too: It may cause the Customers service to be accessed too much. It causes overhead in requests. This is a tradeoff to consider. The point is: be aware of the freshness of data passed between microservices.","title":"Access by Reference"},{"location":"building-microservices/integration/#service-versioning","text":"The following points can help you have a good service versioning in your system: Defer breaking changes as long as possible (e.g. by using the Tolerant reader pattern). Robustness principle: \u201c Be conservative in what you do, be liberal in what you accept from others \u201d. Catch breaking changes early, tests help a lot here. Use semantic versioning. Have coexisting service versions to gradually adopt the new version in the system. Another option is to concurrently deploy microservices of different versions, but suppose you need to fix a bug in the service, then you would need to deploy 2 different services. Still, this is a good approach if you are doing blue/green deploys.","title":"Service versioning"},{"location":"building-microservices/integration/#user-interfaces","text":"Each type of user interface (e.g. browser, desktop, mobile) has its own constraints. So even though our core services are the same, we might need a way to adapt them for these constraints. Let\u2019s look at a few models of user interfaces to see how this might be achieved.","title":"User interfaces"},{"location":"building-microservices/integration/#api-composition","text":"Each part of the UI communicates with a specific service via its API. Downsides: Little ability to tailor the responses for different sorts of devices. If another team is creating the UI, making even small changes requires change requests to multiple teams. This communication could also be fairly chatty. Opening lots of calls directly to services can be quite intensive for mobile devices.","title":"API composition"},{"location":"building-microservices/integration/#ui-fragment-composition","text":"Rather than having our UI make API calls and map everything back to UI controls, we could have our services provide parts of the UI directly. The same team that makes changes to the services can also be in charge of making changes to those parts of the UI, allowing us to get changes out faster. Downsides: We need to ensure consistency of the user experience, CSS and HTML style guides can help. Not ideal for native interfaces, it would require falling back to the API composition model. The more cross-cutting a form of interaction is, the less likely this model will fit, falling back to the API composition model.","title":"UI Fragment composition"},{"location":"building-microservices/integration/#backends-for-frontends","text":"A common solution to the problem of chatty interfaces with backend services, or the need to vary content for different types of devices, is to have a server-side aggregation endpoint, or API gateway . The problem that can occur is that normally we\u2019ll have one giant layer for all our services, losing ability to deploy clients independently. A model that solves this problem is Backends for frontends ( BFFs ), it restricts the use of backends for a specific client. The danger with this approach is the same as with any aggregating layer: it can take on logic it shouldn\u2019t. These BFFs should only contain behavior specific to delivering a particular user experience.","title":"Backends for Frontends"},{"location":"building-microservices/integration/#a-hybrid-approach","text":"Some systems use different models together (e.g. BFFs for mobile and UI fragment composition for web). The tricky part still remains avoiding putting too much logic into any intermediate layer. This causes coupling and low cohesion.","title":"A Hybrid Approach"},{"location":"building-microservices/integration/#integrating-with-third-party-software","text":"Challenges associated with integrating third-party software into your system: Lack of control: probably many of the technical decisions have been made for you to simplify product usage. The tool selection process should take into account ease of use of third-party software. Customization: many enterprise tools sell themselves on their ability to be heavily customized just for you. But the cost of customization can be more expensive than building something bespoke from scratch. Integration spaghetti: ideally you want to standardize on a few types of integration. If one product forces you tu use proprietary protocols, it could mean troubles. Best practices: Treat third-party software as a service and place all the customization code in services you control, if possible. When moving away from integrated COTS or legacy software, adopt the Strangler Application Pattern : intercept calls to such software and route them either to the legacy services or to your new services. This allows for a gradual switch.","title":"Integrating with Third-Party Software"},{"location":"building-microservices/integration/#summary","text":"To ensure our microservices remain as decoupled as possible from their other collaborators: Avoid database integration at all costs. Understand the trade-offs between REST and RPC, but strongly consider REST as a good starting point for request/response integration. Prefer choreography over orchestration. Avoid breaking changes and the need to version by understanding Postel\u2019s Law and using tolerant readers. Think of user interfaces as compositional layers.","title":"Summary"},{"location":"building-microservices/microservices-at-scale/","text":"Microservices at Scale \u00b6 What happens when we have to handle failure of multiple separate services or manage hundreds of services? Failure Is Everywhere \u00b6 The network is unreliable , hard disks are unreliable too. The problem is: failure becomes a statistical certainty at scale . You should spend a bit less of your time trying to stop the inevitable, and a bit more of your time dealing with it gracefully. This means developing antifragile systems , just like Netflix and Amazon are doing. How Much Is Too Much? \u00b6 Services may have different cross-functional requirements. Usually these come from users but you can ask them specific questions to gain the information you need. When it comes to considering if and how to scale out your system to better handle load or failure, start by understanding the following requirements: Response time/latency Availability Durability of data Once you have these requirements in place, you\u2019ll want a way to systematically measure them on an ongoing basis. Architectural Safety Measures \u00b6 There are a few patterns that we can make use of to ensure that if something does go wrong, it doesn\u2019t cause nasty ripple-out effects. Put timeouts on all out-of-process calls, and pick a default timeout for everything. Log when timeouts occur, look at what happens, and change them accordingly. Use circuit breakers . With a circuit breaker, after a certain number of requests to the downstream resource have failed, the circuit breaker is blown. All further requests fail fast while the circuit breaker is in its blown state. After a certain period, the client sends a few requests through to see if the downstream service has recovered, and if it gets enough healthy responses it resets the circuit breaker. In shipping, a bulkhead is a part of the ship that can be sealed off to protect the rest of the ship. So if the ship springs a leak, you can close the bulkhead doors. You lose part of the ship, but the rest of it remains intact. This concept can be adopted in software by isolating components . Circuit breakers can be used as an automatic mechanism to seal a bulkhead. Isolation . The more one service depends on another being up, the more the health of one impacts the ability of the other to do its job. In idempotent operations , the outcome doesn\u2019t change after the first application, even if the operation is subsequently applied multiple times. This is very useful when we want to replay messages that we aren\u2019t sure have been processed, a common way of recovering from error. Scaling \u00b6 We scale our systems in general for one of two reasons: safety and performance. Let\u2019s look at some common scaling techniques and think about how they apply to microservice architectures: Prefer horizontal scaling to vertical scaling because the former adds resilience too and is more cost-efficient. Split workloads across different hosts as it increases resiliency by reducing failure impact area. It also makes it easier to scale instances. You can even split a single microservice into multiple host if there are parts with critical workloads. Spread the risk by ensuring that multiple hosts are spread on different physical machines. Or spread on multiple datacenter regions. Put multiple instances of a single microservice behind a load balancer . This allow us to add more instances of our microservice in a way that is transparent to any service consumers. Also, we can place SSL termination at this level. Use worker-based systems to share load and reduce fragility of operations. The need to change our systems to deal with scale isn\u2019t a sign of failure. It is a sign of success. SO don't plan ahead for handling massive loads. Scaling Databases \u00b6 What if we are storing data in a database? We\u2019ll need to know how to scale that too. Straight off, it is important to separate the concept of availability of the service from the durability of the data itself. Scaling for Reads \u00b6 In a relational database management system (RDBMS), data can be copied from a primary node to one or more replicas. This is often done to ensure that a copy of our data is kept safe, but we can also use it to distribute our reads, while writes are made to the master database. Scaling for Writes \u00b6 A common approach when scaling for writes is to use sharding . With sharding, you have multiple database nodes. You take a piece of data to be written, apply some hashing function to the key of the data, and based on the result of the function learn where to send the data. Benefits: Writes are scaled on multiple shards . Downsides: Difficult to handle queries involving multiple shards . Usually, no resiliency is added since data is not replicated between shards. CQRS \u00b6 The Command-Query Responsibility Segregation (CQRS) pattern refers to an alternate model for storing and querying information: part of the system deals with commands, which capture requests to modify state, while another part of the system deals with queries. The command and query parts of our system could live in different services, or on different hardware, and could make use of radically different types of data store. This can unlock many ways to handle scale. Caching \u00b6 Caching is a commonly used performance optimization whereby the previous result of some operation is stored, so that subsequent requests can use this stored value rather than spending time and resources recalculating the value. THere are 3 models of caching: In client-side caching , the client stores the cached result. This drastically reduces network load but it's tricky to write a good cache invalidator and changes to the caching strategy need to be rolled out to every client. With proxy caching , a proxy is placed between the client and the server. This is often a very simple way to add caching to an existing system. The additional network hops are worth the cost. WIth server-side caching , the server handles the cached results. Everything is opaque to the client but network calls continue to be made. Caching in HTTP \u00b6 HTTP provides some really useful controls to help us cache: We can use cache-control directives to tell client whether to cache the resource at all. An ETag is used to determine if the value of a resource has changed. This becomes powerful when using a conditional GET : it allows us to retrieve only modified data. Caching for writes \u00b6 By using a write-behind cache , you can write to a local cache, and at some later point the data will be flushed to a downstream source. This can be useful when you have bursts of writes, or when there is a good chance that the same data will be written multiple times. Caching for Resilience \u00b6 Caching can be used to implement resiliency in case of failure. With client-side caching, if the downstream service is unavailable, the client could decide to simply use cached but potentially stale data. Hiding the origin \u00b6 With a normal cache, if a request results in a cache miss, the request goes on to the origin to fetch the fresh data with the caller blocking, waiting on the result. But, if we suffer a massive cache miss, many requests will hit the origin. One way to protect the origin in such a situation is to use asynchronous cache filling: Keep it simple \u00b6 The more caches between you and the source of fresh data, the more stale the data can be, and the harder it can be to determine the freshness of the data that a client eventually sees. Autoscaling \u00b6 There are different approaches to autoscaling: You could have the scaling of your services triggered by well-known trends that emerged form gathered data. You could be reactive, bringing up additional instances when you see an increase in load or an instance failure, and remove instances when you no longer needed them. A combination of the previous two. Being reactive and predictive can help you be very cost-efficient if you're on a pay-per-use platform. CAP Theorem \u00b6 In a distributed system, we have 3 properties available: consistency, availability and partition tolerance. The theorem states that we get to keep only 2 of them. You can read the proof here . Sacrificing Consistency \u00b6 In a failure situation, sacrificing consistency means that if we keep accepting writes then at some point in the future they have to be resynchronized. The longer the partition lasts, the more difficult this resynchronization can become. In reality, even with a functioning network, replication of data is not instantaneous because of network latency. Systems that go with AP are called eventually consistent . Sacrificing Availability \u00b6 If we really need consistency, then we need to sacrifice availability. But how to achieve consistency in a distributed system? I want to read a record from the local database node. I have to go and ask the other node if the record it's up to date. But I also have to ask that database node to not allow it to be updated while the read completes; this means I need to initiate a transactional read across multiple nodes to ensure consistency. Transactional reads are make the system very slow. In CP systems, tradeoffs can be made (e.g. instead of asking every node in the network, we may be satisfied by asking half the nodes in the network if a record is up to date). Sacrificing Partition Tolerance \u00b6 This isn't an option, since systems operating over a network absolutely need partition tolerance. AP or CP? \u00b6 AP systems are simpler to build and scale more easily, while CP systems require more work to support distributed consistency. In our systems, we can actually have a mix of both for different services, depending on business requirements. Service discovery \u00b6 Service discovery includes activities such as: knowing what is running in a given environment, knowing which APIs are available, etc. There are different solutions available. Our ideal solution should cope well with constantly creating and destroy services in an environment. DNS \u00b6 DNS lets us associate a name with the IP address of one or more machines. There are multiple approach for picking services' names: Templates that includes environment , e.g. [servicename]-[environment].mysytem.com . Environment-agnostic naming , e.g. [servicename].mysytem.com , which resolves to the different hosts in different environments. This requires more work than the previous approach. Advantages: Well understood standard, has support in every technology stack. Disadvantages: Few DNS services (e.g. Amazon's Route53) are suitable for dealing with highly disposable hosts. Subject to cache poisoning because DNS entries have a time to live (TTL). Registering a DNS entry only for a load balancer, which keeps track of a cluster of services of the same type, partially helps against the disadvantages. Dynamic Service Registries \u00b6 The limitations of DNS in a dynamic environment have led to alternative systems: Zookeeper , a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services. Consul , a distributed service mesh to connect, secure, and configure services across any runtime platform and public or private cloud. Eureka , a REST based service that is primarily used in the AWS cloud for locating services for the purpose of load balancing and failover of middle-tier servers. It's also important to have visualizations helpful to humans trying to understand the state of the system. Documenting Services \u00b6 So we know where our APIs are, but we should also know what's their format and what they do. Some helpful tools for documentation: Swagger lets you describe your API in order to generate a web UI that allows you to view the documentation and interact with the API via a web browser. HAL is a standard that describes standards for hypermedia controls that we expose. The HAL Browser is similar to Swagger's UI.","title":"Microservices at Scale"},{"location":"building-microservices/microservices-at-scale/#microservices-at-scale","text":"What happens when we have to handle failure of multiple separate services or manage hundreds of services?","title":"Microservices at Scale"},{"location":"building-microservices/microservices-at-scale/#failure-is-everywhere","text":"The network is unreliable , hard disks are unreliable too. The problem is: failure becomes a statistical certainty at scale . You should spend a bit less of your time trying to stop the inevitable, and a bit more of your time dealing with it gracefully. This means developing antifragile systems , just like Netflix and Amazon are doing.","title":"Failure Is Everywhere"},{"location":"building-microservices/microservices-at-scale/#how-much-is-too-much","text":"Services may have different cross-functional requirements. Usually these come from users but you can ask them specific questions to gain the information you need. When it comes to considering if and how to scale out your system to better handle load or failure, start by understanding the following requirements: Response time/latency Availability Durability of data Once you have these requirements in place, you\u2019ll want a way to systematically measure them on an ongoing basis.","title":"How Much Is Too Much?"},{"location":"building-microservices/microservices-at-scale/#architectural-safety-measures","text":"There are a few patterns that we can make use of to ensure that if something does go wrong, it doesn\u2019t cause nasty ripple-out effects. Put timeouts on all out-of-process calls, and pick a default timeout for everything. Log when timeouts occur, look at what happens, and change them accordingly. Use circuit breakers . With a circuit breaker, after a certain number of requests to the downstream resource have failed, the circuit breaker is blown. All further requests fail fast while the circuit breaker is in its blown state. After a certain period, the client sends a few requests through to see if the downstream service has recovered, and if it gets enough healthy responses it resets the circuit breaker. In shipping, a bulkhead is a part of the ship that can be sealed off to protect the rest of the ship. So if the ship springs a leak, you can close the bulkhead doors. You lose part of the ship, but the rest of it remains intact. This concept can be adopted in software by isolating components . Circuit breakers can be used as an automatic mechanism to seal a bulkhead. Isolation . The more one service depends on another being up, the more the health of one impacts the ability of the other to do its job. In idempotent operations , the outcome doesn\u2019t change after the first application, even if the operation is subsequently applied multiple times. This is very useful when we want to replay messages that we aren\u2019t sure have been processed, a common way of recovering from error.","title":"Architectural Safety Measures"},{"location":"building-microservices/microservices-at-scale/#scaling","text":"We scale our systems in general for one of two reasons: safety and performance. Let\u2019s look at some common scaling techniques and think about how they apply to microservice architectures: Prefer horizontal scaling to vertical scaling because the former adds resilience too and is more cost-efficient. Split workloads across different hosts as it increases resiliency by reducing failure impact area. It also makes it easier to scale instances. You can even split a single microservice into multiple host if there are parts with critical workloads. Spread the risk by ensuring that multiple hosts are spread on different physical machines. Or spread on multiple datacenter regions. Put multiple instances of a single microservice behind a load balancer . This allow us to add more instances of our microservice in a way that is transparent to any service consumers. Also, we can place SSL termination at this level. Use worker-based systems to share load and reduce fragility of operations. The need to change our systems to deal with scale isn\u2019t a sign of failure. It is a sign of success. SO don't plan ahead for handling massive loads.","title":"Scaling"},{"location":"building-microservices/microservices-at-scale/#scaling-databases","text":"What if we are storing data in a database? We\u2019ll need to know how to scale that too. Straight off, it is important to separate the concept of availability of the service from the durability of the data itself.","title":"Scaling Databases"},{"location":"building-microservices/microservices-at-scale/#scaling-for-reads","text":"In a relational database management system (RDBMS), data can be copied from a primary node to one or more replicas. This is often done to ensure that a copy of our data is kept safe, but we can also use it to distribute our reads, while writes are made to the master database.","title":"Scaling for Reads"},{"location":"building-microservices/microservices-at-scale/#scaling-for-writes","text":"A common approach when scaling for writes is to use sharding . With sharding, you have multiple database nodes. You take a piece of data to be written, apply some hashing function to the key of the data, and based on the result of the function learn where to send the data. Benefits: Writes are scaled on multiple shards . Downsides: Difficult to handle queries involving multiple shards . Usually, no resiliency is added since data is not replicated between shards.","title":"Scaling for Writes"},{"location":"building-microservices/microservices-at-scale/#cqrs","text":"The Command-Query Responsibility Segregation (CQRS) pattern refers to an alternate model for storing and querying information: part of the system deals with commands, which capture requests to modify state, while another part of the system deals with queries. The command and query parts of our system could live in different services, or on different hardware, and could make use of radically different types of data store. This can unlock many ways to handle scale.","title":"CQRS"},{"location":"building-microservices/microservices-at-scale/#caching","text":"Caching is a commonly used performance optimization whereby the previous result of some operation is stored, so that subsequent requests can use this stored value rather than spending time and resources recalculating the value. THere are 3 models of caching: In client-side caching , the client stores the cached result. This drastically reduces network load but it's tricky to write a good cache invalidator and changes to the caching strategy need to be rolled out to every client. With proxy caching , a proxy is placed between the client and the server. This is often a very simple way to add caching to an existing system. The additional network hops are worth the cost. WIth server-side caching , the server handles the cached results. Everything is opaque to the client but network calls continue to be made.","title":"Caching"},{"location":"building-microservices/microservices-at-scale/#caching-in-http","text":"HTTP provides some really useful controls to help us cache: We can use cache-control directives to tell client whether to cache the resource at all. An ETag is used to determine if the value of a resource has changed. This becomes powerful when using a conditional GET : it allows us to retrieve only modified data.","title":"Caching in HTTP"},{"location":"building-microservices/microservices-at-scale/#caching-for-writes","text":"By using a write-behind cache , you can write to a local cache, and at some later point the data will be flushed to a downstream source. This can be useful when you have bursts of writes, or when there is a good chance that the same data will be written multiple times.","title":"Caching for writes"},{"location":"building-microservices/microservices-at-scale/#caching-for-resilience","text":"Caching can be used to implement resiliency in case of failure. With client-side caching, if the downstream service is unavailable, the client could decide to simply use cached but potentially stale data.","title":"Caching for Resilience"},{"location":"building-microservices/microservices-at-scale/#hiding-the-origin","text":"With a normal cache, if a request results in a cache miss, the request goes on to the origin to fetch the fresh data with the caller blocking, waiting on the result. But, if we suffer a massive cache miss, many requests will hit the origin. One way to protect the origin in such a situation is to use asynchronous cache filling:","title":"Hiding the origin"},{"location":"building-microservices/microservices-at-scale/#keep-it-simple","text":"The more caches between you and the source of fresh data, the more stale the data can be, and the harder it can be to determine the freshness of the data that a client eventually sees.","title":"Keep it simple"},{"location":"building-microservices/microservices-at-scale/#autoscaling","text":"There are different approaches to autoscaling: You could have the scaling of your services triggered by well-known trends that emerged form gathered data. You could be reactive, bringing up additional instances when you see an increase in load or an instance failure, and remove instances when you no longer needed them. A combination of the previous two. Being reactive and predictive can help you be very cost-efficient if you're on a pay-per-use platform.","title":"Autoscaling"},{"location":"building-microservices/microservices-at-scale/#cap-theorem","text":"In a distributed system, we have 3 properties available: consistency, availability and partition tolerance. The theorem states that we get to keep only 2 of them. You can read the proof here .","title":"CAP Theorem"},{"location":"building-microservices/microservices-at-scale/#sacrificing-consistency","text":"In a failure situation, sacrificing consistency means that if we keep accepting writes then at some point in the future they have to be resynchronized. The longer the partition lasts, the more difficult this resynchronization can become. In reality, even with a functioning network, replication of data is not instantaneous because of network latency. Systems that go with AP are called eventually consistent .","title":"Sacrificing Consistency"},{"location":"building-microservices/microservices-at-scale/#sacrificing-availability","text":"If we really need consistency, then we need to sacrifice availability. But how to achieve consistency in a distributed system? I want to read a record from the local database node. I have to go and ask the other node if the record it's up to date. But I also have to ask that database node to not allow it to be updated while the read completes; this means I need to initiate a transactional read across multiple nodes to ensure consistency. Transactional reads are make the system very slow. In CP systems, tradeoffs can be made (e.g. instead of asking every node in the network, we may be satisfied by asking half the nodes in the network if a record is up to date).","title":"Sacrificing Availability"},{"location":"building-microservices/microservices-at-scale/#sacrificing-partition-tolerance","text":"This isn't an option, since systems operating over a network absolutely need partition tolerance.","title":"Sacrificing Partition Tolerance"},{"location":"building-microservices/microservices-at-scale/#ap-or-cp","text":"AP systems are simpler to build and scale more easily, while CP systems require more work to support distributed consistency. In our systems, we can actually have a mix of both for different services, depending on business requirements.","title":"AP or CP?"},{"location":"building-microservices/microservices-at-scale/#service-discovery","text":"Service discovery includes activities such as: knowing what is running in a given environment, knowing which APIs are available, etc. There are different solutions available. Our ideal solution should cope well with constantly creating and destroy services in an environment.","title":"Service discovery"},{"location":"building-microservices/microservices-at-scale/#dns","text":"DNS lets us associate a name with the IP address of one or more machines. There are multiple approach for picking services' names: Templates that includes environment , e.g. [servicename]-[environment].mysytem.com . Environment-agnostic naming , e.g. [servicename].mysytem.com , which resolves to the different hosts in different environments. This requires more work than the previous approach. Advantages: Well understood standard, has support in every technology stack. Disadvantages: Few DNS services (e.g. Amazon's Route53) are suitable for dealing with highly disposable hosts. Subject to cache poisoning because DNS entries have a time to live (TTL). Registering a DNS entry only for a load balancer, which keeps track of a cluster of services of the same type, partially helps against the disadvantages.","title":"DNS"},{"location":"building-microservices/microservices-at-scale/#dynamic-service-registries","text":"The limitations of DNS in a dynamic environment have led to alternative systems: Zookeeper , a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services. Consul , a distributed service mesh to connect, secure, and configure services across any runtime platform and public or private cloud. Eureka , a REST based service that is primarily used in the AWS cloud for locating services for the purpose of load balancing and failover of middle-tier servers. It's also important to have visualizations helpful to humans trying to understand the state of the system.","title":"Dynamic Service Registries"},{"location":"building-microservices/microservices-at-scale/#documenting-services","text":"So we know where our APIs are, but we should also know what's their format and what they do. Some helpful tools for documentation: Swagger lets you describe your API in order to generate a web UI that allows you to view the documentation and interact with the API via a web browser. HAL is a standard that describes standards for hypermedia controls that we expose. The HAL Browser is similar to Swagger's UI.","title":"Documenting Services"},{"location":"building-microservices/microservices/","text":"Microservices \u00b6 Microservices are small and autonomous services. Benefits \u00b6 Allow adoption of new technologies with reduced risk Resiliency of services, the system can be kept up on partial failures Scaling can be aimed at specific services, thus providing cost savings due to efficiency Ease of deployment, can deploy small parts with small deltas to deploy more frequently Organizational alignment, can assign team of ideal size (not too big, not too small) to a microservice development Composability and reusability of services Small services can be easily decommissioned and replaced when the need arises Relationship with SOA \u00b6 SOA has some issues because it's not a well-defined specification, so there are lots of ways to do SOA. Microservices are a specific way to do SOA. Some say they are SOA done right. Similar decomposition techniques \u00b6 Do we need microservices? Can similar decomposition techniques offer the same benefits provided by microservices? Shared libraries \u00b6 Drawbacks: Need to run on the same platform as the service (losing technology heterogeneity) Cannot scale services independently Unless using DLLs, services cannot load a new version of the library without stopping their execution (losing ease of deploy in isolation) Lack of system resiliency Shared libraries are best suited for common code reuse. But be careful: business code reuse can cause coupling in microservices. Modules \u00b6 Usually languages do not have proper support for isolated life cycle management of modules, it's hard (if possible at all) for developers to add this functionality. The drawbacks in these cases are nearly the same as the ones provided by shared libraries. Consider even languages that have proper support for ILM (such as Erlang). The system should be based only on that language (losing technology heterogeneity) and this is usually not the case for projects that integrate with legacy software. Also, in practice, using modules will likely make developers produce coupled code between modules, thus losing independence. No silver bullet \u00b6 Microservices are no silver bullet because they add to your system the challenges of distributed systems. Also, you have to be confident with deploys, testing, monitoring and scaling in order to effectively gain the benefits of microservices.","title":"Microservices"},{"location":"building-microservices/microservices/#microservices","text":"Microservices are small and autonomous services.","title":"Microservices"},{"location":"building-microservices/microservices/#benefits","text":"Allow adoption of new technologies with reduced risk Resiliency of services, the system can be kept up on partial failures Scaling can be aimed at specific services, thus providing cost savings due to efficiency Ease of deployment, can deploy small parts with small deltas to deploy more frequently Organizational alignment, can assign team of ideal size (not too big, not too small) to a microservice development Composability and reusability of services Small services can be easily decommissioned and replaced when the need arises","title":"Benefits"},{"location":"building-microservices/microservices/#relationship-with-soa","text":"SOA has some issues because it's not a well-defined specification, so there are lots of ways to do SOA. Microservices are a specific way to do SOA. Some say they are SOA done right.","title":"Relationship with SOA"},{"location":"building-microservices/microservices/#similar-decomposition-techniques","text":"Do we need microservices? Can similar decomposition techniques offer the same benefits provided by microservices?","title":"Similar decomposition techniques"},{"location":"building-microservices/microservices/#shared-libraries","text":"Drawbacks: Need to run on the same platform as the service (losing technology heterogeneity) Cannot scale services independently Unless using DLLs, services cannot load a new version of the library without stopping their execution (losing ease of deploy in isolation) Lack of system resiliency Shared libraries are best suited for common code reuse. But be careful: business code reuse can cause coupling in microservices.","title":"Shared libraries"},{"location":"building-microservices/microservices/#modules","text":"Usually languages do not have proper support for isolated life cycle management of modules, it's hard (if possible at all) for developers to add this functionality. The drawbacks in these cases are nearly the same as the ones provided by shared libraries. Consider even languages that have proper support for ILM (such as Erlang). The system should be based only on that language (losing technology heterogeneity) and this is usually not the case for projects that integrate with legacy software. Also, in practice, using modules will likely make developers produce coupled code between modules, thus losing independence.","title":"Modules"},{"location":"building-microservices/microservices/#no-silver-bullet","text":"Microservices are no silver bullet because they add to your system the challenges of distributed systems. Also, you have to be confident with deploys, testing, monitoring and scaling in order to effectively gain the benefits of microservices.","title":"No silver bullet"},{"location":"building-microservices/monitoring/","text":"Monitoring \u00b6 Breaking our system up in microservices adds complexity in the monitoring process. The answer is to add monitoring at a single service level and then aggregate the data, because eventually there are gonna be too many services for manual monitoring. In microservices, monitoring can help you efficiently scale your system too. Single Service, Single Server \u00b6 Let's first consider the simplest setup: one host, running one service. What should we monitor? The host (e.g. CPU and memory usage). The logs of the server , so when a user reports an error we can pinpoint it to a log record. The application (e.g. response times). Single Service, Multiple Servers \u00b6 Now there multiple copies of the service running on separate hosts, behind a load balancer. What should we monitor? The hosts , both individually and by aggregating data: it would be useful to determine if memory usage is due to a software bug or to a rogue OS process. The logs can still be saved on each host. We would be able to easily navigate them via tools like ssh-multiplexers. The load balancer can help with aggregating data for tasks like response time tracking . Ideally the load balancer should be able to tell if a microservice is healthy and remove it if that's not the case. Multiple Services, Multiple Servers \u00b6 Multiple services are providing capabilities to our users, and those services are running on multiple hosts \u2014 be they physical or virtual. In this case we would need specific subsystems to aggregate (e.g. Logstash , Graphite ) and visualize (e.g. Kibana ) data. Service Metrics \u00b6 Ideally your service should expose basic metrics too (e.g. invoices emitted per day). The benefits are: You can detect which features of a service are actually used in production. You can react to how your users are using your system in order to improve it. It's hard to know what data will be useful when you first deploy your system. Synthetic Monitoring \u00b6 It's about monitoring systems acting like users and reporting back issues if they occur. For example, out monitoring system could publish a message in a queue from time to time and track how the system behaves when handling that message. If any issues are found, they are reported and we would be able to debug them because we have low-level monitoring in place too. Sending this fake message is an example of a synthetic transaction used to ensure the system was behaving semantically, which is why this technique is often called semantic monitoring . But how to implement semantic monitoring? We can reuse the code in our tests and run a subset of it against the production system. You must carefully check the data requirements and ensure that no side effects are going to take place. A common solution is to enable a set of fake users that will perform synthetic transaction in production. Correlation IDs \u00b6 With many services, a single initiating call can end up generating multiple more downstream service calls. How to track these calls? A possible solution is to use correlation IDs: when the first call is made, you generate a GUID for the call. This is then passed along to all subsequent calls. It would be better to adopt correlation ids from the start, because it's usually very hard to retro fit call logs with correlation ids. Also, note that using correlation IDs justifies the development of a client library shared between microservices, because it would simplify communication a lot. Just remember to keep these shared libraries as thin as possible to avoid any kind of coupling. Cascading Failures \u00b6 Cascading failures can be especially perilous. Imagine a situation where a service cannot communicate with a downstream service while they both look healthy. Using synthetic monitoring would pick up the problem. But we\u2019d also need to report on the fact that one service cannot see another in order to determine the cause of the problem. Therefore, monitoring the integration points between systems is fundamental. Each service instance should track and expose the health of its downstream dependencies, from the database to other collaborating services. Standardization \u00b6 One of the ongoing balancing acts you\u2019ll need to pull off is where to allow for decisions to be made narrowly for a single service versus where you need to standardize across your system. Monitoring is one area where standardization is incredibly important, because it allows for easy data aggregation and monitoring. Consider the audience \u00b6 It's fundamental to display data according to who is going to consume it (e.g. sysops can consume bulks of system-level logs while a manager is going to need a dashboard to visualize business-related metrics). Summary \u00b6 For each service: Track inbound response time at a bare minimum. Once you\u2019ve done that, follow with error rates and then start working on application-level metrics. Track the health of all downstream responses, at a bare minimum including the response time of downstream calls, and at best tracking error rates. Standardize on how and where metrics are collected. Log into a standard location, in a standard format if possible. Aggregation is a pain if every service uses a different layout! Monitor the underlying operating system so you can track down rogue processes and do capacity planning. For the system: Aggregate host-level metrics like CPU together with application-level metrics. Ensure your metric storage tool allows for aggregation at a system or service level, and drill down to individual hosts. Ensure your metric storage tool allows you to maintain data long enough to understand trends in your system. Have a single, queryable tool for aggregating and storing logs. Strongly consider standardizing on the use of correlation IDs. Understand what requires a call to action, and structure alerting and dashboards accordingly. Investigate the possibility of unifying how you aggregate all of your various metrics.","title":"Monitoring"},{"location":"building-microservices/monitoring/#monitoring","text":"Breaking our system up in microservices adds complexity in the monitoring process. The answer is to add monitoring at a single service level and then aggregate the data, because eventually there are gonna be too many services for manual monitoring. In microservices, monitoring can help you efficiently scale your system too.","title":"Monitoring"},{"location":"building-microservices/monitoring/#single-service-single-server","text":"Let's first consider the simplest setup: one host, running one service. What should we monitor? The host (e.g. CPU and memory usage). The logs of the server , so when a user reports an error we can pinpoint it to a log record. The application (e.g. response times).","title":"Single Service, Single Server"},{"location":"building-microservices/monitoring/#single-service-multiple-servers","text":"Now there multiple copies of the service running on separate hosts, behind a load balancer. What should we monitor? The hosts , both individually and by aggregating data: it would be useful to determine if memory usage is due to a software bug or to a rogue OS process. The logs can still be saved on each host. We would be able to easily navigate them via tools like ssh-multiplexers. The load balancer can help with aggregating data for tasks like response time tracking . Ideally the load balancer should be able to tell if a microservice is healthy and remove it if that's not the case.","title":"Single Service, Multiple Servers"},{"location":"building-microservices/monitoring/#multiple-services-multiple-servers","text":"Multiple services are providing capabilities to our users, and those services are running on multiple hosts \u2014 be they physical or virtual. In this case we would need specific subsystems to aggregate (e.g. Logstash , Graphite ) and visualize (e.g. Kibana ) data.","title":"Multiple Services, Multiple Servers"},{"location":"building-microservices/monitoring/#service-metrics","text":"Ideally your service should expose basic metrics too (e.g. invoices emitted per day). The benefits are: You can detect which features of a service are actually used in production. You can react to how your users are using your system in order to improve it. It's hard to know what data will be useful when you first deploy your system.","title":"Service Metrics"},{"location":"building-microservices/monitoring/#synthetic-monitoring","text":"It's about monitoring systems acting like users and reporting back issues if they occur. For example, out monitoring system could publish a message in a queue from time to time and track how the system behaves when handling that message. If any issues are found, they are reported and we would be able to debug them because we have low-level monitoring in place too. Sending this fake message is an example of a synthetic transaction used to ensure the system was behaving semantically, which is why this technique is often called semantic monitoring . But how to implement semantic monitoring? We can reuse the code in our tests and run a subset of it against the production system. You must carefully check the data requirements and ensure that no side effects are going to take place. A common solution is to enable a set of fake users that will perform synthetic transaction in production.","title":"Synthetic Monitoring"},{"location":"building-microservices/monitoring/#correlation-ids","text":"With many services, a single initiating call can end up generating multiple more downstream service calls. How to track these calls? A possible solution is to use correlation IDs: when the first call is made, you generate a GUID for the call. This is then passed along to all subsequent calls. It would be better to adopt correlation ids from the start, because it's usually very hard to retro fit call logs with correlation ids. Also, note that using correlation IDs justifies the development of a client library shared between microservices, because it would simplify communication a lot. Just remember to keep these shared libraries as thin as possible to avoid any kind of coupling.","title":"Correlation IDs"},{"location":"building-microservices/monitoring/#cascading-failures","text":"Cascading failures can be especially perilous. Imagine a situation where a service cannot communicate with a downstream service while they both look healthy. Using synthetic monitoring would pick up the problem. But we\u2019d also need to report on the fact that one service cannot see another in order to determine the cause of the problem. Therefore, monitoring the integration points between systems is fundamental. Each service instance should track and expose the health of its downstream dependencies, from the database to other collaborating services.","title":"Cascading Failures"},{"location":"building-microservices/monitoring/#standardization","text":"One of the ongoing balancing acts you\u2019ll need to pull off is where to allow for decisions to be made narrowly for a single service versus where you need to standardize across your system. Monitoring is one area where standardization is incredibly important, because it allows for easy data aggregation and monitoring.","title":"Standardization"},{"location":"building-microservices/monitoring/#consider-the-audience","text":"It's fundamental to display data according to who is going to consume it (e.g. sysops can consume bulks of system-level logs while a manager is going to need a dashboard to visualize business-related metrics).","title":"Consider the audience"},{"location":"building-microservices/monitoring/#summary","text":"For each service: Track inbound response time at a bare minimum. Once you\u2019ve done that, follow with error rates and then start working on application-level metrics. Track the health of all downstream responses, at a bare minimum including the response time of downstream calls, and at best tracking error rates. Standardize on how and where metrics are collected. Log into a standard location, in a standard format if possible. Aggregation is a pain if every service uses a different layout! Monitor the underlying operating system so you can track down rogue processes and do capacity planning. For the system: Aggregate host-level metrics like CPU together with application-level metrics. Ensure your metric storage tool allows for aggregation at a system or service level, and drill down to individual hosts. Ensure your metric storage tool allows you to maintain data long enough to understand trends in your system. Have a single, queryable tool for aggregating and storing logs. Strongly consider standardizing on the use of correlation IDs. Understand what requires a call to action, and structure alerting and dashboards accordingly. Investigate the possibility of unifying how you aggregate all of your various metrics.","title":"Summary"},{"location":"building-microservices/security/","text":"Security \u00b6 We need to be aware of the importance of customers' data in our systems. How can we work out what is enough security to protect that data? Authentication and Authorization \u00b6 Authentication is the mechanism by which an actor proves that he is who he says he is. The actor who is being authenticated is called principal . Authorization maps a principal to the action he's allowed to do. Generally, in monolithic applications, web framework provide all the authentication and authorization functionalities for you. While in distributed systems we aim to authenticate a principal a single time for all microservices. Single Sign-On \u00b6 SSO is a common approach to authentication and authorization. How it works: When a principal tries to access a resource , she is directed to authenticate with an identity provider that may ask her to provide a secret . If the identity provider is satisfied of the secret, it gives information to the service provider, allowing it to decide whether to grant her access to the resource. The identity provider can be an external (e.g. Google's OpenID Connect service) or internal (common for enterprise platforms). Common SSO implementations: SAML (SOAP-based standard) OpenID Connect (authentication layer on top of OAuth 2.0) Single Sign-On Gateway \u00b6 Rather than having each microservice communicate with the identity provider, you could add a SSO Gateway to your system: Information about principals can be passed to downstream microservices with HTTP headers. Benefits: Drastically reduces network usage in your system. Can be used to terminate HTTPS at this level, run intrusion detection and so on. Downsides: You need to make sure that developers are able to launch microservices behind a SSO gateway to test the system without too much effort. Could give a false sense of security. A gateway may be able to provide fairly effective coarse-grained authorization using roles . Always prefer coarse over fine-grained authorization. The latter make your system hard to manage and reason about because business rules end up in the identity provider. Instead, these rules should be owned by your microservices. Service-to-Service Authentication and Authorization \u00b6 When talking about programs authenticating with each other, we have several possibilities. Allow Everything Inside the Perimeter \u00b6 Authentication is placed only at the perimeter of the system. Depending on the sensitivity of the data, this approach could be fine. Benefits: Very easy to implement. Downsides: Should an attacker penetrate the perimeter , you'll end up with major troubles. This can be mitigated by using HTTPS but still the stakes are high. HTTP(S) Basic Authentication \u00b6 HTTP Basic Authentication allows for a client to send username and password in a standard HTTP header. This should normally be used with HTTPS to securely send credentials. Benefits: Well understood and supported standard. Downsides: You need to manage SSL certificates for your microservices. The overhead of HTTPS traffic can place additional strain on servers. SSL encrypted traffic can usually be cached only at service level. You need to manage user and passwords, either by syncing with an existing identity provider or independently (could cause functionality duplication). Reuse a SSO implementation \u00b6 If you already have a SSO gateway implemented, you could reuse it for service-to-service authentication. Microservices should have their own service accounts to authenticate with each other. Benefits: Reuses existing infrastructure. With each microservice having its own credentials, it's easy to revoke/restore them if they get compromised. Downsides: You need to write code that supports the SSO implementation of your choice. You need to securely store the service accounts' credentials. Client certificates \u00b6 Clients can install their own TLS certificates to authenticate to servers. This could be a mandatory choice if your service-to-service traffic goes through networks you don't control (e.g. Internet). Benefits: Servers have strong guarantee that they're communicating with the right client. Downsides: Even more difficult than managing only server certificates. HMAC over HTTP \u00b6 An alternative approach is to use a hash-based messaging code (HMAC) to sign the request with a hash computed from the request's body and a private key. Benefits: Prevents MITM attacks. Traffic can be cached more easily than HTTPS traffic. Usually the overhead of generating hashes is lower than the one caused by HTTPS. Downsides: You need a way to share the secret key. Hard to revoke the private key in case it's compromised. It's a pattern, not a standard, so there are divergent ways to implement it. Traffic is not encrypted. API Keys \u00b6 API keys allow a service to identify who is making a call, and place limits on what they can do. Benefits: Easy to use for service-to-service authentication. Downsides: You need an API keys manager. Still, there are a lot of available tools. The deputy problem \u00b6 Having a principal authenticate with a given microservice is simple. But what happens if that service then needs to call more services to complete an operation? Without countermeasures, users can potentially trick the system into making calls to microservices that retrieve information they're not allowed to access (e.g. other users' data). There is no simple answer to this problem. Depending on the sensitivity of the operation, you have different choices: Use an implicit trust model inside the microservices perimeter. Verify the identity of the caller before accepting a request (this could add duplicate logic in your microservices). Ask the caller to provide the credentials of the original principal. Securing Data at Rest \u00b6 It's important to carefully store data at rest to limit the damage if an attacker obtains access to it.c Some principles to observe: Use existing implementations of cryptographic algorithms. Be careful how you manage your keys. Identify sensitive data that should be encrypted and data that can be shown in logs. Encrypt data when you first see it. Only decrypt it on demand. Encrypt backups. Other protections \u00b6 There are other protective measures that you can apply to your system: Firewall. Logging (but be careful of what you log!). Intrusion detection systems. Network segregation (e.g. put separate microservices in separate networks). Best practices: Keep your software updated and look out for new vulnerabilities. Give the service user on the server OS as few permissions as possible. Be frugal: collect only the data you need. The Human Element \u00b6 You may also need policies to deal with the human element in your organization: How do you revoke access to credentials when someone leaves the organization? How can you protect yourself against social engineering? Baking Security In \u00b6 Helping educate developers about security concerns is key. Also, you can integrate automated vulnerabilities probing tools in your CI build. But when in doubt, reach out to an expert for a thorough check of your system. Summary \u00b6 Having a system decomposed into finer-grained services gives us many more options as to how to solve a problem. This concept applies to security too. So, while we have additional problems to solve (e.g. the deputy problem), we can apply a mix of known solutions to solve these problems.","title":"Security"},{"location":"building-microservices/security/#security","text":"We need to be aware of the importance of customers' data in our systems. How can we work out what is enough security to protect that data?","title":"Security"},{"location":"building-microservices/security/#authentication-and-authorization","text":"Authentication is the mechanism by which an actor proves that he is who he says he is. The actor who is being authenticated is called principal . Authorization maps a principal to the action he's allowed to do. Generally, in monolithic applications, web framework provide all the authentication and authorization functionalities for you. While in distributed systems we aim to authenticate a principal a single time for all microservices.","title":"Authentication and Authorization"},{"location":"building-microservices/security/#single-sign-on","text":"SSO is a common approach to authentication and authorization. How it works: When a principal tries to access a resource , she is directed to authenticate with an identity provider that may ask her to provide a secret . If the identity provider is satisfied of the secret, it gives information to the service provider, allowing it to decide whether to grant her access to the resource. The identity provider can be an external (e.g. Google's OpenID Connect service) or internal (common for enterprise platforms). Common SSO implementations: SAML (SOAP-based standard) OpenID Connect (authentication layer on top of OAuth 2.0)","title":"Single Sign-On"},{"location":"building-microservices/security/#single-sign-on-gateway","text":"Rather than having each microservice communicate with the identity provider, you could add a SSO Gateway to your system: Information about principals can be passed to downstream microservices with HTTP headers. Benefits: Drastically reduces network usage in your system. Can be used to terminate HTTPS at this level, run intrusion detection and so on. Downsides: You need to make sure that developers are able to launch microservices behind a SSO gateway to test the system without too much effort. Could give a false sense of security. A gateway may be able to provide fairly effective coarse-grained authorization using roles . Always prefer coarse over fine-grained authorization. The latter make your system hard to manage and reason about because business rules end up in the identity provider. Instead, these rules should be owned by your microservices.","title":"Single Sign-On Gateway"},{"location":"building-microservices/security/#service-to-service-authentication-and-authorization","text":"When talking about programs authenticating with each other, we have several possibilities.","title":"Service-to-Service Authentication and Authorization"},{"location":"building-microservices/security/#allow-everything-inside-the-perimeter","text":"Authentication is placed only at the perimeter of the system. Depending on the sensitivity of the data, this approach could be fine. Benefits: Very easy to implement. Downsides: Should an attacker penetrate the perimeter , you'll end up with major troubles. This can be mitigated by using HTTPS but still the stakes are high.","title":"Allow Everything Inside the Perimeter"},{"location":"building-microservices/security/#https-basic-authentication","text":"HTTP Basic Authentication allows for a client to send username and password in a standard HTTP header. This should normally be used with HTTPS to securely send credentials. Benefits: Well understood and supported standard. Downsides: You need to manage SSL certificates for your microservices. The overhead of HTTPS traffic can place additional strain on servers. SSL encrypted traffic can usually be cached only at service level. You need to manage user and passwords, either by syncing with an existing identity provider or independently (could cause functionality duplication).","title":"HTTP(S) Basic Authentication"},{"location":"building-microservices/security/#reuse-a-sso-implementation","text":"If you already have a SSO gateway implemented, you could reuse it for service-to-service authentication. Microservices should have their own service accounts to authenticate with each other. Benefits: Reuses existing infrastructure. With each microservice having its own credentials, it's easy to revoke/restore them if they get compromised. Downsides: You need to write code that supports the SSO implementation of your choice. You need to securely store the service accounts' credentials.","title":"Reuse a SSO implementation"},{"location":"building-microservices/security/#client-certificates","text":"Clients can install their own TLS certificates to authenticate to servers. This could be a mandatory choice if your service-to-service traffic goes through networks you don't control (e.g. Internet). Benefits: Servers have strong guarantee that they're communicating with the right client. Downsides: Even more difficult than managing only server certificates.","title":"Client certificates"},{"location":"building-microservices/security/#hmac-over-http","text":"An alternative approach is to use a hash-based messaging code (HMAC) to sign the request with a hash computed from the request's body and a private key. Benefits: Prevents MITM attacks. Traffic can be cached more easily than HTTPS traffic. Usually the overhead of generating hashes is lower than the one caused by HTTPS. Downsides: You need a way to share the secret key. Hard to revoke the private key in case it's compromised. It's a pattern, not a standard, so there are divergent ways to implement it. Traffic is not encrypted.","title":"HMAC over HTTP"},{"location":"building-microservices/security/#api-keys","text":"API keys allow a service to identify who is making a call, and place limits on what they can do. Benefits: Easy to use for service-to-service authentication. Downsides: You need an API keys manager. Still, there are a lot of available tools.","title":"API Keys"},{"location":"building-microservices/security/#the-deputy-problem","text":"Having a principal authenticate with a given microservice is simple. But what happens if that service then needs to call more services to complete an operation? Without countermeasures, users can potentially trick the system into making calls to microservices that retrieve information they're not allowed to access (e.g. other users' data). There is no simple answer to this problem. Depending on the sensitivity of the operation, you have different choices: Use an implicit trust model inside the microservices perimeter. Verify the identity of the caller before accepting a request (this could add duplicate logic in your microservices). Ask the caller to provide the credentials of the original principal.","title":"The deputy problem"},{"location":"building-microservices/security/#securing-data-at-rest","text":"It's important to carefully store data at rest to limit the damage if an attacker obtains access to it.c Some principles to observe: Use existing implementations of cryptographic algorithms. Be careful how you manage your keys. Identify sensitive data that should be encrypted and data that can be shown in logs. Encrypt data when you first see it. Only decrypt it on demand. Encrypt backups.","title":"Securing Data at Rest"},{"location":"building-microservices/security/#other-protections","text":"There are other protective measures that you can apply to your system: Firewall. Logging (but be careful of what you log!). Intrusion detection systems. Network segregation (e.g. put separate microservices in separate networks). Best practices: Keep your software updated and look out for new vulnerabilities. Give the service user on the server OS as few permissions as possible. Be frugal: collect only the data you need.","title":"Other protections"},{"location":"building-microservices/security/#the-human-element","text":"You may also need policies to deal with the human element in your organization: How do you revoke access to credentials when someone leaves the organization? How can you protect yourself against social engineering?","title":"The Human Element"},{"location":"building-microservices/security/#baking-security-in","text":"Helping educate developers about security concerns is key. Also, you can integrate automated vulnerabilities probing tools in your CI build. But when in doubt, reach out to an expert for a thorough check of your system.","title":"Baking Security In"},{"location":"building-microservices/security/#summary","text":"Having a system decomposed into finer-grained services gives us many more options as to how to solve a problem. This concept applies to security too. So, while we have additional problems to solve (e.g. the deputy problem), we can apply a mix of known solutions to solve these problems.","title":"Summary"},{"location":"building-microservices/splitting-the-monolith/","text":"Splitting the monolith \u00b6 Why would you want to split a monolith? There are lots of changes coming to a part of the monolith, splitting that part into a service will make you roll out those changes faster. Separate teams work on separate parts of the monolith. A part of the monolith requires high security measures not needed by the rest of the system. A part of the monolith can be improved by switching technology. How do we go about decomposing monolithic applications without having to embark on a big-bang rewrite? Seams \u00b6 We want our services to be highly cohesive and loosely coupled. The problem with the monolith is that all too often it is the opposite of both. A seam is a portion of the code that can be treated in isolation and worked on without impacting the rest of the codebase. Bounded contexts are good seams. So when splitting, the first step is to identify seams in our system and then gradually move the code of these seams into different packages. Tests are really useful to make sure you're not introducing bugs with this packaging. This process will also help identify seams that you did not think of: they will come out when you are left with some code that you don't know in which package to place. The splitting should start from the seam that is least depended on. Databases \u00b6 We have to find seams in databases too, but this is a difficult task. After having packaged your application code by seams, you should do the same for the code accessing the database (usually the code in the so called repository layer ). Foreign keys \u00b6 Some tables may have foreign keys linking them to other tables. A common solution for this problem is to remove the table relationship and make the service accessing that table call the API of the service handling the other table. Shared static data \u00b6 Let's suppose we have different services accessing a table filled with static data. There are several solutions: Duplicate tables in each db, but this can cause consistency issues. Treat static data as code/configuration files in each service. This can cause consistency issues too, but they would be far easier to solve. Create a microservice to handle the static data. This is overkill in most situations, but it can be justified if the static data has high complexity. Shared mutable data \u00b6 Let's suppose we have different services accessing a table filled with mutable data. Usually this means we need a Customer microservice to handle that data. This service can then be called by Warehouse and Finance . Shared tables \u00b6 Let's suppose we have different services accessing a table which aggregates different information in the same record (catalog entry and stock level). The answer here is to split the table in two, creating a stock levels table for the Warehouse and a catalog entry table for the Catalog . Staging the break \u00b6 The best way to commit the database changes would be to keep the services together and split the schemas. The db split will increase the number of db calls and make you lose transactional integrity. Having the same application will enable you to deal more easily with these problems. Then, when you are satisfied with the new db, you can commit the changes. Transactional Boundaries \u00b6 Transactions allow us to say that operations either all happen together, or none of them happen. Transactions are typically used in databases, but they can be supported but other systems such as message brokers. Splitting schemas will cause the loss of transactional integrity in our system. There are several solutions to this problem: A try again later mechanism, but this alone is not sufficient since it assumes that eventually a failed request will be successful. This is a form of eventual consistency : rather than using a transactional boundary to ensure that the system is in a consistent state when the transaction completes, instead we accept that the system will get itself into a consistent state at some point in the future. Compensating transactions can be used to undo the committed transactions preceding a failed operation. But what if a compensating transaction fails? We would need other mechanism such as automated jobs or human administration. Also, this mechanism becomes more difficult to manage as the number of operations increases in transactions. Distributed transactions are transactions done across different process or network boundaries. They are orchestrated by a transaction manager . The most common algorithm handling short-lived distributed transactions is two-phase commit . With a two-phase commit, first comes the voting phase: each participant in the distributed transaction tells the transaction manager whether it thinks its local transaction can be completed. If the transaction manager gets a yes vote from everyone, then it tells them all to go ahead and perform their commits. A single no vote is enough for the transaction manager to send out a rollback to all parties. Distributed transactions make scaling systems much more difficult, since the transaction manager is a single point of failure and waiting for response while locking resources can cause outages. Also, there is no guarantee that the transactions are actually committed when the clients approve them. Each of these solutions adds complexity. Before implementing business operations happening in a transaction, ask yourself: can they happen in different, local transactions, and rely on the concept of eventual consistency? These systems are much easier to build and scale. If you do encounter state that really needs to be kept consistent, try to avoid splitting it. If you really need to split it, try moving from a purely technical view of the process (e.g., a database transaction) and actually create a concrete concept to represent the transaction. This gives you a hook on which to run other operations like compensating transactions, and a way to monitor and manage these more complex concepts in your system. Reporting \u00b6 When splitting data, we'll come across the problem of splitting reporting data too. The Reporting Database \u00b6 In monolithic systems, aggregating data for reporting is easy. Usually reporting is implemented like this: Benefits: All data is one place so it's easy to query it. Downsides: The db schema is a shared API between the monolith and the reporting service. Cannot optimize schema structure for both use cases. Either the db is optimized for the monolith or the reporting. Cannot use different technology that could be more efficient for reporting. There are several alternatives to this approach when our data is distributed across different services. Data Retrieval via Service Calls \u00b6 A very simple approach: call service APIs and aggregate the results for reporting. Benefits: Easy to implement and works well for small volumes of data (e.g. #orders placed in the last 15 minutes). Downsides: Breaks down when trying to do reporting with large volumes of data (e.g. customer behavior of last 24 months). Reporting systems usually need to integrate with third-party tools over SQL-like interfaces, this approach would require extra work. The API may not have been designed for reporting, leading to an inefficient reporting system and general slowdown. Caching can help, but reporting data is usually historic so there would be a lot of expensive cache misses. Adding reporting-specific APIs can help. Data Pumps \u00b6 Rather than have the reporting system pull the data, the data can instead be pushed to the reporting system. This data pump needs to have intimate knowledge of both the internal database for the service, and also the reporting schema. The pump\u2019s job is to map one from the other. Benefits: Can handle large amounts of data without maintaining a reporting-specific API. Downsides: Causes coupling with the reporting db schema. The reporting service must be treated as a published API that is hard to change. There is also a potential mitigation: exposing only specific schemas that are mapped to an underlying monolithic schema, but this can cause performance issues depending on the db technology choice. Event Data Pump \u00b6 We can write a subscriber listening to microservices events that pushes data in the reporting db. Benefits: Avoids coupling between db schemas. Can see reported data as it happens, opposed to wait for a scheduled data transfer. It is easier to only process new events (i.e. deltas ), while with a data pump we would need to write the code ourselves. The event mapper can be managed by a different team, and it can evolve independently of the services. Downsides: All information must be broadcast as event. It may not scale well with large volumes of data, for which a data pump is more efficient. Backup data pump \u00b6 Using backup data as a source for reporting. This approach was taken by Netflix: backed up Cassandra tables would be stored in Amazon's S3 object store and accessed by Hadoop for reporting. This ended up as a tool named Aegisthus . Benefits: Can handle enormous amounts of data. Efficient if there is already a backup system in place. Downsides: Has coupling with the reporting db schema. Summary \u00b6 We decompose our system by finding seams along which service boundaries can emerge, and this can be an incremental approach. This way, costs of errors are mitigated and we can continue to evolve the system as we proceed.","title":"Splitting the monolith"},{"location":"building-microservices/splitting-the-monolith/#splitting-the-monolith","text":"Why would you want to split a monolith? There are lots of changes coming to a part of the monolith, splitting that part into a service will make you roll out those changes faster. Separate teams work on separate parts of the monolith. A part of the monolith requires high security measures not needed by the rest of the system. A part of the monolith can be improved by switching technology. How do we go about decomposing monolithic applications without having to embark on a big-bang rewrite?","title":"Splitting the monolith"},{"location":"building-microservices/splitting-the-monolith/#seams","text":"We want our services to be highly cohesive and loosely coupled. The problem with the monolith is that all too often it is the opposite of both. A seam is a portion of the code that can be treated in isolation and worked on without impacting the rest of the codebase. Bounded contexts are good seams. So when splitting, the first step is to identify seams in our system and then gradually move the code of these seams into different packages. Tests are really useful to make sure you're not introducing bugs with this packaging. This process will also help identify seams that you did not think of: they will come out when you are left with some code that you don't know in which package to place. The splitting should start from the seam that is least depended on.","title":"Seams"},{"location":"building-microservices/splitting-the-monolith/#databases","text":"We have to find seams in databases too, but this is a difficult task. After having packaged your application code by seams, you should do the same for the code accessing the database (usually the code in the so called repository layer ).","title":"Databases"},{"location":"building-microservices/splitting-the-monolith/#foreign-keys","text":"Some tables may have foreign keys linking them to other tables. A common solution for this problem is to remove the table relationship and make the service accessing that table call the API of the service handling the other table.","title":"Foreign keys"},{"location":"building-microservices/splitting-the-monolith/#shared-static-data","text":"Let's suppose we have different services accessing a table filled with static data. There are several solutions: Duplicate tables in each db, but this can cause consistency issues. Treat static data as code/configuration files in each service. This can cause consistency issues too, but they would be far easier to solve. Create a microservice to handle the static data. This is overkill in most situations, but it can be justified if the static data has high complexity.","title":"Shared static data"},{"location":"building-microservices/splitting-the-monolith/#shared-mutable-data","text":"Let's suppose we have different services accessing a table filled with mutable data. Usually this means we need a Customer microservice to handle that data. This service can then be called by Warehouse and Finance .","title":"Shared mutable data"},{"location":"building-microservices/splitting-the-monolith/#shared-tables","text":"Let's suppose we have different services accessing a table which aggregates different information in the same record (catalog entry and stock level). The answer here is to split the table in two, creating a stock levels table for the Warehouse and a catalog entry table for the Catalog .","title":"Shared tables"},{"location":"building-microservices/splitting-the-monolith/#staging-the-break","text":"The best way to commit the database changes would be to keep the services together and split the schemas. The db split will increase the number of db calls and make you lose transactional integrity. Having the same application will enable you to deal more easily with these problems. Then, when you are satisfied with the new db, you can commit the changes.","title":"Staging the break"},{"location":"building-microservices/splitting-the-monolith/#transactional-boundaries","text":"Transactions allow us to say that operations either all happen together, or none of them happen. Transactions are typically used in databases, but they can be supported but other systems such as message brokers. Splitting schemas will cause the loss of transactional integrity in our system. There are several solutions to this problem: A try again later mechanism, but this alone is not sufficient since it assumes that eventually a failed request will be successful. This is a form of eventual consistency : rather than using a transactional boundary to ensure that the system is in a consistent state when the transaction completes, instead we accept that the system will get itself into a consistent state at some point in the future. Compensating transactions can be used to undo the committed transactions preceding a failed operation. But what if a compensating transaction fails? We would need other mechanism such as automated jobs or human administration. Also, this mechanism becomes more difficult to manage as the number of operations increases in transactions. Distributed transactions are transactions done across different process or network boundaries. They are orchestrated by a transaction manager . The most common algorithm handling short-lived distributed transactions is two-phase commit . With a two-phase commit, first comes the voting phase: each participant in the distributed transaction tells the transaction manager whether it thinks its local transaction can be completed. If the transaction manager gets a yes vote from everyone, then it tells them all to go ahead and perform their commits. A single no vote is enough for the transaction manager to send out a rollback to all parties. Distributed transactions make scaling systems much more difficult, since the transaction manager is a single point of failure and waiting for response while locking resources can cause outages. Also, there is no guarantee that the transactions are actually committed when the clients approve them. Each of these solutions adds complexity. Before implementing business operations happening in a transaction, ask yourself: can they happen in different, local transactions, and rely on the concept of eventual consistency? These systems are much easier to build and scale. If you do encounter state that really needs to be kept consistent, try to avoid splitting it. If you really need to split it, try moving from a purely technical view of the process (e.g., a database transaction) and actually create a concrete concept to represent the transaction. This gives you a hook on which to run other operations like compensating transactions, and a way to monitor and manage these more complex concepts in your system.","title":"Transactional Boundaries"},{"location":"building-microservices/splitting-the-monolith/#reporting","text":"When splitting data, we'll come across the problem of splitting reporting data too.","title":"Reporting"},{"location":"building-microservices/splitting-the-monolith/#the-reporting-database","text":"In monolithic systems, aggregating data for reporting is easy. Usually reporting is implemented like this: Benefits: All data is one place so it's easy to query it. Downsides: The db schema is a shared API between the monolith and the reporting service. Cannot optimize schema structure for both use cases. Either the db is optimized for the monolith or the reporting. Cannot use different technology that could be more efficient for reporting. There are several alternatives to this approach when our data is distributed across different services.","title":"The Reporting Database"},{"location":"building-microservices/splitting-the-monolith/#data-retrieval-via-service-calls","text":"A very simple approach: call service APIs and aggregate the results for reporting. Benefits: Easy to implement and works well for small volumes of data (e.g. #orders placed in the last 15 minutes). Downsides: Breaks down when trying to do reporting with large volumes of data (e.g. customer behavior of last 24 months). Reporting systems usually need to integrate with third-party tools over SQL-like interfaces, this approach would require extra work. The API may not have been designed for reporting, leading to an inefficient reporting system and general slowdown. Caching can help, but reporting data is usually historic so there would be a lot of expensive cache misses. Adding reporting-specific APIs can help.","title":"Data Retrieval via Service Calls"},{"location":"building-microservices/splitting-the-monolith/#data-pumps","text":"Rather than have the reporting system pull the data, the data can instead be pushed to the reporting system. This data pump needs to have intimate knowledge of both the internal database for the service, and also the reporting schema. The pump\u2019s job is to map one from the other. Benefits: Can handle large amounts of data without maintaining a reporting-specific API. Downsides: Causes coupling with the reporting db schema. The reporting service must be treated as a published API that is hard to change. There is also a potential mitigation: exposing only specific schemas that are mapped to an underlying monolithic schema, but this can cause performance issues depending on the db technology choice.","title":"Data Pumps"},{"location":"building-microservices/splitting-the-monolith/#event-data-pump","text":"We can write a subscriber listening to microservices events that pushes data in the reporting db. Benefits: Avoids coupling between db schemas. Can see reported data as it happens, opposed to wait for a scheduled data transfer. It is easier to only process new events (i.e. deltas ), while with a data pump we would need to write the code ourselves. The event mapper can be managed by a different team, and it can evolve independently of the services. Downsides: All information must be broadcast as event. It may not scale well with large volumes of data, for which a data pump is more efficient.","title":"Event Data Pump"},{"location":"building-microservices/splitting-the-monolith/#backup-data-pump","text":"Using backup data as a source for reporting. This approach was taken by Netflix: backed up Cassandra tables would be stored in Amazon's S3 object store and accessed by Hadoop for reporting. This ended up as a tool named Aegisthus . Benefits: Can handle enormous amounts of data. Efficient if there is already a backup system in place. Downsides: Has coupling with the reporting db schema.","title":"Backup data pump"},{"location":"building-microservices/splitting-the-monolith/#summary","text":"We decompose our system by finding seams along which service boundaries can emerge, and this can be an incremental approach. This way, costs of errors are mitigated and we can continue to evolve the system as we proceed.","title":"Summary"},{"location":"building-microservices/testing/","text":"Testing \u00b6 Distributed systems add complexity in automated tests too. Types of Tests \u00b6 Tests can be categorized by the following diagram: In microservices, the amount of manual tests should be kept at a minimum in order to reduce test times. Also, since there are no significant difference in manual testing, we will examine how automated testing changes from monolithic systems to microservices systems. Test scope \u00b6 The Test Pyramid is a model proposed by Mike Cohn to associate the ideal amount of tests to each test scope. Note that terms like service and unit in this context are ambiguous and we will refer to the UI layer as end-to-end tests. As we go up the pyramid our confidence increases but we reduce the ability to pinpoint bug causes and have a slower feedback. Ideally, you want test of different scopes for different purposes (e.g. you can catch an integration bug with an e2e test and then you can keep it covered with a unit test). How many tests? It's better to increase the number of tests as you go down the pyramid. Doing the opposite has the potential to keep your build red for long times. Unit Tests \u00b6 They typically test a single function or method call in isolation (i.e. without starting services or using external resources such as network connectivity). Done right, they can be very fast. You could run a lot of them in less than a minute. These are technology-facing tests that will help us catch the most bugs and guide us through code restructuring thanks to their fast feedback and reliability. Unit tests are also easier to be implemented than other tests. Service Tests \u00b6 They are designed to bypass the user interface and test services directly. In monolithic systems, a group of classes that provide a certain service to users can be tested together. In microservices, we need to isolate the service we want to test so that we are able to quickly find the root cause of a bug. To achieve this isolation, we need to stub out other services interacting with the one under test. But note that while a stubbed service does not care if it's called 1 or 100 times, a mocked service can provide you with that information so you could write more solid tests. The downside is that mocked services can make your tests brittle because of the magnitude of details tested. So, after our service tests pass, we are confident that the new microservice is ready to contact other microservices with no errors. But what about other microservices calling the one we want to deploy? End-to-End Tests \u00b6 They are run against the whole system, so they cover a lot of code and give you a lot of confidence that the system will work in production. On the other hand, it's harder to diagnose an issue that comes up in e2e tests. These tests are tricky to deal with, suppose we add them at the end of our deploy pipeline: Then we have 2 issues: Which services version are we going to use in our tests? Executing such a pipeline for each microservice is going to be really inefficient. Both of them are solved with a fan in model: But there are other disadvantages when using e2e tests: As the scope increases, we might face more errors due to causes unrelated to the behavior we want to test (e.g. network failures). When tests sometimes fail because of unrelated issues (these are called flaky tests ), people will tend to re-run them without any effort to understand the errors. This will cause lots of scheduled builds and lead to a broken system because some issues (e.g. concurrency issues) may slip through this process as unrelated issues. Flaky tests will also cause a normalization of deviance in the test system, so it's mandatory to remove them (or temporarily disable them to apply a fix) as soon as they're spotted. Ownership \u00b6 Usually the tests of a service are owned by the team developing the service. But in e2e tests there can be multiple services involved in a single test. Avoid: A free for all approach, because no team would have a real ownership of any test, causing failing tests to be ignored or easily dismissed as responsibility of another team. A test team whose job is solely to write e2e tests. A team like this would not know how to fix issues caught in tests and would cause the development team to become distant from the tests of their code. Instead, aim to share responsibilities between the teams involved in each single test: everyone should be able to add a new test and it must be clear who is responsible for the success of that test. Speed \u00b6 When e2e tests are too slow: They are more prone to unrelated issues happening during their execution, this makes them brittle tests. The feedback cycle is slowed, so it takes more time to fix a failing build. This will cause builds to pile up: we will no longer able to quickly deploy small features. Things can be sped up by running tests in parallel, but it can only help to a certain extent. To speed up your e2e tests, you should aim to remove useless tests and weigh risk/rewards of implemented tests to determine if they're worth having around. But this is a really difficult task since humans are not that good at estimating risks. The Metaversion \u00b6 After running e2e tests one can start thinking, So, I know all these services at these versions work together, so why not deploy them all together? . This reasoning is to avoid because it usually leads to a proposal of a unified version number for the system, which in turn leads to coupling deploys. In the end, it will lead to a microservices system without the benefits of microservices. Test Journeys, Not Stories \u00b6 Despite the disadvantages, e2e test can be doable with one or two services. So what if we have 20 services? Avoid: Adding an e2e for each story to implement because it leads to slow feedback times and huge overlaps in test coverage. Instead, you should test on a few core journeys. Any functionality not covered in these journeys needs to be covered in tests that analyze services in isolation from each other. Consumer-Driven Tests \u00b6 We use e2e to be sure that when a new service is deployed the system will keep working. Another way to approach this problem is by defining consumer-driven contracts (CDC) , which express the expectations of the consumers of a service. These tests can be run in isolation and are at the service test scope, although they serve a different purpose from classic service tests: we are able to identify breaking changes and decide whether to introduce breaking changes in the involved consumer service too or think through it before deploying the new service. The major benefit is that these tests are not as expensive as e2e tests, while they give us confidence that the new service does not break the expectations of its consumers. Consumer-driven tests can be implemented by the teams developing consumer services but this requires a good communication channel between the teams. If for some reason it's hard to communicate, they can be written by the team developing the new service. Testing after production \u00b6 Most testing is done before the system is in production. Still, when the system is in production: Some bugs may have slipped through our tests. New failure modes are discovered. Our users use the system in ways we could never expect. One reaction to this is often to define more and more tests. However, at a certain point we have we will hit diminishing returns with this approach. Separating Deployment from Release \u00b6 If we deploy the system in an environment prior to directing production loads against it, we are able to detect issues specific to the environment. In this environment we can also run smoke tests to make sure that the deploy was successful and there are no environmental issues. There are different practices that you can adopt to follow this approach. Blue/Green deployment \u00b6 With blue/green deployment we have two copies of our software deployed at a time, but only one version of it is receiving real requests. This diagram describes how blue/green deployment works: Benefits: Smoke tests can be run after the deploy. If we keep the old version running, we can quickly fallback to it if we detect errors in the new version. The detect and revert process can be automated too. Zero downtime between deploys can be achieved. Downsides: Requires some networking engineering but usually cloud providers support the needed functionalities. Canary Releasing \u00b6 With canary releasing, we verify our newly deployed service by directing amounts of production traffic against it to see if it performs as expected. Performance can be measured the way you prefer, some examples are: Measuring response times. Measuring error rates. Measuring sales conversions of a new recommendation algorithm. When considering canary releasing, you need to decide if you are going to divert a portion of production requests to the canary or just copy production load. Benefits: Has all the benefits provided by blue/green deployment. Lets you evaluate the performance of the new service according to custom/complex metrics. Downsides: More difficult to setup than blue/green deployment. Needs advanced network routing capabilities. May need more computing power because of multiple services that have to run together for long times. MTBF vs MTTR \u00b6 By using techniques such as blue/green and canary deployment we acknowledge that some issues can only be discovered in production: sometimes expending the same effort into getting better at remediation of a release can be significantly more beneficial than adding more automated tests. This is referred to as the trade-off between optimizing for mean time between failures ( MTBF ) and mean time to repair ( MTTR ). For different organizations, this trade-off between MTBF and MTTR will vary, and much of this lies with understanding the true impact of failure in a production environment. Cross-Functional Testing \u00b6 Nonfunctional requirements describe those characteristics your system exhibits that cannot simply be implemented like a normal feature. We will use the term cross-functional requirements (CFR) instead to refer to these tests. As example, these are popular CFRs: Latency of a web server. Maximum concurrent users. How secure the stored data should be. Tests around CFRs should follow the pyramid too: some tests will have to be end-to-end (e.g. load tests) but others won\u2019t (e.g. tests to catch performance bottlenecks). It's really important to consider CFRs from the start of the development process, because they shape too the design of your system. Performance Tests \u00b6 Performance tests are a way of ensuring that some of our CFRs can be met. In microservices systems performance is critical since what would be a single database call in a monolithic system could now become 3-4 calls to different services. As with functional tests, you may want different performance tests for each test scope. Due to the time it takes to run performance tests, it isn\u2019t always feasible to run them on every check-in. It is a common practice to run a subset every day, and a larger set every week. Also, it's important to have targets so that the results of these tests can be correctly interpreted and evaluated (they may mark a build as failed is the performance level is below a certain threshold). Summary \u00b6 Fundamental notions: Optimize for fast feedback, and separate types of tests accordingly. Avoid the need for end-to-end tests wherever possible by using consumer-driven contracts. Use consumer-driven contracts to provide focus points for conversations between teams. Try to understand the trade-off between putting more efforts into testing and detecting issues faster in production (optimizing for MTBF versus MTTR).","title":"Testing"},{"location":"building-microservices/testing/#testing","text":"Distributed systems add complexity in automated tests too.","title":"Testing"},{"location":"building-microservices/testing/#types-of-tests","text":"Tests can be categorized by the following diagram: In microservices, the amount of manual tests should be kept at a minimum in order to reduce test times. Also, since there are no significant difference in manual testing, we will examine how automated testing changes from monolithic systems to microservices systems.","title":"Types of Tests"},{"location":"building-microservices/testing/#test-scope","text":"The Test Pyramid is a model proposed by Mike Cohn to associate the ideal amount of tests to each test scope. Note that terms like service and unit in this context are ambiguous and we will refer to the UI layer as end-to-end tests. As we go up the pyramid our confidence increases but we reduce the ability to pinpoint bug causes and have a slower feedback. Ideally, you want test of different scopes for different purposes (e.g. you can catch an integration bug with an e2e test and then you can keep it covered with a unit test). How many tests? It's better to increase the number of tests as you go down the pyramid. Doing the opposite has the potential to keep your build red for long times.","title":"Test scope"},{"location":"building-microservices/testing/#unit-tests","text":"They typically test a single function or method call in isolation (i.e. without starting services or using external resources such as network connectivity). Done right, they can be very fast. You could run a lot of them in less than a minute. These are technology-facing tests that will help us catch the most bugs and guide us through code restructuring thanks to their fast feedback and reliability. Unit tests are also easier to be implemented than other tests.","title":"Unit Tests"},{"location":"building-microservices/testing/#service-tests","text":"They are designed to bypass the user interface and test services directly. In monolithic systems, a group of classes that provide a certain service to users can be tested together. In microservices, we need to isolate the service we want to test so that we are able to quickly find the root cause of a bug. To achieve this isolation, we need to stub out other services interacting with the one under test. But note that while a stubbed service does not care if it's called 1 or 100 times, a mocked service can provide you with that information so you could write more solid tests. The downside is that mocked services can make your tests brittle because of the magnitude of details tested. So, after our service tests pass, we are confident that the new microservice is ready to contact other microservices with no errors. But what about other microservices calling the one we want to deploy?","title":"Service Tests"},{"location":"building-microservices/testing/#end-to-end-tests","text":"They are run against the whole system, so they cover a lot of code and give you a lot of confidence that the system will work in production. On the other hand, it's harder to diagnose an issue that comes up in e2e tests. These tests are tricky to deal with, suppose we add them at the end of our deploy pipeline: Then we have 2 issues: Which services version are we going to use in our tests? Executing such a pipeline for each microservice is going to be really inefficient. Both of them are solved with a fan in model: But there are other disadvantages when using e2e tests: As the scope increases, we might face more errors due to causes unrelated to the behavior we want to test (e.g. network failures). When tests sometimes fail because of unrelated issues (these are called flaky tests ), people will tend to re-run them without any effort to understand the errors. This will cause lots of scheduled builds and lead to a broken system because some issues (e.g. concurrency issues) may slip through this process as unrelated issues. Flaky tests will also cause a normalization of deviance in the test system, so it's mandatory to remove them (or temporarily disable them to apply a fix) as soon as they're spotted.","title":"End-to-End Tests"},{"location":"building-microservices/testing/#ownership","text":"Usually the tests of a service are owned by the team developing the service. But in e2e tests there can be multiple services involved in a single test. Avoid: A free for all approach, because no team would have a real ownership of any test, causing failing tests to be ignored or easily dismissed as responsibility of another team. A test team whose job is solely to write e2e tests. A team like this would not know how to fix issues caught in tests and would cause the development team to become distant from the tests of their code. Instead, aim to share responsibilities between the teams involved in each single test: everyone should be able to add a new test and it must be clear who is responsible for the success of that test.","title":"Ownership"},{"location":"building-microservices/testing/#speed","text":"When e2e tests are too slow: They are more prone to unrelated issues happening during their execution, this makes them brittle tests. The feedback cycle is slowed, so it takes more time to fix a failing build. This will cause builds to pile up: we will no longer able to quickly deploy small features. Things can be sped up by running tests in parallel, but it can only help to a certain extent. To speed up your e2e tests, you should aim to remove useless tests and weigh risk/rewards of implemented tests to determine if they're worth having around. But this is a really difficult task since humans are not that good at estimating risks.","title":"Speed"},{"location":"building-microservices/testing/#the-metaversion","text":"After running e2e tests one can start thinking, So, I know all these services at these versions work together, so why not deploy them all together? . This reasoning is to avoid because it usually leads to a proposal of a unified version number for the system, which in turn leads to coupling deploys. In the end, it will lead to a microservices system without the benefits of microservices.","title":"The Metaversion"},{"location":"building-microservices/testing/#test-journeys-not-stories","text":"Despite the disadvantages, e2e test can be doable with one or two services. So what if we have 20 services? Avoid: Adding an e2e for each story to implement because it leads to slow feedback times and huge overlaps in test coverage. Instead, you should test on a few core journeys. Any functionality not covered in these journeys needs to be covered in tests that analyze services in isolation from each other.","title":"Test Journeys, Not Stories"},{"location":"building-microservices/testing/#consumer-driven-tests","text":"We use e2e to be sure that when a new service is deployed the system will keep working. Another way to approach this problem is by defining consumer-driven contracts (CDC) , which express the expectations of the consumers of a service. These tests can be run in isolation and are at the service test scope, although they serve a different purpose from classic service tests: we are able to identify breaking changes and decide whether to introduce breaking changes in the involved consumer service too or think through it before deploying the new service. The major benefit is that these tests are not as expensive as e2e tests, while they give us confidence that the new service does not break the expectations of its consumers. Consumer-driven tests can be implemented by the teams developing consumer services but this requires a good communication channel between the teams. If for some reason it's hard to communicate, they can be written by the team developing the new service.","title":"Consumer-Driven Tests"},{"location":"building-microservices/testing/#testing-after-production","text":"Most testing is done before the system is in production. Still, when the system is in production: Some bugs may have slipped through our tests. New failure modes are discovered. Our users use the system in ways we could never expect. One reaction to this is often to define more and more tests. However, at a certain point we have we will hit diminishing returns with this approach.","title":"Testing after production"},{"location":"building-microservices/testing/#separating-deployment-from-release","text":"If we deploy the system in an environment prior to directing production loads against it, we are able to detect issues specific to the environment. In this environment we can also run smoke tests to make sure that the deploy was successful and there are no environmental issues. There are different practices that you can adopt to follow this approach.","title":"Separating Deployment from Release"},{"location":"building-microservices/testing/#bluegreen-deployment","text":"With blue/green deployment we have two copies of our software deployed at a time, but only one version of it is receiving real requests. This diagram describes how blue/green deployment works: Benefits: Smoke tests can be run after the deploy. If we keep the old version running, we can quickly fallback to it if we detect errors in the new version. The detect and revert process can be automated too. Zero downtime between deploys can be achieved. Downsides: Requires some networking engineering but usually cloud providers support the needed functionalities.","title":"Blue/Green deployment"},{"location":"building-microservices/testing/#canary-releasing","text":"With canary releasing, we verify our newly deployed service by directing amounts of production traffic against it to see if it performs as expected. Performance can be measured the way you prefer, some examples are: Measuring response times. Measuring error rates. Measuring sales conversions of a new recommendation algorithm. When considering canary releasing, you need to decide if you are going to divert a portion of production requests to the canary or just copy production load. Benefits: Has all the benefits provided by blue/green deployment. Lets you evaluate the performance of the new service according to custom/complex metrics. Downsides: More difficult to setup than blue/green deployment. Needs advanced network routing capabilities. May need more computing power because of multiple services that have to run together for long times.","title":"Canary Releasing"},{"location":"building-microservices/testing/#mtbf-vs-mttr","text":"By using techniques such as blue/green and canary deployment we acknowledge that some issues can only be discovered in production: sometimes expending the same effort into getting better at remediation of a release can be significantly more beneficial than adding more automated tests. This is referred to as the trade-off between optimizing for mean time between failures ( MTBF ) and mean time to repair ( MTTR ). For different organizations, this trade-off between MTBF and MTTR will vary, and much of this lies with understanding the true impact of failure in a production environment.","title":"MTBF vs MTTR"},{"location":"building-microservices/testing/#cross-functional-testing","text":"Nonfunctional requirements describe those characteristics your system exhibits that cannot simply be implemented like a normal feature. We will use the term cross-functional requirements (CFR) instead to refer to these tests. As example, these are popular CFRs: Latency of a web server. Maximum concurrent users. How secure the stored data should be. Tests around CFRs should follow the pyramid too: some tests will have to be end-to-end (e.g. load tests) but others won\u2019t (e.g. tests to catch performance bottlenecks). It's really important to consider CFRs from the start of the development process, because they shape too the design of your system.","title":"Cross-Functional Testing"},{"location":"building-microservices/testing/#performance-tests","text":"Performance tests are a way of ensuring that some of our CFRs can be met. In microservices systems performance is critical since what would be a single database call in a monolithic system could now become 3-4 calls to different services. As with functional tests, you may want different performance tests for each test scope. Due to the time it takes to run performance tests, it isn\u2019t always feasible to run them on every check-in. It is a common practice to run a subset every day, and a larger set every week. Also, it's important to have targets so that the results of these tests can be correctly interpreted and evaluated (they may mark a build as failed is the performance level is below a certain threshold).","title":"Performance Tests"},{"location":"building-microservices/testing/#summary","text":"Fundamental notions: Optimize for fast feedback, and separate types of tests accordingly. Avoid the need for end-to-end tests wherever possible by using consumer-driven contracts. Use consumer-driven contracts to provide focus points for conversations between teams. Try to understand the trade-off between putting more efforts into testing and detecting issues faster in production (optimizing for MTBF versus MTTR).","title":"Summary"},{"location":"oauth2-and-openid-connect/","text":"OAuth 2.0 and OpenID Connect \u00b6 Notes on the OAuth 2.0 authorization framework and the OpenID Connect identity layer. Topics: JWT (JSON Web Token) OAuth 2.0 OpenID Connect","title":"Home"},{"location":"oauth2-and-openid-connect/#oauth-20-and-openid-connect","text":"Notes on the OAuth 2.0 authorization framework and the OpenID Connect identity layer. Topics: JWT (JSON Web Token) OAuth 2.0 OpenID Connect","title":"OAuth 2.0 and OpenID Connect"},{"location":"oauth2-and-openid-connect/jwt/","text":"JWT (JSON Web Token) \u00b6 JSON Web Token is a standard for creating JSON access tokens that can assert a number of claims. JWTs are stateless because servers do not need to store any session data in order to validate these tokens. Structure \u00b6 This is the structure of JWTs: Component Example Description Header { \"alg\" : \"HS256\", \"typ\" : \"JWT\" } Contains token metadata, such as the algorithm used to generate the signature. Payload { \"loggedInAs\" : \"admin\", \"iat\" : 1422779638 } Contains a set of claims. They can be standard or custom. Signature HMAC-SHA256(base64url(header) + '.' + base64url(payload), secret) The token signature, used to to provide integrity and authenticity proof. The actual token is produced by the following function: base64url(header) + '.' + base64url(payload) + '.' + base64url(signature) The token can be safely passed into HTML and HTTP. Standard fields \u00b6 Header \u00b6 This table enumerates the standard fields that can be used inside a JWT header. Name Extended name Description typ Token type If present, it is recommended to set this to JWT . cty Content type If nested signing or encryption is employed, it is recommended to set this to JWT . Otherwise, omit this field. alg Message authentication code algorithm The issuer can freely set an algorithm to verify the signature on the token. However, some supported algorithms are insecure. Any other field allowed by the JWS and JWE standards. Payload \u00b6 This table enumerates the standard claims that can be used inside a JWT payload. Name Extended name Description iss Issuer The principal that issued the JWT. sub Subject The subject of the JWT. aud Audience Lists the recipients that the JWT is intended for. If the principal processing the claim does not identify itself with a value in the aud claim when this claim is present, then the JWT must be rejected. exp Expiration Time Identifies the expiration time on and after which the JWT must not be accepted for processing. The value must be a NumericDate (i.e. the number of seconds past 1970-01-01 00:00:00Z ). nbf Not Before Identifies the time on which the JWT will start to be accepted for processing. The value must be a NumericDate. iat Issued at Identifies the time at which the JWT was issued. The value must be a NumericDate. jti JWT ID Case sensitive unique identifier of the token even among different issuers. How JWTs are commonly used \u00b6 This is how JWTs are commonly used: The client requests a token by providing credentials to the issuer. The issuer validates the client's request and responds with the token. The client can use the token to access some protected resources . For such requests, the client typically sends the token in the Authorization header, using the Bearer schema. Example: Authorization: Bearer abc123...321cba . How to validate a JWT \u00b6 Here's how the JWT validation algorithm works: Deserialize the token. Validate the standard claims : iss , aud , exp , nbf . If you want to allow manual token expiration before the expiration date, you also need to check the token against a data store . By doing so, your tokens will not be stateless anymore. Validate the signature using the algorithm specified in the token's header.","title":"JWT (JSON Web Token)"},{"location":"oauth2-and-openid-connect/jwt/#jwt-json-web-token","text":"JSON Web Token is a standard for creating JSON access tokens that can assert a number of claims. JWTs are stateless because servers do not need to store any session data in order to validate these tokens.","title":"JWT (JSON Web Token)"},{"location":"oauth2-and-openid-connect/jwt/#structure","text":"This is the structure of JWTs: Component Example Description Header { \"alg\" : \"HS256\", \"typ\" : \"JWT\" } Contains token metadata, such as the algorithm used to generate the signature. Payload { \"loggedInAs\" : \"admin\", \"iat\" : 1422779638 } Contains a set of claims. They can be standard or custom. Signature HMAC-SHA256(base64url(header) + '.' + base64url(payload), secret) The token signature, used to to provide integrity and authenticity proof. The actual token is produced by the following function: base64url(header) + '.' + base64url(payload) + '.' + base64url(signature) The token can be safely passed into HTML and HTTP.","title":"Structure"},{"location":"oauth2-and-openid-connect/jwt/#standard-fields","text":"","title":"Standard fields"},{"location":"oauth2-and-openid-connect/jwt/#header","text":"This table enumerates the standard fields that can be used inside a JWT header. Name Extended name Description typ Token type If present, it is recommended to set this to JWT . cty Content type If nested signing or encryption is employed, it is recommended to set this to JWT . Otherwise, omit this field. alg Message authentication code algorithm The issuer can freely set an algorithm to verify the signature on the token. However, some supported algorithms are insecure. Any other field allowed by the JWS and JWE standards.","title":"Header"},{"location":"oauth2-and-openid-connect/jwt/#payload","text":"This table enumerates the standard claims that can be used inside a JWT payload. Name Extended name Description iss Issuer The principal that issued the JWT. sub Subject The subject of the JWT. aud Audience Lists the recipients that the JWT is intended for. If the principal processing the claim does not identify itself with a value in the aud claim when this claim is present, then the JWT must be rejected. exp Expiration Time Identifies the expiration time on and after which the JWT must not be accepted for processing. The value must be a NumericDate (i.e. the number of seconds past 1970-01-01 00:00:00Z ). nbf Not Before Identifies the time on which the JWT will start to be accepted for processing. The value must be a NumericDate. iat Issued at Identifies the time at which the JWT was issued. The value must be a NumericDate. jti JWT ID Case sensitive unique identifier of the token even among different issuers.","title":"Payload"},{"location":"oauth2-and-openid-connect/jwt/#how-jwts-are-commonly-used","text":"This is how JWTs are commonly used: The client requests a token by providing credentials to the issuer. The issuer validates the client's request and responds with the token. The client can use the token to access some protected resources . For such requests, the client typically sends the token in the Authorization header, using the Bearer schema. Example: Authorization: Bearer abc123...321cba .","title":"How JWTs are commonly used"},{"location":"oauth2-and-openid-connect/jwt/#how-to-validate-a-jwt","text":"Here's how the JWT validation algorithm works: Deserialize the token. Validate the standard claims : iss , aud , exp , nbf . If you want to allow manual token expiration before the expiration date, you also need to check the token against a data store . By doing so, your tokens will not be stateless anymore. Validate the signature using the algorithm specified in the token's header.","title":"How to validate a JWT"},{"location":"oauth2-and-openid-connect/oauth2/","text":"OAuth 2.0 \u00b6 OAuth 2.0 is an open framework that allows to securely obtain authorization to access resources via HTTP. For example, you can tell Google that Trello is allowed to access your Google Calendar in order to associate tasks with dates, without sharing your Google password with Trello . This is possible because Trello can send OAuth 2.0 tokens to Google which prove your consent. OAuth 2.0 is not considered to be a protocol because the specification is rather vague. In fact, this is one of the reasons why Eran Hammer resigned from his role of lead author for OAuth 2.0. In the following sections we will go through the main concepts of the OAuth 2.0 framework. Purpose \u00b6 One of the best analogies is to consider an OAuth 2.0 token to be the valet key to your car. By using the valet key, the valet can start and move (for a limited distance) your car but he can't access the trunk. An OAuth 2.0 token serves the same purpose of the valet key. As a resource owner, you decide which data each consumer is allowed to access from your service providers. You can give each consumer a different valet key, while being sure that none of these consumers has access to your full key or to any data from which they can build a full key. Roles \u00b6 Roles identify the actors that can participate in OAuth 2.0 flows. There are 4 roles: Resource Owner : the user who is giving authorization to access his account information. Client : the application that attempts to get access to the user's account. It needs the user's permission in order to get this access. Authorization Server : the server that allows users to approve or deny an authorization request. Resource Server : the server that consumes the user's information. This is how OAuth 2.0 roles would be translated in our example: Endpoints \u00b6 Endpoints are URIs that define the location of authorization services. In OAuth 2.0, there are 3 endpoints: The Authorization endpoint is on the authorization server. It's used for the resource owner's log in and grants authorization to the client application. The Redirect endpoint is on the client application. The resource owner is redirected to this endpoint after having granted authorization at the authorization endpoint. The Token endpoint is on the authorization server. This is where the client application exchanges some secret information for an access token. Note that some OAuth 2.0 grants don't involve all these endpoints but just some of them. This is how the OAuth 2.0 endpoints would be translated in our Trello example: Scopes \u00b6 Scopes can be used to limit a client's access to the user's resources. Basically: A client can request one or more scopes. The requested scopes are presented to the user in the consent screen. If the user gives his consent, the access token issued to the application will be limited to those scopes. The OAuth 2.0 specification does not define any specific values for scopes, since they are highly dependent on the service provider. For example, these are the scopes for the Google Calendar's API: Grants \u00b6 Authorization grants are methods for a client to get an access token . The access token proves that a client has the permission to access the user's data. Authorization grants are also known as flows . OAuth 2.0 recommends to use one of the following grants: Authorization code Authorization code with PKCE Client credentials Device authorization Refresh token But there are also some grants whose usage is discouraged by the OAuth 2.0 Security Best Current Practice document : Implicit grant Password grant Authorization code grant \u00b6 The Authorization code grant has been designed for clients which can securely store a client secret. This grant is typically used when the client is a web server. Here's how the Authorization code grant would work in our Trello example: The user clicks on a button in the Trello's web app in order to connect his Google account to Trello. The Trello web app requests an authorization code. This operation can be done in several ways (e.g. redirection, link on HTML button). The code can be retrieved by making a HTTP GET request to the Google account's authorization endpoint with these parameters: client_id : the public id of the client. This was determined by the authorization server when the client was registered. response_type : specifies the grant type. By setting this parameter to code , the client indicates that it wants to start an authorization code grant. state : used to prevent CSRF attacks . redirect_uri : this is where the user will be sent after he approves the permission request. scope : describes the scope of the authorization (optional). The Google account server responds with the consent screen page. The user examines the requested permissions in the consent screen and then gives consent to Trello. The Google account server redirects the user to the redirect_uri on the client with these parameters: code : the authorization code. state : the state that was previously set for this authorization request. Trello now has the authorization code and exchanges it for the access token to the user's Google account by making a HTTP POST request: code : the authorization code. grant_type : specifies the grant type. By setting this parameter to authorization_code , the client indicates that it wants to complete an authorization code grant. redirect_uri : the uri where the authorization code has been previously sent. client_id : the public id of the client. This was determined by the authorization server when the client was registered. client_secret : the private secret stored by the client. It was associated by the authorization server to the client_id when the client was registered. Trello uses the access token to create an event in the user's Google Calendar. You can try the Authorization code grant in the OAuth.net playground . Proof Key for Code Exchange (PKCE) \u00b6 PKCE extends the Authorization code grant to securely perform the code exchange with clients that cannot privately store the authorization code . This technique can be applied to any client, although it's typically used when the client is a mobile or a web browser application. Here's how the Authorization code grant with PKCE would work in our Trello example: The user clicks on a button in the Trello's web app in order to connect his Google account to Trello. The Trello web app generates a code verifier (i.e. a cryptographically secure random string) and the respective code challenge , which can be an hash of the code verifier or the code verifier itself. This code must be saved in the web app for later use. The Trello web app requests an authorization code. This operation can be done in several ways (e.g. redirection, link on HTML button). The code can be retrieved by making a HTTP GET request to the Google account's authorization endpoint with these parameters: client_id : the public id of the client. This was determined by the authorization server when the client was registered. response_type : specifies the grant type. By setting this parameter to code , the client indicates that it wants to start an authorization code grant. state : used to prevent CSRF attacks . redirect_uri : this is where the user will be sent after he approves the permission request. scope : describes the scope of the authorization (optional). code_challenge : used to send the code challenge previously generated by the Trello web app. code_challenge_method : if it's set to S256 , indicates that the code verifier will be hashed with SHA256 and then base64-url-encoded into the code challenge. Otherwise, the secret code will be used as the code challenge. The Google account server responds with the consent screen page. The user examines the requested permissions in the consent screen and then gives consent to Trello. The Google account server redirects the user to the redirect_uri on the client with these parameters: code : the authorization code. state : the state that was previously set for this authorization request. Trello now has the authorization code and exchanges it for the access token to the user's Google account by making a HTTP POST request: code : the authorization code. grant_type : specifies the grant type. By setting this parameter to code , the client indicates that it wants to complete an authorization code grant. redirect_uri : the uri where the authorization code has been previously sent. client_id : the public id of the client. This was determined by the authorization server when the client was registered. code_verifier : the code verifier that was initially generated by the Trello client. Google Account will check whether it matches the challenge that was specified by Trello in the authorization request. If an attacker intercepted the authorization code, it won't be able to exchange it for an access token. Trello uses the access token to create an event in the user's Google Calendar. You can try the Authorization code grant with PKCE in the OAuth.net playground . Client credentials grant \u00b6 The Client credentials grant is used when clients need to be authorized to act on behalf of a service account rather than a human user. For example, let's suppose that Trello has a system-internal service that computes statistics about users to create some reports. In order to do so, the Report service needs to be authorized to read the Trello users' data. Here's how the client credentials grant would work in this example: The Report service requests the access token that represents its service account by making a HTTP POST request: grant_type : specifies the grant type. By setting this parameter to client_credentials , the client indicates that it wants to complete a client credentials grant. client_id : the public id of the client. This was determined by the authorization server when the client was registered. client_secret : the private secret stored by the client. It was associated by the authorization server to the client_id when the client was registered. The Report service uses the access token to read the Trello users' data. Device authorization grant \u00b6 The Device authorization grant has been designed for devices with limited input capabilities. For example, let's suppose we are using a Trello app for Apple TV and you want to authorize the app to access your Google Calendar's data. Here's how the device authorization grant would work in this case: The user clicks on a button in the Trello's web app in order to connect his Google account to Trello. The Trello app requests a device code with a HTTP POST request to the device endpoint on the authorization server: client_id : the public id of the client. This was determined by the authorization server when the client was registered. The authorization server responds with the requested data: device_code : a code representing the device, to be later displayed to the user. user_code : a code representing the user, to be later displayed to the user. verification_uri : represents the URL the user must visit to enter the user_code and device_code. interval : the minimum amount of time in seconds that the client should wait between polling requests to the token endpoint. expires_in : the lifetime in seconds of the device_code and user_code. (optional) The Trello app displays a message to the user, instructing him to visit the verification_url and the insert the user_code and device_code. The user visits the verification_uri, then approves the authorization request by inserting the user_code and device_code. While the Trello app was waiting for the user to approve the authorization request, it was polling the token endpoint with an HTTP POST request: grant_type : set this parameter to urn:ietf:params:oauth:grant-type:device_code to complete a device authorization grant. client_id : the public id of the client. This was determined by the authorization server when the client was registered. device_code : a code representing the device. After the user approved the request, the token endpoint responds with the access token. The Trello app uses the access token to create an event in the user's Google Calendar. You can try the Device authorization grant in the OAuth.net playground . Refresh token grant \u00b6 The Refresh token grant has been designed to exchange a refresh token for an expired access token. Here's how this grant would work in our Trello example: Trello requests the authorization server to refresh an expired token with a HTTP POST request: grant_type : set this parameter to refresh_token to request a token refresh. refresh_token : refresh token. This token was initially returned by the authorization server alongside the access token. client_id : the public id of the client. This was determined by the authorization server when the client was registered. client_secret : the private secret stored by the client. It was associated by the authorization server to the client_id when the client was registered. This parameter can be omitted if the client does not have a client secret. The token endpoint responds with the new access token. The response may not contain a new refresh token, meaning that the existing refresh token is still valid. The Trello app uses the new access token to create an event in the user's Google Calendar. Implicit grant \u00b6 The Implicit grant has been designed to immediately return an access token to the client, without first performing a code exchange. This grant was the recommended choice for native and JavaScript applications. But now its usage is discouraged because returning access tokens in a HTTP redirect brings several security risks to your application. These kinds of applications should instead use the Authorization code flow with PKCE. Here's how the Implicit grant would work in our Trello example: The user clicks on a button in the Trello's web app in order to connect his Google account to the Trello web app. The Trello web app requests the token by making a HTTP GET request to the Google account's authorization endpoint with these parameters: client_id : the public id of the client. This was determined by the authorization server when the client was registered. response_type : specifies the grant type. By setting this parameter to token , the client indicates that it wants to use the implicit grant. state : used to prevent attackers from crafting redirect URLs with malicious tokens. redirect_uri : this is where the user will be sent after he approves the permission request. scope : describes the scope of the authorization (optional). The Google account server responds with the consent screen page. The user examines the requested permissions in the consent screen and then gives consent to the Trello web app. The Google account server redirects the user to the redirect_uri on the client. This URL uses the hash character ( # ) to specify parameters so that they are not sent to the server. These are the parameters received by the client: access_token : the OAuth 2.0 access token. token_type : set to Bearer to indicate that the access token must be used with the Bearer scheme. expires_in : the lifespan of the access token. scope : describes the scope of the authorization (optional). state : the state that was previously set for this authorization request. The client needs to verify this match before using the access token to avoid malicious token injections. The Trello web app uses the access token to create an event in the user's Google Calendar. Password grant \u00b6 The Password grant has been designed to exchange the user's credentials for an access token. Since the client needs to read the password in cleartext, this grant should not be used at all. This is also stated in the latest OAuth 2.0 Security Best Current Practice . Which grant should you use? \u00b6 Now we understand which are the the grant types in the OAuth 2.0 specification. So, which grant should you use for your application ? The following flow chart answer this question: Vulnerabilities and Attacks \u00b6 There are some known attacks that can exploit vulnerable implementations of OAuth 2.0. In this section we will go through some of the most known OAuth 2.0 attacks. Phishing with fake consent screens \u00b6 In some OAuth 2.0 grants, the user is redirected to a consent screen (e.g. the Google consent screen). How can the user be sure that he's actually dealing with a legitimate consent screen? Some malicious apps may exploit this mechanism to make the user input his credentials in a fake, carefully crafted consent screen. Authorization Code CSRF \u00b6 If applications don't use and validate state parameters they can be vulnerable to different types of attacks. Let's suppose that Trello doesn't validate state parameters. Then, a malicious party would be able to carry out this CSRF attack: Alice visits the Trello app and clicks on the button to authorize access to Google Calendar. Trello redirects Alice's browser to Google Account, requesting a redirect back to itself once Alice approves the authorization request. Alice is redirected to the Google Account, where she enters her credentials in order to authorize access to the Trello application. After a successful login, Alice prevents the subsequent redirect request and saves its URL (i.e. the callback URL with an authorizaiton code). Alice somehow gets Bob to visit the redirect URL. Bob may already be logged in to Trello with his account. Bob clicks the link to Trello and the authorization code is exchanged for an access token. If Bob is logged in then Trello may associate Bob's account with Alice's access token. Now if Bob links a date to an item on his Trello account, Alice will be able to see that in her Google Calendar interface. In reality, Trello maintains and validates a state parameter. This means Trello would reject such requests even if Bob would inadvertently click on the URL sent by Alice. Implicit grant security risks \u00b6 As of today, OAuth 2.0 discourages usage of the Implicit grant because of the following security risks: Lack of confidentiality for access tokens Misuse of access token to impersonate resource owner More resources \u00b6 If you want to learn more about OAuth 2.0, you can consult these resources: Aaron Parecki's blog OAuth 2.0 Simplified, by Aaron Parecki OAuth.net Playground Getting Started with OAuth 2.0, on Pluralsight","title":"OAuth 2.0"},{"location":"oauth2-and-openid-connect/oauth2/#oauth-20","text":"OAuth 2.0 is an open framework that allows to securely obtain authorization to access resources via HTTP. For example, you can tell Google that Trello is allowed to access your Google Calendar in order to associate tasks with dates, without sharing your Google password with Trello . This is possible because Trello can send OAuth 2.0 tokens to Google which prove your consent. OAuth 2.0 is not considered to be a protocol because the specification is rather vague. In fact, this is one of the reasons why Eran Hammer resigned from his role of lead author for OAuth 2.0. In the following sections we will go through the main concepts of the OAuth 2.0 framework.","title":"OAuth 2.0"},{"location":"oauth2-and-openid-connect/oauth2/#purpose","text":"One of the best analogies is to consider an OAuth 2.0 token to be the valet key to your car. By using the valet key, the valet can start and move (for a limited distance) your car but he can't access the trunk. An OAuth 2.0 token serves the same purpose of the valet key. As a resource owner, you decide which data each consumer is allowed to access from your service providers. You can give each consumer a different valet key, while being sure that none of these consumers has access to your full key or to any data from which they can build a full key.","title":"Purpose"},{"location":"oauth2-and-openid-connect/oauth2/#roles","text":"Roles identify the actors that can participate in OAuth 2.0 flows. There are 4 roles: Resource Owner : the user who is giving authorization to access his account information. Client : the application that attempts to get access to the user's account. It needs the user's permission in order to get this access. Authorization Server : the server that allows users to approve or deny an authorization request. Resource Server : the server that consumes the user's information. This is how OAuth 2.0 roles would be translated in our example:","title":"Roles"},{"location":"oauth2-and-openid-connect/oauth2/#endpoints","text":"Endpoints are URIs that define the location of authorization services. In OAuth 2.0, there are 3 endpoints: The Authorization endpoint is on the authorization server. It's used for the resource owner's log in and grants authorization to the client application. The Redirect endpoint is on the client application. The resource owner is redirected to this endpoint after having granted authorization at the authorization endpoint. The Token endpoint is on the authorization server. This is where the client application exchanges some secret information for an access token. Note that some OAuth 2.0 grants don't involve all these endpoints but just some of them. This is how the OAuth 2.0 endpoints would be translated in our Trello example:","title":"Endpoints"},{"location":"oauth2-and-openid-connect/oauth2/#scopes","text":"Scopes can be used to limit a client's access to the user's resources. Basically: A client can request one or more scopes. The requested scopes are presented to the user in the consent screen. If the user gives his consent, the access token issued to the application will be limited to those scopes. The OAuth 2.0 specification does not define any specific values for scopes, since they are highly dependent on the service provider. For example, these are the scopes for the Google Calendar's API:","title":"Scopes"},{"location":"oauth2-and-openid-connect/oauth2/#grants","text":"Authorization grants are methods for a client to get an access token . The access token proves that a client has the permission to access the user's data. Authorization grants are also known as flows . OAuth 2.0 recommends to use one of the following grants: Authorization code Authorization code with PKCE Client credentials Device authorization Refresh token But there are also some grants whose usage is discouraged by the OAuth 2.0 Security Best Current Practice document : Implicit grant Password grant","title":"Grants"},{"location":"oauth2-and-openid-connect/oauth2/#authorization-code-grant","text":"The Authorization code grant has been designed for clients which can securely store a client secret. This grant is typically used when the client is a web server. Here's how the Authorization code grant would work in our Trello example: The user clicks on a button in the Trello's web app in order to connect his Google account to Trello. The Trello web app requests an authorization code. This operation can be done in several ways (e.g. redirection, link on HTML button). The code can be retrieved by making a HTTP GET request to the Google account's authorization endpoint with these parameters: client_id : the public id of the client. This was determined by the authorization server when the client was registered. response_type : specifies the grant type. By setting this parameter to code , the client indicates that it wants to start an authorization code grant. state : used to prevent CSRF attacks . redirect_uri : this is where the user will be sent after he approves the permission request. scope : describes the scope of the authorization (optional). The Google account server responds with the consent screen page. The user examines the requested permissions in the consent screen and then gives consent to Trello. The Google account server redirects the user to the redirect_uri on the client with these parameters: code : the authorization code. state : the state that was previously set for this authorization request. Trello now has the authorization code and exchanges it for the access token to the user's Google account by making a HTTP POST request: code : the authorization code. grant_type : specifies the grant type. By setting this parameter to authorization_code , the client indicates that it wants to complete an authorization code grant. redirect_uri : the uri where the authorization code has been previously sent. client_id : the public id of the client. This was determined by the authorization server when the client was registered. client_secret : the private secret stored by the client. It was associated by the authorization server to the client_id when the client was registered. Trello uses the access token to create an event in the user's Google Calendar. You can try the Authorization code grant in the OAuth.net playground .","title":"Authorization code grant"},{"location":"oauth2-and-openid-connect/oauth2/#proof-key-for-code-exchange-pkce","text":"PKCE extends the Authorization code grant to securely perform the code exchange with clients that cannot privately store the authorization code . This technique can be applied to any client, although it's typically used when the client is a mobile or a web browser application. Here's how the Authorization code grant with PKCE would work in our Trello example: The user clicks on a button in the Trello's web app in order to connect his Google account to Trello. The Trello web app generates a code verifier (i.e. a cryptographically secure random string) and the respective code challenge , which can be an hash of the code verifier or the code verifier itself. This code must be saved in the web app for later use. The Trello web app requests an authorization code. This operation can be done in several ways (e.g. redirection, link on HTML button). The code can be retrieved by making a HTTP GET request to the Google account's authorization endpoint with these parameters: client_id : the public id of the client. This was determined by the authorization server when the client was registered. response_type : specifies the grant type. By setting this parameter to code , the client indicates that it wants to start an authorization code grant. state : used to prevent CSRF attacks . redirect_uri : this is where the user will be sent after he approves the permission request. scope : describes the scope of the authorization (optional). code_challenge : used to send the code challenge previously generated by the Trello web app. code_challenge_method : if it's set to S256 , indicates that the code verifier will be hashed with SHA256 and then base64-url-encoded into the code challenge. Otherwise, the secret code will be used as the code challenge. The Google account server responds with the consent screen page. The user examines the requested permissions in the consent screen and then gives consent to Trello. The Google account server redirects the user to the redirect_uri on the client with these parameters: code : the authorization code. state : the state that was previously set for this authorization request. Trello now has the authorization code and exchanges it for the access token to the user's Google account by making a HTTP POST request: code : the authorization code. grant_type : specifies the grant type. By setting this parameter to code , the client indicates that it wants to complete an authorization code grant. redirect_uri : the uri where the authorization code has been previously sent. client_id : the public id of the client. This was determined by the authorization server when the client was registered. code_verifier : the code verifier that was initially generated by the Trello client. Google Account will check whether it matches the challenge that was specified by Trello in the authorization request. If an attacker intercepted the authorization code, it won't be able to exchange it for an access token. Trello uses the access token to create an event in the user's Google Calendar. You can try the Authorization code grant with PKCE in the OAuth.net playground .","title":"Proof Key for Code Exchange (PKCE)"},{"location":"oauth2-and-openid-connect/oauth2/#client-credentials-grant","text":"The Client credentials grant is used when clients need to be authorized to act on behalf of a service account rather than a human user. For example, let's suppose that Trello has a system-internal service that computes statistics about users to create some reports. In order to do so, the Report service needs to be authorized to read the Trello users' data. Here's how the client credentials grant would work in this example: The Report service requests the access token that represents its service account by making a HTTP POST request: grant_type : specifies the grant type. By setting this parameter to client_credentials , the client indicates that it wants to complete a client credentials grant. client_id : the public id of the client. This was determined by the authorization server when the client was registered. client_secret : the private secret stored by the client. It was associated by the authorization server to the client_id when the client was registered. The Report service uses the access token to read the Trello users' data.","title":"Client credentials grant"},{"location":"oauth2-and-openid-connect/oauth2/#device-authorization-grant","text":"The Device authorization grant has been designed for devices with limited input capabilities. For example, let's suppose we are using a Trello app for Apple TV and you want to authorize the app to access your Google Calendar's data. Here's how the device authorization grant would work in this case: The user clicks on a button in the Trello's web app in order to connect his Google account to Trello. The Trello app requests a device code with a HTTP POST request to the device endpoint on the authorization server: client_id : the public id of the client. This was determined by the authorization server when the client was registered. The authorization server responds with the requested data: device_code : a code representing the device, to be later displayed to the user. user_code : a code representing the user, to be later displayed to the user. verification_uri : represents the URL the user must visit to enter the user_code and device_code. interval : the minimum amount of time in seconds that the client should wait between polling requests to the token endpoint. expires_in : the lifetime in seconds of the device_code and user_code. (optional) The Trello app displays a message to the user, instructing him to visit the verification_url and the insert the user_code and device_code. The user visits the verification_uri, then approves the authorization request by inserting the user_code and device_code. While the Trello app was waiting for the user to approve the authorization request, it was polling the token endpoint with an HTTP POST request: grant_type : set this parameter to urn:ietf:params:oauth:grant-type:device_code to complete a device authorization grant. client_id : the public id of the client. This was determined by the authorization server when the client was registered. device_code : a code representing the device. After the user approved the request, the token endpoint responds with the access token. The Trello app uses the access token to create an event in the user's Google Calendar. You can try the Device authorization grant in the OAuth.net playground .","title":"Device authorization grant"},{"location":"oauth2-and-openid-connect/oauth2/#refresh-token-grant","text":"The Refresh token grant has been designed to exchange a refresh token for an expired access token. Here's how this grant would work in our Trello example: Trello requests the authorization server to refresh an expired token with a HTTP POST request: grant_type : set this parameter to refresh_token to request a token refresh. refresh_token : refresh token. This token was initially returned by the authorization server alongside the access token. client_id : the public id of the client. This was determined by the authorization server when the client was registered. client_secret : the private secret stored by the client. It was associated by the authorization server to the client_id when the client was registered. This parameter can be omitted if the client does not have a client secret. The token endpoint responds with the new access token. The response may not contain a new refresh token, meaning that the existing refresh token is still valid. The Trello app uses the new access token to create an event in the user's Google Calendar.","title":"Refresh token grant"},{"location":"oauth2-and-openid-connect/oauth2/#implicit-grant","text":"The Implicit grant has been designed to immediately return an access token to the client, without first performing a code exchange. This grant was the recommended choice for native and JavaScript applications. But now its usage is discouraged because returning access tokens in a HTTP redirect brings several security risks to your application. These kinds of applications should instead use the Authorization code flow with PKCE. Here's how the Implicit grant would work in our Trello example: The user clicks on a button in the Trello's web app in order to connect his Google account to the Trello web app. The Trello web app requests the token by making a HTTP GET request to the Google account's authorization endpoint with these parameters: client_id : the public id of the client. This was determined by the authorization server when the client was registered. response_type : specifies the grant type. By setting this parameter to token , the client indicates that it wants to use the implicit grant. state : used to prevent attackers from crafting redirect URLs with malicious tokens. redirect_uri : this is where the user will be sent after he approves the permission request. scope : describes the scope of the authorization (optional). The Google account server responds with the consent screen page. The user examines the requested permissions in the consent screen and then gives consent to the Trello web app. The Google account server redirects the user to the redirect_uri on the client. This URL uses the hash character ( # ) to specify parameters so that they are not sent to the server. These are the parameters received by the client: access_token : the OAuth 2.0 access token. token_type : set to Bearer to indicate that the access token must be used with the Bearer scheme. expires_in : the lifespan of the access token. scope : describes the scope of the authorization (optional). state : the state that was previously set for this authorization request. The client needs to verify this match before using the access token to avoid malicious token injections. The Trello web app uses the access token to create an event in the user's Google Calendar.","title":"Implicit grant"},{"location":"oauth2-and-openid-connect/oauth2/#password-grant","text":"The Password grant has been designed to exchange the user's credentials for an access token. Since the client needs to read the password in cleartext, this grant should not be used at all. This is also stated in the latest OAuth 2.0 Security Best Current Practice .","title":"Password grant"},{"location":"oauth2-and-openid-connect/oauth2/#which-grant-should-you-use","text":"Now we understand which are the the grant types in the OAuth 2.0 specification. So, which grant should you use for your application ? The following flow chart answer this question:","title":"Which grant should you use?"},{"location":"oauth2-and-openid-connect/oauth2/#vulnerabilities-and-attacks","text":"There are some known attacks that can exploit vulnerable implementations of OAuth 2.0. In this section we will go through some of the most known OAuth 2.0 attacks.","title":"Vulnerabilities and Attacks"},{"location":"oauth2-and-openid-connect/oauth2/#phishing-with-fake-consent-screens","text":"In some OAuth 2.0 grants, the user is redirected to a consent screen (e.g. the Google consent screen). How can the user be sure that he's actually dealing with a legitimate consent screen? Some malicious apps may exploit this mechanism to make the user input his credentials in a fake, carefully crafted consent screen.","title":"Phishing with fake consent screens"},{"location":"oauth2-and-openid-connect/oauth2/#authorization-code-csrf","text":"If applications don't use and validate state parameters they can be vulnerable to different types of attacks. Let's suppose that Trello doesn't validate state parameters. Then, a malicious party would be able to carry out this CSRF attack: Alice visits the Trello app and clicks on the button to authorize access to Google Calendar. Trello redirects Alice's browser to Google Account, requesting a redirect back to itself once Alice approves the authorization request. Alice is redirected to the Google Account, where she enters her credentials in order to authorize access to the Trello application. After a successful login, Alice prevents the subsequent redirect request and saves its URL (i.e. the callback URL with an authorizaiton code). Alice somehow gets Bob to visit the redirect URL. Bob may already be logged in to Trello with his account. Bob clicks the link to Trello and the authorization code is exchanged for an access token. If Bob is logged in then Trello may associate Bob's account with Alice's access token. Now if Bob links a date to an item on his Trello account, Alice will be able to see that in her Google Calendar interface. In reality, Trello maintains and validates a state parameter. This means Trello would reject such requests even if Bob would inadvertently click on the URL sent by Alice.","title":"Authorization Code CSRF"},{"location":"oauth2-and-openid-connect/oauth2/#implicit-grant-security-risks","text":"As of today, OAuth 2.0 discourages usage of the Implicit grant because of the following security risks: Lack of confidentiality for access tokens Misuse of access token to impersonate resource owner","title":"Implicit grant security risks"},{"location":"oauth2-and-openid-connect/oauth2/#more-resources","text":"If you want to learn more about OAuth 2.0, you can consult these resources: Aaron Parecki's blog OAuth 2.0 Simplified, by Aaron Parecki OAuth.net Playground Getting Started with OAuth 2.0, on Pluralsight","title":"More resources"},{"location":"oauth2-and-openid-connect/openid-connect/","text":"OpenID Connect \u00b6 OpenID Connect ( OIDC ) is an authentication protocol. It builds upon OAuth 2.0 and JSON Web Tokens to provide users one login for multiple sites. For example, Trello lets you create a new account using an existing Google account. In this case, Trello only needs to access some basic information of your Google account (e.g. email and username). The main advantage is that you don't need to share your Google password with Trello . This is possible because Trello can use OpenID Connect tokens to validate your Google account identity. In the following sections we will go through the main concepts of the OpenID Connect protocol. Purpose \u00b6 Sometimes you need only an authentication mechanism, without any authorization logic. But OAuth 2.0 is not good enough by itself to be used for authentication. Let's see why by examining how authentication could be implemented with OAuth 2.0: We could use a custom OAuth 2.0 scope named signin that represents the permission to access the basic information of the user. The idea is: if the user is able to grant us that token, then surely he must have authenticated . But there are some problems with this approach: The authentication step made by the user is not correlated to the authorization request. The user data endpoint is usually different for each service (e.g. Google may have a different endpoint from Facebook). In fact, this usage of OAuth 2.0 is vulnerable to a very simple yet dangerous impersonification attack: if a malicious application has been granted access to your user data, it can reuse the same access_token to authenticate as you to another service (e.g. e-banking) using the same authentication mechanism. With this approach, providers using OAuth 2.0 for authentication would need to implement extra security measures. But this measure may not be standardized across different providers! This means software developers would need to write different code to authenticate to different providers. So, in order to make OAuth 2.0 suitable for authentication we need: A new token type which can be validated in a standardized way. A standardized mechanism to access the user's information. This is exactly what OpenID Connect provides on top of OAuth 2.0, with the help of JWT. Endpoints \u00b6 Endpoints are URIs that define the location of authentication services. In OpenID Connect, there are 3 endpoints: The Authorization endpoint is usually on the authorization server. It's used to perform authentication of the user. The Token endpoint is usually on the authorization server. This is where the client application exchanges some secret information for an access token, ID token and optionally a refresh token. The UserInfo endpoint is usually on the authorization server. It's used to retrieve claims about the currently authenticated user. This is how the OpenID Connect endpoints would be translated in our Trello example: Scopes \u00b6 OAuth 2.0 scopes in OpenID Connect are used to define to which ID token claims the client is requesting access. The OpenID Connect specification defines a set of standard scopes . The only required scope is openid , which states that the client intends to use the OpenId Connect protocol. Id Tokens \u00b6 ID Tokens are JWTs introduced by OpenID Connect that contain identity data. An ID Token can be returned in a token response, alongside an access_token . It can be consumed by the application and it also contains user information (e.g. username, email). The OpenID Connect specification defines a set of standard claims . Still, it's possible to define custom claims and add them to your tokens. Flows \u00b6 The OpenID Connect specification defines 3 authentication flows: Authorization code flow Implicit flow Hybrid flow Authorization code flow \u00b6 The Authorization code flow has been designed for clients which can securely store a client secret. This grant is typically used when the client is a web server. Here's how the Authorization code flow would work in our Trello example: The user clicks on a button in the Trello's web app in order to login with his Google account to Trello. The Trello web app requests an authorization code. This operation can be done in several ways (e.g. redirection, link on HTML button). The code can be retrieved by making a HTTP GET request to the Google account's authorization endpoint with these parameters: client_id : the public id of the client. This was determined by the authorization server when the client was registered. response_type : specifies the grant type. By setting this parameter to code , the client indicates that it wants to start an authorization code flow. state : used to prevent CSRF attacks . redirect_uri : this is where the user will be sent after he approves the permission request. scope : describes the scope of the authorization. By setting this parameter to openid+... , the client specifies that it intends to start an OpenID Connect authorization code flow. The Google account server responds with the consent screen page. The user examines the requested permissions in the consent screen and then gives consent to Trello. The Google account server redirects the user to the redirect_uri on the client with these parameters: code : the authorization code. state : the state that was previously set for this authentication request. Trello now has the authorization code and exchanges it for the access token and ID token for the user's Google account by making a HTTP POST request: code : the authorization code. grant_type : specifies the grant type. By setting this parameter to authorization_code , the client indicates that it wants to complete an authorization code grant. redirect_uri : the uri where the authorization code has been previously sent. client_id : the public id of the client. This was determined by the authorization server when the client was registered. client_secret : the private secret stored by the client. It was associated by the authorization server to the client_id when the client was registered. Trello now validates the ID token. If the validation succeeds, Trello can be sure that the token it received was actually intended for its client. This solves the impersonification attack we've discussed earlier . If Trello needs more user information than the one included in the Id token, it can contact the UserInfo endpoint on the authorization server. The UserInfo endpoint on the authorization server responds in a standardized format. Implicit flow \u00b6 The Implicit flow has been designed to immediately return an access token to the client, without first performing a code exchange. It is mainly used by web browser applications without a server. The Access Token and ID Token are returned directly to the client, which may expose them to the user and other applications that have access to the client. Here's how the Implicit flow would work in our Trello example: The user clicks on a button in the Trello's web app in order to login with his Google account to Trello. The Trello web app requests the ID token. It can be retrieved by making a HTTP GET request to the Google account's authorization endpoint with these parameters: client_id : the public id of the client. This was determined by the authorization server when the client was registered. response_type : specifies the grant type. By setting this parameter to id_token+token , the client indicates that it wants to start an OpenID Connect Implicit flow that returns an ID token and an access token. state : used to prevent CSRF attacks . redirect_uri : this is where the user will be sent after he authenticates. scope : describes the scope of the authorization. By setting this parameter to openid+... , the client specifies that it intends to start an OpenID Connect authorization code flow. nonce : associates a client session with an ID token in order to mitigate replay attacks . It's a mandatory parameter in the OpenID Connect implicit flow. The Google account server responds with the consent screen page. The user examines the requested permissions in the consent screen and then gives consent to Trello. The Google account server redirects the user to the redirect_uri on the client. This URL uses the hash character ( # ) to specify parameters so that they are not sent to the server. These are the parameters received by the client: access_token : the OAuth 2.0 access token. id_token : the OpenID Connect ID token. token_type : set to Bearer to indicate that the access token must be used with the Bearer scheme. expires_in : the lifespan of the access token. state : the state that was previously set for this authorization request. The client needs to verify this match before using the access token to avoid malicious token injections. The Trello web app now validates the ID token. Also, the value of the nonce claim must be checked to verify that it is the same value as the one that was sent in the authentication request. If the validation succeeds, the Trello web app can be sure that the token it received was actually intended for its client. This solves the impersonification attack we've discussed earlier . If the Trello web app needs more user information than the one included in the Id token, it can contact the UserInfo endpoint on the authorization server. The UserInfo endpoint on the authorization server responds in a standardized format. Hybrid flow \u00b6 The Hybrid flow combines mechanisms of the Authorization code flow and Implicit flow . It allows the front end and back end of the application to receive their own tokens, with their own scopes. The Hybrid flow has three possible variations. To use a specific variation, set the response_type parameter to one of these values: code id_token code token code id_token token These notes won't contain the details of the Hybrid flow because it's rarely used in practice. Which flow should you use? \u00b6 Now we understand which are the the available flows in the OpenID Connect specification. So, which flow should you use for your application ? The following flow chart answer this question: Vulnerabilities and attacks \u00b6 There are some known attacks that can exploit vulnerable implementations of OpenID Connect. In this section we will go through some of the most known OpenID Connect attacks. Replay attack \u00b6 When we're using the OpenID Connect Implicit flow in a web browser app without server, the nonce parameter is a fundamental security measure. Let's suppose that, in our example, the Trello web app is not using a nonce parameter. Here's a replay attack that can be carried out in this situation: The Trello web app redirects the user to the authentication server with a response_type of id_token . The user authenticates and gives his consent. Google Account redirects the user back to the Trello web app with an id_token . An attacker is able to get the response URL in some way (e.g. via packet sniffing). The attacker pastes the response URL into their browser's URL bar, effectively authenticating as the original user. Instead, a canonical implementation of OpenID Connect would mandate handling the nonce in this way: The Trello web app generates a cryptographically secure random nonce and stores in clear text, for example in the browser's local storage. The Trello web app hashes the nonce and sends the hash as the value of the nonce parameter in the authentication request. When the Trello web app receives the authentication response, it reads and then removes the nonce from the persistent storage, hashes it, and compares it against the hashed nonce in the id_token. If they don't match, then the client application refuses to establish identity. By using the nonce, even if the attacker intercepts the response, he would still need the clear-text nonce to authenticate with the Trello web app. More resources \u00b6 If you want to learn more about OpenID Connect, you can consult these resources: OpenID Foundation's website Comprehensive list of OpenID Connect specifications","title":"OpenID Connect"},{"location":"oauth2-and-openid-connect/openid-connect/#openid-connect","text":"OpenID Connect ( OIDC ) is an authentication protocol. It builds upon OAuth 2.0 and JSON Web Tokens to provide users one login for multiple sites. For example, Trello lets you create a new account using an existing Google account. In this case, Trello only needs to access some basic information of your Google account (e.g. email and username). The main advantage is that you don't need to share your Google password with Trello . This is possible because Trello can use OpenID Connect tokens to validate your Google account identity. In the following sections we will go through the main concepts of the OpenID Connect protocol.","title":"OpenID Connect"},{"location":"oauth2-and-openid-connect/openid-connect/#purpose","text":"Sometimes you need only an authentication mechanism, without any authorization logic. But OAuth 2.0 is not good enough by itself to be used for authentication. Let's see why by examining how authentication could be implemented with OAuth 2.0: We could use a custom OAuth 2.0 scope named signin that represents the permission to access the basic information of the user. The idea is: if the user is able to grant us that token, then surely he must have authenticated . But there are some problems with this approach: The authentication step made by the user is not correlated to the authorization request. The user data endpoint is usually different for each service (e.g. Google may have a different endpoint from Facebook). In fact, this usage of OAuth 2.0 is vulnerable to a very simple yet dangerous impersonification attack: if a malicious application has been granted access to your user data, it can reuse the same access_token to authenticate as you to another service (e.g. e-banking) using the same authentication mechanism. With this approach, providers using OAuth 2.0 for authentication would need to implement extra security measures. But this measure may not be standardized across different providers! This means software developers would need to write different code to authenticate to different providers. So, in order to make OAuth 2.0 suitable for authentication we need: A new token type which can be validated in a standardized way. A standardized mechanism to access the user's information. This is exactly what OpenID Connect provides on top of OAuth 2.0, with the help of JWT.","title":"Purpose"},{"location":"oauth2-and-openid-connect/openid-connect/#endpoints","text":"Endpoints are URIs that define the location of authentication services. In OpenID Connect, there are 3 endpoints: The Authorization endpoint is usually on the authorization server. It's used to perform authentication of the user. The Token endpoint is usually on the authorization server. This is where the client application exchanges some secret information for an access token, ID token and optionally a refresh token. The UserInfo endpoint is usually on the authorization server. It's used to retrieve claims about the currently authenticated user. This is how the OpenID Connect endpoints would be translated in our Trello example:","title":"Endpoints"},{"location":"oauth2-and-openid-connect/openid-connect/#scopes","text":"OAuth 2.0 scopes in OpenID Connect are used to define to which ID token claims the client is requesting access. The OpenID Connect specification defines a set of standard scopes . The only required scope is openid , which states that the client intends to use the OpenId Connect protocol.","title":"Scopes"},{"location":"oauth2-and-openid-connect/openid-connect/#id-tokens","text":"ID Tokens are JWTs introduced by OpenID Connect that contain identity data. An ID Token can be returned in a token response, alongside an access_token . It can be consumed by the application and it also contains user information (e.g. username, email). The OpenID Connect specification defines a set of standard claims . Still, it's possible to define custom claims and add them to your tokens.","title":"Id Tokens"},{"location":"oauth2-and-openid-connect/openid-connect/#flows","text":"The OpenID Connect specification defines 3 authentication flows: Authorization code flow Implicit flow Hybrid flow","title":"Flows"},{"location":"oauth2-and-openid-connect/openid-connect/#authorization-code-flow","text":"The Authorization code flow has been designed for clients which can securely store a client secret. This grant is typically used when the client is a web server. Here's how the Authorization code flow would work in our Trello example: The user clicks on a button in the Trello's web app in order to login with his Google account to Trello. The Trello web app requests an authorization code. This operation can be done in several ways (e.g. redirection, link on HTML button). The code can be retrieved by making a HTTP GET request to the Google account's authorization endpoint with these parameters: client_id : the public id of the client. This was determined by the authorization server when the client was registered. response_type : specifies the grant type. By setting this parameter to code , the client indicates that it wants to start an authorization code flow. state : used to prevent CSRF attacks . redirect_uri : this is where the user will be sent after he approves the permission request. scope : describes the scope of the authorization. By setting this parameter to openid+... , the client specifies that it intends to start an OpenID Connect authorization code flow. The Google account server responds with the consent screen page. The user examines the requested permissions in the consent screen and then gives consent to Trello. The Google account server redirects the user to the redirect_uri on the client with these parameters: code : the authorization code. state : the state that was previously set for this authentication request. Trello now has the authorization code and exchanges it for the access token and ID token for the user's Google account by making a HTTP POST request: code : the authorization code. grant_type : specifies the grant type. By setting this parameter to authorization_code , the client indicates that it wants to complete an authorization code grant. redirect_uri : the uri where the authorization code has been previously sent. client_id : the public id of the client. This was determined by the authorization server when the client was registered. client_secret : the private secret stored by the client. It was associated by the authorization server to the client_id when the client was registered. Trello now validates the ID token. If the validation succeeds, Trello can be sure that the token it received was actually intended for its client. This solves the impersonification attack we've discussed earlier . If Trello needs more user information than the one included in the Id token, it can contact the UserInfo endpoint on the authorization server. The UserInfo endpoint on the authorization server responds in a standardized format.","title":"Authorization code flow"},{"location":"oauth2-and-openid-connect/openid-connect/#implicit-flow","text":"The Implicit flow has been designed to immediately return an access token to the client, without first performing a code exchange. It is mainly used by web browser applications without a server. The Access Token and ID Token are returned directly to the client, which may expose them to the user and other applications that have access to the client. Here's how the Implicit flow would work in our Trello example: The user clicks on a button in the Trello's web app in order to login with his Google account to Trello. The Trello web app requests the ID token. It can be retrieved by making a HTTP GET request to the Google account's authorization endpoint with these parameters: client_id : the public id of the client. This was determined by the authorization server when the client was registered. response_type : specifies the grant type. By setting this parameter to id_token+token , the client indicates that it wants to start an OpenID Connect Implicit flow that returns an ID token and an access token. state : used to prevent CSRF attacks . redirect_uri : this is where the user will be sent after he authenticates. scope : describes the scope of the authorization. By setting this parameter to openid+... , the client specifies that it intends to start an OpenID Connect authorization code flow. nonce : associates a client session with an ID token in order to mitigate replay attacks . It's a mandatory parameter in the OpenID Connect implicit flow. The Google account server responds with the consent screen page. The user examines the requested permissions in the consent screen and then gives consent to Trello. The Google account server redirects the user to the redirect_uri on the client. This URL uses the hash character ( # ) to specify parameters so that they are not sent to the server. These are the parameters received by the client: access_token : the OAuth 2.0 access token. id_token : the OpenID Connect ID token. token_type : set to Bearer to indicate that the access token must be used with the Bearer scheme. expires_in : the lifespan of the access token. state : the state that was previously set for this authorization request. The client needs to verify this match before using the access token to avoid malicious token injections. The Trello web app now validates the ID token. Also, the value of the nonce claim must be checked to verify that it is the same value as the one that was sent in the authentication request. If the validation succeeds, the Trello web app can be sure that the token it received was actually intended for its client. This solves the impersonification attack we've discussed earlier . If the Trello web app needs more user information than the one included in the Id token, it can contact the UserInfo endpoint on the authorization server. The UserInfo endpoint on the authorization server responds in a standardized format.","title":"Implicit flow"},{"location":"oauth2-and-openid-connect/openid-connect/#hybrid-flow","text":"The Hybrid flow combines mechanisms of the Authorization code flow and Implicit flow . It allows the front end and back end of the application to receive their own tokens, with their own scopes. The Hybrid flow has three possible variations. To use a specific variation, set the response_type parameter to one of these values: code id_token code token code id_token token These notes won't contain the details of the Hybrid flow because it's rarely used in practice.","title":"Hybrid flow"},{"location":"oauth2-and-openid-connect/openid-connect/#which-flow-should-you-use","text":"Now we understand which are the the available flows in the OpenID Connect specification. So, which flow should you use for your application ? The following flow chart answer this question:","title":"Which flow should you use?"},{"location":"oauth2-and-openid-connect/openid-connect/#vulnerabilities-and-attacks","text":"There are some known attacks that can exploit vulnerable implementations of OpenID Connect. In this section we will go through some of the most known OpenID Connect attacks.","title":"Vulnerabilities and attacks"},{"location":"oauth2-and-openid-connect/openid-connect/#replay-attack","text":"When we're using the OpenID Connect Implicit flow in a web browser app without server, the nonce parameter is a fundamental security measure. Let's suppose that, in our example, the Trello web app is not using a nonce parameter. Here's a replay attack that can be carried out in this situation: The Trello web app redirects the user to the authentication server with a response_type of id_token . The user authenticates and gives his consent. Google Account redirects the user back to the Trello web app with an id_token . An attacker is able to get the response URL in some way (e.g. via packet sniffing). The attacker pastes the response URL into their browser's URL bar, effectively authenticating as the original user. Instead, a canonical implementation of OpenID Connect would mandate handling the nonce in this way: The Trello web app generates a cryptographically secure random nonce and stores in clear text, for example in the browser's local storage. The Trello web app hashes the nonce and sends the hash as the value of the nonce parameter in the authentication request. When the Trello web app receives the authentication response, it reads and then removes the nonce from the persistent storage, hashes it, and compares it against the hashed nonce in the id_token. If they don't match, then the client application refuses to establish identity. By using the nonce, even if the attacker intercepts the response, he would still need the clear-text nonce to authenticate with the Trello web app.","title":"Replay attack"},{"location":"oauth2-and-openid-connect/openid-connect/#more-resources","text":"If you want to learn more about OpenID Connect, you can consult these resources: OpenID Foundation's website Comprehensive list of OpenID Connect specifications","title":"More resources"}]}